{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Title and Abstract Screening Baseline and Uncertainty**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#1)\n",
    "2. [Loading The Relevant Libraries and Packages](#2)\n",
    "3. [Loading Dataset](#3)\n",
    "4. [Prompt Engineering and Title and Abstract Screening](#4)\n",
    "5. [Calculation of Entropy based uncertainty](#5)\n",
    "6. [Calculation of Performance Metrics for each individual Run](#5)\n",
    "7. [Summarization of Zero-shot Performance in Boxplot graph](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id = 1></a>\n",
    "\n",
    "This Jupyter Notebook implements the refinement phase of my Bachelor Thesis on Active Prompting for Large Language Model-Assisted Title and Abstract Screening. Building on the uncertainty evaluation phase, it utilizes the previously selected uncertain and certain example pools. The notebook outlines a single run of GPT-4o-mini for abstract screening, employing both few-shot and chain-of-thought (CoT) approaches. A critical step in this process is the manual insertion of examples from their respective pools into the prompts. Few-shot prompts include only the abstract and solution, while CoT prompts additionally incorporate the reasoning chains. This manual insertion is essential for the experiment's execution. Following the screening, the notebook calculates performance metrics identical to those used in the uncertainty evaluation phase, facilitating direct comparison. This refined phase aims to compare the effectiveness of active prompting (using the uncertain pool) against a control group (using the certain pool). By doing so, it seeks to quantify the impact of each approach on the model's screening performance, offering insights into the potential benefits of active prompting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the relevant Libraries and Packages <a id = 2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python packages\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Data preprocessing libraries\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import math\n",
    "\n",
    "# OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# load environment variables from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "temperature = 0.1\n",
    "max_tokens = 500\n",
    "n_value = 1\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "rate_limit_timeout = 0.15\n",
    "gold_standard_file = os.path.join('Experiment with Certain Exemplars', 'gold_standard_certain_exemplars_removed.xlsx') \n",
    "output_file = os.path.join('Experiment with Certain Exemplars', '3_TN_Exemplars_Few-Shot.xlsx')\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gold standard \n",
    "df = pd.read_excel(gold_standard_file, index_col=None)\n",
    "df = df[df.filter(regex='^(?!Unnamed)').columns]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(abstract): \n",
    "    return  f\"\"\"\n",
    "          You are an experienced researcher tasked with evaluating the relevance of scientific papers. Below is an abstract along with inclusion and exclusion criteria. Determine whether it should be included or excluded based on the criteria and provide a brief explanation for your decision. \n",
    "\n",
    "          ## Inclusion Criteria:\n",
    "\n",
    "          1. The abstract must explicitly mention automated machine learning (AutoML) or related concepts such as low-code/no-code machine learning tools or neural architecture search.\n",
    "          2. The abstract must explicitly mention reproducibility or related concepts such as transparency or explainability in the context of AutoML or the related concepts mentioned above.\n",
    "\n",
    "          ## Exclusion Criteria:\n",
    "\n",
    "          None\n",
    "\n",
    "          ## Instructions\n",
    "          - Evaluate the abstract against each criterion separately.\n",
    "          - For each criterion, state whether it is MET or NOT MET, and provide a brief explanation.\n",
    "          - If you are UNCERTAIN about Criterion A, treat it as MET (include).\n",
    "          - If you are UNCERTAIN about Criterion B, treat it as NOT MET (exclude).\n",
    "          - After evaluating both criteria, provide a combined evaluation (INCLUDE or EXCLUDE).\n",
    "          - For the final evaluation, use 1 for INCLUDE and 0 for EXCLUDE.\n",
    "          - Provide a concise summary (max. 2 sentences) of the key factors influencing inclusion or exclusion.\n",
    "          - Make sure to use double quotes in the response format.\n",
    "\n",
    "          ## Examples: \n",
    "          // It is necessary to manually insert examples from few-host or chain-of-thought the pools \n",
    "\n",
    "          \n",
    "          ## Return the response in JSON format:\n",
    "\n",
    "          {{\n",
    "            \"evaluation\":  // 0 = exclude, 1 = include,\n",
    "            \"explanation\": // \"reason for exclusion\"\n",
    "          }}\n",
    "\n",
    "          ## Abstract to Evaluate:\n",
    "          {abstract}\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate openai client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def query_openai(abstract):\n",
    "    # Create a chat completion request using the gpt-4o-mini model\n",
    "    response = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": generate_prompt(abstract)}],\n",
    "            max_tokens=max_tokens,        \n",
    "            temperature=temperature,         \n",
    "            n=n_value,                   \n",
    "        )\n",
    "\n",
    "    # openai appends responses in json format with ```json ```\n",
    "    def clean_json_string(json_string):\n",
    "        pattern = r'^```json\\s*(.*?)\\s*```$'\n",
    "        cleaned_string = re.sub(pattern, r'\\1', json_string, flags=re.DOTALL)\n",
    "        return cleaned_string.strip()\n",
    "\n",
    "    try:\n",
    "        json_string = clean_json_string(response.choices[0].message.content)\n",
    "        parsed = json.loads(json_string)\n",
    "        return parsed\n",
    "    # Handle parsing errors gracefully\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing JSON response:\", response)\n",
    "        return None, None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, row in df.iterrows():\n",
    "    abstract = row['abstract']\n",
    "    eval_col_name = f\"refined_evaluation\"\n",
    "    expl_col_name = f\"refined_explanation\"\n",
    "\n",
    "    # request classification from openai\n",
    "    data = query_openai(abstract)\n",
    "\n",
    "    # save response data to df     \n",
    "    df.at[j, eval_col_name] = int(data['evaluation'])\n",
    "    df.at[j, expl_col_name] = data['explanation']\n",
    "\n",
    "    if j % 10 == 0:\n",
    "        print(f\"{j} papers scanned\") \n",
    "\n",
    "    # add timeout to prevent exceeding the openai rate-limit\n",
    "    time.sleep(rate_limit_timeout)\n",
    " \n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string evaluations to int; this is important for the further analysis\n",
    "df['gold_standard_evaluation'] = pd.to_numeric(df['gold_standard_evaluation'], errors='coerce')\n",
    "df['refined_evaluation'] = pd.to_numeric(df['refined_evaluation'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = []\n",
    "\n",
    "column_name = f'refined_evaluation'\n",
    "\n",
    "# True Positives (TP): Gold standard is 1, and the model predicted 1\n",
    "tp = ((df['gold_standard_evaluation'] == 1) & (df[column_name] == 1)).sum()\n",
    "    \n",
    "# True Negatives (TN): Gold standard is 0, and the model predicted 0\n",
    "tn = ((df['gold_standard_evaluation'] == 0) & (df[column_name] == 0)).sum()\n",
    "    \n",
    "# False Positives (FP): Gold standard is 0, but the model predicted 1\n",
    "fp = ((df['gold_standard_evaluation'] == 0) & (df[column_name] == 1)).sum()\n",
    "    \n",
    "# False Negatives (FN): Gold standard is 1, but the model predicted 0\n",
    "fn = ((df['gold_standard_evaluation'] == 1) & (df[column_name] == 0)).sum()\n",
    "\n",
    "# Calculate Accuracy \n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "# Calculate Recall \n",
    "recall = tp / (tp + fn) if (tp + fn) != 0 else 0  # Avoid division by zero\n",
    "\n",
    "# Calculate Presicion\n",
    "precision = tp / (tp + fp) if (tp + fp) != 0 else 0 # Avoid division by zero\n",
    "    \n",
    "# Calculate Specificity\n",
    "specificity = tn / (tn + fp) if (tn + fp) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "# Calculate Negative Predictive Value NPV\n",
    "npv = tn / (tn + fn) if (tn + fn) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "analysis_data.append({\n",
    "    'True Positives': tp,\n",
    "    'True Negatives': tn,\n",
    "    'False Positives': fp,\n",
    "    'False Negatives': fn,\n",
    "    'Accuracy': accuracy,\n",
    "    'Recall': recall,\n",
    "    'Precision': precision,\n",
    "    'Specificity': specificity,\n",
    "    'NPV': npv,\n",
    "    'F1': f1_score\n",
    "})\n",
    "\n",
    "# Create analysis DataFrame\n",
    "df_analysis = pd.DataFrame(analysis_data)\n",
    "\n",
    "# Display analysis results\n",
    "print(df_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:  \n",
    "    # Write each DataFrame to a different sheet\n",
    "    df.to_excel(writer, sheet_name='Original Data', index=False)  \n",
    "    df_analysis.to_excel(writer, sheet_name='Analysis Results', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
