{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and Abstract Screening: Baseline Evaluation and Uncertainty Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#1)\n",
    "2. [Environment Setup](#2)\n",
    "3. [Parameter Configuration](#3)\n",
    "4. [OpenAI API Connection and Query Definition](#4)\n",
    "5. [Gold Standard Import](#5)\n",
    "6. [Query Execution](#5)\n",
    "7. [Uncertainty Calculation](#7)\n",
    "8. [Performance Evaluation](#8)\n",
    "9. [Formatation & Export](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id = 1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook implements the uncertainty evaluation phase of my Bachelor Thesis on Active Prompting for Large Language Model-Assisted Title and Abstract Screening. It showcases GPT-4o-mini conducting multiple zero-shot screenings of abstracts, determining inclusion or exclusion. The code calculates entropy from these predictions to quantify the model's certainty for each abstract. Additionally, it computes performance metrics for each iteration, establishing a benchmark for later comparative analysis. Based on these results, two example pools are generated:\n",
    "\n",
    "1. An uncertain pool with high-entropy examples, following active prompting principles.\n",
    "2. A certain pool with low-entropy examples, serving as a control group.\n",
    "\n",
    "These pools will be used for few-shot and chain-of-thought annotation in the subsequent refinement phase, allowing for comparative analysis of the active prompting approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup <a id = 2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python packages\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Data preprocessing libraries\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import math\n",
    "\n",
    "# OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# load environment variables from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Configuration <a id = 3></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "temperature = 0.7\n",
    "max_tokens = 250\n",
    "n_value = 1\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "rate_limit_timeout = 0.15\n",
    "gold_standard_file = os.path.join('data', 'gold_standard.xlsx') \n",
    "output_file = os.path.join('output', 'baseline_and_uncertainty_evaluation.xlsx')\n",
    "iterations = 10\n",
    "\n",
    "def generate_prompt(abstract): \n",
    "    return  f\"\"\"\n",
    "You are an experienced researcher tasked with evaluating the relevance of scientific papers. Below is an abstract along with inclusion and exclusion criteria. Determine whether it should be included or excluded based on the criteria and provide a brief explanation for your decision. \n",
    "\n",
    "## Inclusion Criteria:\n",
    "\n",
    "1. The abstract must explicitly mention automated machine learning (AutoML) or related concepts such as low-code/no-code machine learning tools or neural architecture search.\n",
    "2. The abstract must explicitly mention reproducibility or related concepts such as transparency or explainability in the context of AutoML or the related concepts mentioned above.\n",
    "\n",
    "## Exclusion Criteria:\n",
    "\n",
    "None\n",
    "\n",
    "## Instructions\n",
    "- Provide a concise summary (max. 2 sentences) of the key factors influencing inclusion or exclusion.\n",
    "- Make sure to use double quotes in response format\n",
    "\n",
    "Return the response in json format:\n",
    "\n",
    "{{\n",
    "  \"evaluation\": // 0 = exclude, 1 = include\n",
    "  \"explanation\": // \"reason for exclusion\"  \n",
    "}}\n",
    " \n",
    "## Abstract:\n",
    "{abstract}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OpenAI API Connection and Query Definition  <a id = 4></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate openai client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def query_openai(abstract):\n",
    "    # Create a chat completion request using the gpt-4o-mini model\n",
    "    response = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": generate_prompt(abstract)}],\n",
    "            max_tokens=max_tokens,        \n",
    "            temperature=temperature,         \n",
    "            n=n_value,                   \n",
    "        )\n",
    "\n",
    "    # openai appends responses in json format with ```json ```\n",
    "    def clean_json_string(json_string):\n",
    "        pattern = r'^```json\\s*(.*?)\\s*```$'\n",
    "        cleaned_string = re.sub(pattern, r'\\1', json_string, flags=re.DOTALL)\n",
    "        return cleaned_string.strip()\n",
    "\n",
    "    try:\n",
    "        json_string = clean_json_string(response.choices[0].message.content)\n",
    "        parsed = json.loads(json_string)\n",
    "        return parsed\n",
    "    # Handle parsing errors gracefully\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing JSON response:\", response)\n",
    "        return None, None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gold Standard Import <a id = 5></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gold standard \n",
    "df = pd.read_excel(gold_standard_file, index_col=None, dtype={'publication_year': str, 'gold_standard_evaluation': int})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Execution <a id = 6></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    for j, row in df.iterrows():\n",
    "        abstract = row['abstract']\n",
    "        eval_col_name = f\"eval_{i + 1}\"\n",
    "        expl_col_name = f\"expl_{i + 1}\"\n",
    "\n",
    "        # request classification from openai\n",
    "        data = query_openai(abstract)\n",
    "\n",
    "        # save response data to df     \n",
    "        df.at[j, eval_col_name] = int(data['evaluation'])\n",
    "        df.at[j, expl_col_name] = data['explanation']\n",
    "\n",
    "        if j % 10 == 0:\n",
    "            print(f\"{j} papers of iteration {i} scanned\") \n",
    "\n",
    "        # add timeout to prevent exceeding the openai rate-limit\n",
    "        time.sleep(rate_limit_timeout)\n",
    "\n",
    "    print(f'iteration_{i + 1} complete')\n",
    " \n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uncertainty Calculation <a id = 7></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_entropy(p):\n",
    "    \"\"\"Calculates the binary entropy (in bits) of a probability p.\"\"\"\n",
    "    if p == 0 or p == 1:\n",
    "        return 0 \n",
    "    else:\n",
    "        return -(p * math.log2(p) + (1 - p) * math.log2(1 - p))\n",
    "\n",
    "for n in range(len(df)):\n",
    "    row = df.iloc[n]\n",
    "    run_columns = [col for col in row.index if col.startswith('eval_')]\n",
    "    positive_probabilities = row[run_columns].mean() \n",
    "    df.at[n, 'entropy'] = binary_entropy(positive_probabilities)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Evaluation <a id = 8></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    column_name = f'eval_{i + 1}'\n",
    "\n",
    "    # True Positives (TP): Gold standard is 1, and the model predicted 1\n",
    "    tp = ((df['gold_standard_evaluation'] == 1) & (df[column_name] == 1)).sum()\n",
    "    \n",
    "    # True Negatives (TN): Gold standard is 0, and the model predicted 0\n",
    "    tn = ((df['gold_standard_evaluation'] == 0) & (df[column_name] == 0)).sum()\n",
    "    \n",
    "    # False Positives (FP): Gold standard is 0, but the model predicted 1\n",
    "    fp = ((df['gold_standard_evaluation'] == 0) & (df[column_name] == 1)).sum()\n",
    "    \n",
    "    # False Negatives (FN): Gold standard is 1, but the model predicted 0\n",
    "    fn = ((df['gold_standard_evaluation'] == 1) & (df[column_name] == 0)).sum()\n",
    "\n",
    "    # Calculate Accuracy \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "    # Calculate Recall \n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0  # Avoid division by zero\n",
    "\n",
    "    # Calculate Presicion\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0 # Avoid division by zero\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "    # Calculate Negative Predictive Value NPV\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0 # Avoid division by zero\n",
    "\n",
    "    analysis_data.append({\n",
    "        'Iteration': i + 1,\n",
    "        'True Positives': tp,\n",
    "        'True Negatives': tn,\n",
    "        'False Positives': fp,\n",
    "        'False Negatives': fn,\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'Specificity': specificity,\n",
    "        'NPV': npv,\n",
    "        'F1': f1_score\n",
    "    })\n",
    "\n",
    "# Create analysis DataFrame\n",
    "df_analysis = pd.DataFrame(analysis_data)\n",
    "\n",
    "# Display analysis results\n",
    "print(df_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Formatation & Export <a id = 9></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cols = [col for col in df.columns if col.startswith('eval_')]\n",
    "expl_cols = [col for col in df.columns if col.startswith('expl_')]\n",
    "entropy_col = [col for col in df.columns if col.startswith('entropy')]\n",
    "other_cols = [col for col in df.columns if not (col in eval_cols or col in expl_cols or col in entropy_col)]\n",
    "\n",
    "df_formatted = df[other_cols + eval_cols + entropy_col + expl_cols] \n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:  \n",
    "    # Write each DataFrame to a different sheet\n",
    "    df_formatted.to_excel(writer, sheet_name='Original Data', index=False)  \n",
    "    df_analysis.to_excel(writer, sheet_name='Analysis Results', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
