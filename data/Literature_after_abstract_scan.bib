% This file was created with Citavi 6.18.0.1

@article{Adlung.2021,
 abstract = {Machine learning is increasingly integrated into clinical practice, with applications ranging from pre-clinical data processing, bedside diagnosis assistance, patient stratification, treatment decision making, and early warning as part of primary and secondary prevention. However, a multitude of technological, medical, and ethical considerations are critical in machine-learning utilization, including the necessity for careful validation of machine-learning-based technologies in real-life contexts, unbiased evaluation of benefits and risks, and avoidance of technological over-dependence and associated loss of clinical, ethical, and social-related decision-making capacities. Other challenges include the need for careful benchmarking and external validations, dissemination of end-user knowledge from computational experts to field users, and responsible code and data sharing, enabling transparent assessment of pipelines. In this review, we highlight key promises and achievements in integration of machine-learning platforms into clinical medicine while highlighting limitations, pitfalls, and challenges toward enhanced integration of learning systems into the medical realm. {\copyright} 2021 Elsevier Inc.},
 author = {Adlung, L. and Cohen, Y. and Mor, U. and Elinav, E.},
 year = {2021},
 title = {Machine learning in clinical decision making},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113773331&doi=10.1016%2fj.medj.2021.04.006&partnerID=40&md5=96e058558560599a6a627147b48301e7},
 keywords = {Literature Review},
 pages = {642--665},
 volume = {2},
 number = {6},
 journal = {Med},
 doi = {10.1016/j.medj.2021.04.006},
 file = {Adlung, Cohen et al 2021 - Machine learning in clinical decision:Attachments/Adlung, Cohen et al 2021 - Machine learning in clinical decision.pdf:application/pdf}
}


@inproceedings{Afzal.2021,
 abstract = {Data exploration and quality analysis is an important yet tedious process in the AI pipeline. Current data cleaning and data readiness assessment practices for machine learning tasks are mostly conducted in an arbitrary manner which limits their reuse and often results in loss of productivity. We introduce the concept of a Data Readiness Report as accompanying documentation to a dataset that allows data consumers to get detailed insights into the quality of data. Data characteristics and challenges on various quality dimensions are identified and documented, keeping in mind the principles of transparency and explainability. The Data Readiness Report also serves as a record of all data assessment operations, including applied transformations. This provides a detailed lineage for data governance and management. In effect, the report captures and documents the actions taken by various personas in a data readiness and assessment workflow. Over time this becomes a repository of best practices and can potentially drive a recommendation system for building automated data readiness workflows on the lines of AutoML [1]. The data readiness report could serve as a valuable asset for organizing and operationalizing data in a Data-as-a-service model as it augments the trust and reliability of the datasets. We anticipate that together with the Datasheets [2], Dataset Nutrition Label [3], FactSheets [4] and Model Cards [5], the Data Readiness Report completes the AI documentation pipeline and increases trust and re-useability of data. {\copyright}2021 IEEE},
 author = {Afzal, S. and Rajmohan, C. and Kesarwani, M. and Mehta, S. and Patel, H.},
 title = {Data readiness report},
 keywords = {Artefact Design},
 pages = {42--51},
 year = {2021},
 doi = {10.1109/SMDS53860.2021.00016},
 file = {Afzal, Rajmohan et al 2021 - Data readiness report:Attachments/Afzal, Rajmohan et al 2021 - Data readiness report.pdf:application/pdf}
}


@article{Alcobaca.2020,
 abstract = {Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments. {\copyright} 2020 Edesio Alcoba{\c{c}}a, Felipe Siqueira, Adriano Rivolli, Lu{\'i}s P. F. Garcia, Jefferson T. Oliva, Andr{\'e} C. P. L. F. de Carvalho.},
 author = {Alcoba{\c{c}}a, E. and Siqueira, F. and Rivolli, A. and Garcia, L.P.F. and Oliva, J. T. and de Carvalho, A.C.P.L.F.},
 year = {2020},
 title = {MFE: Towards reproducible meta-feature extraction},
 keywords = {Artefact Design},
 volume = {21},
 journal = {Journal of Machine Learning Research},
 file = {Alcoba{\c{c}}a, Siqueira et al 2020 - MFE Towards reproducible meta-feature extraction:Attachments/Alcoba{\c{c}}a, Siqueira et al 2020 - MFE Towards reproducible meta-feature extraction.pdf:application/pdf}
}


@article{Argesanu.2023,
 abstract = {The integration of machine learning (ML) in various organizations has become an essential aspect with a wide range of applications. However, the development and deployment of machine learning models can be time-consuming and prone to errors due to the iterative nature of the process and the constant testing and retraining of models. As ML becomes more integrated with industrial systems, the demand for controlled, reproducible and repeatable processes rises. This paper proposes a novel approach for the automation of various workflows of the ML model lifecycle via custom Command Line Interfaces (CLI) and Continuous Integration/Continuous Deployment (CI/CD) pipelines. We discuss the challenges and pitfalls of non-automated ML workflows, as well as the benefits of using the proposed toolset. We introduce our bespoke approach to CLI and CI/CD automation, highlighting timesaving as well as consistency-improvement aspects for Process {\&} Pipeline Services' use-case of detecting mechanically induced stress cracking in pipelines. This paper adds to the limited literature on ML lifecycle automation.},
 author = {Argesanu, A. I. and Andreescu, G. D.},
 year = {2023},
 title = {Streamlining Machine Learning Workflows in Industrial Applications with CLI's and CI/CD Pipelines},
 keywords = {Artefact Design},
 pages = {389--396},
 volume = {66},
 number = {3},
 journal = {Acta Technica Napocensis Series - Applied Mathematics Mechanics and Engineering},
 file = {Argesanu, Andreescu 2023 - STREAMLINING MACHINE LEARNING WORKFLOWS:Attachments/Argesanu, Andreescu 2023 - STREAMLINING MACHINE LEARNING WORKFLOWS.pdf:application/pdf}
}


@article{Augusto.2021,
 abstract = {Background: Left ventricular maximum wall thickness (MWT) is central to diagnosis and risk stratification of hypertrophic cardiomyopathy, but human measurement is prone to variability. We developed an automated machine learning algorithm for MWT measurement and compared precision (reproducibility) with that of 11 international experts, using a dataset of patients with hypertrophic cardiomyopathy. Methods: 60 adult patients with hypertrophic cardiomyopathy, including those carrying hypertrophic cardiomyopathy gene mutations, were recruited at three institutes in the UK from August, 2018, to September, 2019: Barts Heart Centre, University College London Hospital (The Heart Hospital), and Leeds Teaching Hospitals NHS Trust. Participants had two cardiovascular magnetic resonance scans (test and retest) on the same day, ensuring no biological variability, using four cardiac MRI scanner models represented across two manufacturers and two field strengths. End-diastolic short-axis MWT was measured in test and retest by 11 international experts (from nine centres in six countries) and an automated machine learning method, which was trained to segment endocardial and epicardial contours on an independent, multicentre, multidisease dataset of 1923 patients. Machine learning MWT measurement was done with a method based on solving Laplace's equation. To assess test--retest reproducibility, we estimated the absolute test--retest MWT difference (precision), the coefficient of variation (CoV) for duplicate measurements, and the number of patients reclassified between test and retest according to different thresholds (MWT {\textgreater}15 mm and {\textgreater}30 mm). We calculated the sample size required to detect a prespecified MWT change between pairs of scans for machine learning and each expert. Findings: 1440 MWT measurements were analysed, corresponding to two scans from 60 participants by 12 observers (11 experts and machine learning). Experts differed in the MWT they measured, ranging from 14·9 mm (SD 4·2) to 19·0 mm (4·7; p{\textless}0·0001 for trend). Machine learning-measured mean MWT was 16·8 mm (4·1). Machine learning precision was superior, with a test--retest difference of 0·7 mm (0·6) compared with experts, who ranged from 1·1 mm (0·9) to 3·7 mm (2·0; p values for machine learning vs expert comparison ranging from {\textless}0·0001 to 0·0073) and a significantly lower CoV than for all experts (4·3{\%} [95{\%} CI 3·3--5·1] vs 5·7--12·1{\%} across experts). On average, 38 (64{\%}) patients were designated as having MWT greater than 15 mm by machine learning compared with 27 (45{\%}) to 50 (83{\%}) patients by experts; five (8{\%}) patients were reclassified in test--retest by machine learning compared with four (7{\%}) to 12 (20{\%}) by experts. With a cutoff point of more than 30 mm for implantable cardioverter-defibrillator, three experts would have changed recommendations between tests a total of four times, but machine learning was consistent. Using machine learning, a clinical trial to detect a 2 mm MWT change would need 2·3 times (range 1·6--4·6) fewer patients. Interpretation: In this preliminary study, machine learning MWT measurement in hypertrophic cardiomyopathy is superior to human experts with potential implications for diagnosis, risk stratification, and clinical trials. Funding: European Regional Development Fund and Barts Charity. {\copyright} 2021 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license},
 author = {Augusto, J. B. and Davies, R. H. and Bhuva, A. N. and Knott, K. D. and Seraphim, A. and Alfarih, M. and Lau, C. and Hughes, R. K. and Lopes, L. R. and Shiwani, H. and Treibel, T. A. and Gerber, B. L. and Hamilton-Craig, C. and Ntusi, N.A.B. and Pontone, G. and Desai, M. Y. and Greenwood, J. P. and Swoboda, P. P. and Captur, G. and Cavalcante, J. and Bucciarelli-Ducci, C. and Petersen, S. E. and Schelbert, E. and Manisty, C. and Moon, J. C.},
 year = {2021},
 title = {Diagnosis and risk stratification in hypertrophic cardiomyopathy using machine learning wall thickness measurement: a comparison with human test-retest performance},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098873583&doi=10.1016%2fS2589-7500%2820%2930267-3&partnerID=40&md5=8d679745e46eed5133eb37f3d507cfb5},
 keywords = {ML implementation},
 pages = {e20-e28},
 volume = {3},
 number = {1},
 journal = {The Lancet Digital Health},
 doi = {10.1016/S2589-7500(20)30267-3},
 file = {Augusto, Davies et al 2021 - Diagnosis and risk stratification:Attachments/Augusto, Davies et al 2021 - Diagnosis and risk stratification.pdf:application/pdf}
}


@proceedings{Bathen.2022,
 abstract = {This paper introduces the concept of Trustless AutoML, and proposes a framework that combines AutoML techniques with blockchain to fully decentralize the design and training process of machine learning models. The goal is to introduce full transparency and trust in the model design pipeline to establish a root-of-trust.  {\copyright} 2022 IEEE.},
 year = {2022},
 title = {Trustless AutoML for the Age of Internet of Things},
 keywords = {Artefact Design},
 editor = {Bathen, L.A.D. and Jadav, D.},
 doi = {10.1109/ICBC54727.2022.9805535},
 file = {Bathen, Jadav (Hg) 2022 - Trustless AutoML for the Age:Attachments/Bathen, Jadav (Hg) 2022 - Trustless AutoML for the Age.pdf:application/pdf}
}


@inproceedings{Bathen.2022b,
 abstract = {Machine learning (ML) and artificial intelligence (AI) technologies are becoming intrinsic in our every-day life. These technologies are present in our vehicles, our customer service departments, the applications in our smart-phones, our hospitals, to name a few. The tasks they are trying to solve have grown in complexity, thus, it is imperative we can trust not just the accuracy of the models, but the process used to design, train, and deploy them. This paper introduces the idea of TrustlessNAS, a platform for decentralized Network Architecture Search (NAS), which serves as the root-of-trust for the design, training, and validation process of NAS-based models. We propose an architecture that relies on permissioned blockchain technology to make the entire design process transparent, auditable, and trusted without the need to fully trust all the participants.  {\copyright} 2022 IEEE.},
 author = {Bathen, L.A.D. and Jadav, D.},
 title = {TrustlessNAS: Towards Trustless Network Architecture Search},
 keywords = {Artefact Design},
 pages = {117--125},
 year = {2022},
 doi = {10.1109/SOSE55356.2022.00020},
 file = {Bathen, Jadav 2022 - TrustlessNAS:Attachments/Bathen, Jadav 2022 - TrustlessNAS.pdf:application/pdf}
}


@inproceedings{Candel.2023,
 abstract = {Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. {\copyright} 2023 Association for Computational Linguistics.},
 author = {Candel, A. and McKinney, J. and Singer, P. and Pfeiffer, P. and Jeblick, M. and Lee, C. M. and Conde, M. V.},
 title = {H2O Open Ecosystem for State-of-the-art Large Language Models},
 keywords = {Artefact Design},
 pages = {82--89},
 year = {2023},
 file = {Candel, McKinney et al 2023 - H2O Open Ecosystem for State-of-the-art:Attachments/Candel, McKinney et al 2023 - H2O Open Ecosystem for State-of-the-art.pdf:application/pdf}
}


@article{Casalicchio.2019,
 abstract = {OpenML is an online machine learning platform where researchers can easily share data, machine learning tasks and experiments as well as organize them online to work and collaborate more efficiently. In this paper, we present an R package to interface with the OpenML platform and illustrate its usage in combination with the machine learning R package mlr~(Bischl et al. J Mach Learn Res 17(170):1--5, 2016). We show how the OpenML package allows R users to easily search, download and upload data sets and machine learning tasks. Furthermore, we also show how to upload results of experiments, share them with others and download results from other users. Beyond ensuring reproducibility of results, the OpenML platform automates much of the drudge work, speeds up research, facilitates collaboration and increases the users' visibility online. {\copyright} 2017, Springer-Verlag GmbH Germany.},
 author = {Casalicchio, G. and Bossek, J. and Lang, M. and Kirchhoff, D. and Kerschke, P. and Hofner, B. and Seibold, H. and Vanschoren, J. and Bischl, B.},
 year = {2019},
 title = {OpenML: An R package to connect to the machine learning platform OpenML},
 keywords = {Artefact Design},
 pages = {977--991},
 volume = {34},
 number = {3},
 journal = {Computational Statistics},
 file = {Casalicchio, Bossek et al. 2019 - OpenML An R package:Attachments/Casalicchio, Bossek et al. 2019 - OpenML An R package.pdf:application/pdf}
}


@proceedings{Chastikova.2021,
 abstract = {The article discusses the current state of technologies for automated machine learning. The development trends and the nature of the distribution model - MLaaS - are defined. There is highlighted a number of problems of automating the machine learning process, such as: Excessive simplification and specialization of tools, vagueness of implemented processes, lack of flexibility in the infrastructure hardware, using closed algorithms. As a partial or complete solution to them, we have proposed the architecture, consisting of separate modules: Models, hybridizer, learning algorithms module, testing module, user support module, and a theoretical framework. The main feature of the given architecture is its modularity, transparency and encapsulation of components. Each module is described as a separate element, implemented as an independent microservice. The paper describes the benefits of applying the given approach to the implementation of automated machine learning systems, the need to implement the given or similar standards. For each of the modules, its purposes, the tasks it solves and the implemented functionality, as well as the data necessary for the functioning and their sources are described. A general diagram showing the flows of information exchange between modules is presented. The main scenarios for the resulting system operation, as well as ways of interacting with it and the result of its operation - the generated model - are described. {\copyright} 2021 Institute of Physics Publishing. All rights reserved.},
 year = {2021},
 title = {Developing the platform model for problem solving of automated machine learning},
 keywords = {Artefact Design;Technical Review},
 volume = {2094},
 editor = {Chastikova, V. A. and Zherlitsyn, S. A.},
 doi = {10.1088/1742-6596/2094/3/032049},
 file = {Chastikova, Zherlitsyn (Hg) 2021 - Developing the platform model:Attachments/Chastikova, Zherlitsyn (Hg) 2021 - Developing the platform model.pdf:application/pdf}
}


@inproceedings{Chen.2022,
 abstract = {As machine learning is applied more widely, it is necessary to have a machine-learning platform for both infrastructure administrators and users including expert data scientists and citizen data scientists [24] to improve their productivity. However, existing machine-learning platforms are ill-equipped to address the {\textquotedbl}Machine Learning tech debts{\textquotedbl}[36] such as glue code, reproducibility, and portability. Furthermore, existing platforms only take expert data scientists into consideration, and thus they are inflexible for infrastructure administrators and non-user-friendly for citizen data scientists. We propose Submarine, a unified machine-learning platform, and takes all infrastructure administrators, expert data scientists, and citizen data scientists into consideration. Submarine has been widely used in many technology companies, including Ke.com and LinkedIn. We present two use cases in Section 5.  {\copyright} 2022 Owner/Author.},
 author = {Chen, K.-H. and Su, H.-P. and Chuang, W.-C. and Hsiao, H.-C. and Tan, W. and Tang, Z. and Liu, X. and Liang, Y. and Lo, W.-C. and Ji, W. and Hsu, B. and Hu, K. and Jian, H. and Zhou, Q. and Wang, C.-M.},
 title = {Apache submarine: A unified machine learning platform made simple},
 keywords = {Artefact Design;Case Study},
 pages = {101--108},
 year = {2022},
 file = {Chen, Su et al 2022 - Apache submarine:Attachments/Chen, Su et al 2022 - Apache submarine.pdf:application/pdf}
}


@article{Chen.2023,
 abstract = {A typical ensemble learning process typically uses a forward integration mechanism to construct the ensemble classifier with a large number of base classifiers. Based on this mechanism, it is difficult to adjust the diversity among base classifiers and optimize the structure inside ensemble since the generation process has a certain amount of randomness, which makes the performance of ensemble classifiers heavily dependent on the human design decisions. To address this issue, we proposed an automatic ensemble classifier construction method based on a dual-layer evolutionary search mechanism, which includes a tree coding-based base classifier population and a binary coding-based ensemble classifier population. Through a collaborative searching process between the two populations, the proposed method can be driven by training data to update the base classifier population and optimize the ensemble classifiers globally. To verify the effectiveness of the dual evolutionary ensemble learning method (DEEL), we tested it on 22 classification tasks from 4 data repositories. The results show that the proposed method can generate a diverse decision tree population on the training data while searching and constructing ensemble classifiers from them. Compared with 9 competitor algorithms, the proposed method achieved the best performance on 17 of 22 test tasks and improved the average accuracies by 0.97--7.65{\%} over the second place. In particular, the generated ensemble classifiers show excellent structure, which involve small number and diverse decision trees. That increases the transparency of ensembles and helps to perform interpretability analysis on them. {\copyright} 2022, The Author(s).},
 author = {Chen, H. and Zhang, G. and Pan, X. and Jia, R.},
 year = {2023},
 title = {Using dual evolutionary search to construct decision tree based ensemble classifier},
 keywords = {Artefact Design},
 pages = {1327--1345},
 volume = {9},
 number = {2},
 journal = {Complex and Intelligent Systems},
 file = {Chen, Zhang et al. 2023 - Using dual evolutionary search:Attachments/Chen, Zhang et al. 2023 - Using dual evolutionary search.pdf:application/pdf}
}


@article{Chou.2022,
 abstract = {Artificial intelligence and machine learning (AI/ML) is becoming increasingly more accessible to biomedical researchers with significant potential to transform biomedicine through optimization of highly-accurate predictive models and enabling better understanding of disease biology. Automated machine learning (AutoML) in particular is positioned to democratize artificial intelligence (AI) by reducing the amount of human input and ML expertise needed. However, successful translation of AI/ML in biomedicine requires moving beyond optimizing only for prediction accuracy and towards establishing reproducible clinical and biological inferences. This is especially challenging for clinical studies on rare disorders where the smaller patient cohorts and corresponding sample size is an obstacle for reproducible modeling results. Here, we present a model-agnostic framework to reinforce AutoML using strategies and tools of explainable and reproducible AI, including novel metrics to assess model reproducibility. The framework enables clinicians to interpret AutoML-generated models for clinical and biological verifiability and consequently integrate domain expertise during model development. We applied the framework towards spinal cord injury prognostication to optimize the intraoperative hemodynamic range during injury-related surgery and additionally identified a strong detrimental relationship between intraoperative hypertension and patient outcome. Furthermore, our analysis captured how evolving clinical practices such as faster time-to-surgery and blood pressure management affect clinical model development. Altogether, we illustrate how expert-augmented AutoML improves inferential reproducibility for biomedical discovery and can ultimately build trust in AI processes towards effective clinical integration. Copyright: {\copyright} 2022 Chou et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
 author = {Chou, A. and Torres-Espin, A. and Kyritsis, N. and {Russell Huie}, J. and Khatry, S. and Funk, J. and Hay, J. and Lofgreen, A. and Shah, R. and McCann, C. and Pascual, L. U. and Amorim, E. and Weinstein, P. R. and Manley, G. T. and Dhall, S. S. and Pan, J. Z. and Bresnahan, J. C. and Beattie, M. S. and Whetstone, W. D. and Ferguson, A. R.},
 year = {2022},
 title = {Expert-augmented automated machine learning optimizes hemodynamic predictors of spinal cord injury outcome},
 keywords = {Artefact Design},
 volume = {17},
 number = {4 April},
 journal = {PLoS ONE},
 file = {Chou, Torres-Espin et al. 2022 - Expert-augmented automated machine learning optimizes:Attachments/Chou, Torres-Espin et al. 2022 - Expert-augmented automated machine learning optimizes.pdf:application/pdf}
}


@inproceedings{Datta.2023,
 abstract = {As the field of multimedia computing has grown rapidly, so has the need for larger datasets[5] and increased modeling capacity. Navigating this complex landscape often necessitates the use of sophisticated tools and cloud architectures, which all need to be addressed before the actual research commences. Recently, AutoML, an innovation previously exclusive to tabular data, has expanded to encompass multimedia data. This development has the potential to greatly streamline the research process, allowing researchers to shift their focus from model construction to the core content of their problems. In doing so, AutoML not only optimizes resource utilization but also boosts the reproducibility of results. The aim of this tutorial is to acquaint the multimedia community with AutoML technologies, underscoring their advantages and their practical applications in the field. {\copyright} 2023 Owner/Author.},
 author = {Datta, D. and Friedland, G.},
 title = {Efficient Multimedia Computing: Unleashing the Power of AutoML},
 keywords = {Case Study;Technical Review},
 pages = {9700--9701},
 year = {2023},
 doi = {10.1145/3581783.3613858},
 file = {Datta, Friedland 2023 - Efficient Multimedia Computing:Attachments/Datta, Friedland 2023 - Efficient Multimedia Computing.pdf:application/pdf}
}


@inproceedings{Drozdal.2020,
 abstract = {We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies - qualitative interviews, a controlled experiment, and a card-sorting task - to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool. {\copyright} ACM.},
 author = {Drozdal, J. and Weisz, J. and Wang, D. and Dass, G. and Yao, B. and Zhao, C. and Muller, M. and Ju, L. and Su, H.},
 title = {Trust in AutoML},
 keywords = {Empirical Study},
 pages = {297--307},
 year = {2020},
 doi = {10.1145/3377325.3377501},
 file = {Drozdal, Weisz et al 2020 - Trust in AutoML:Attachments/Drozdal, Weisz et al 2020 - Trust in AutoML.pdf:application/pdf}
}


@inproceedings{Eimer.2023,
 abstract = {In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhead. As a result of our findings, we recommend a set of best practices for the RL community, which should result in stronger empirical results with fewer computational costs, better reproducibility, and thus faster progress. In order to encourage the adoption of these practices, we provide plug-and-play implementations of the tuning algorithms used in this paper at https://github.com/facebookresearch/how-to-autorl. {\copyright} 2023 Proceedings of Machine Learning Research. All rights reserved.},
 author = {Eimer, T. and Lindauer, M. and Raileanu, R.},
 title = {Hyperparameters in Reinforcement Learning and How To Tune Them},
 keywords = {Artefact Design},
 pages = {9104--9149},
 year = {2023},
 file = {Eimer, Lindauer et al 2023 - Hyperparameters in Reinforcement Learning:Attachments/Eimer, Lindauer et al 2023 - Hyperparameters in Reinforcement Learning.pdf:application/pdf}
}


@inproceedings{Elshawi.2022,
 abstract = {Machine learning algorithms have been widely employed in various applications and fields. Novel technologies in automated machine learning (AutoML) ease algorithm selection and hyperparameter optimization complexity. AutoML frame-works have achieved notable success in hyperparameter tuning and surpassed the performance of human experts. However, depending on such frameworks as black-box can leave machine learning practitioners without insights into the inner working of the AutoML process and hence influence their trust in the models produced. In addition, excluding humans from the loop creates several limitations. For example, most of the current AutoML frameworks ignore the user preferences on defining or controlling the search space, which consequently can impact the performance of the models produced and the acceptance of these models by the end-users. The research in the area of transparency and controllability of AutoML has attracted much interest lately, both in academia and industry. However, existing tools are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning, particularly clustering, remains a largely unexplored problem. Motivated by these shortcomings, we design and implement cSmartML-GlassBox, an interactive visualization tool that en-ables users to refine the search space of AutoML and analyze the results. cSmartML-GlassBox is equipped with a recommendation engine to recommend a time budget that is likely adequate for a new dataset to obtain well-performing pipeline. In addition, the tool supports multi-granularity visualization to enable machine learning practitioners to monitor the AutoML process, analyze the explored configurations and refine/control the search space. Furthermore, cSmartML-GlassBox is equipped with a logging mechanism such that repeated runs on the same dataset can be more effective by avoiding evaluating the same previously considered configurations. We demonstrate the effectiveness and usability of the cSmartML-GlassBox through a user evaluation study with 23 participants and an expert-based usability study based on four experts. We find that the proposed tool increases users' understanding and trust in the AutoML frameworks.  {\copyright} 2022 IEEE.},
 author = {Elshawi, R. and Sakr, S.},
 title = {cSmartML-Glassbox: Increasing Transparency and Controllability in Automated Clustering},
 keywords = {Artefact Design;Case Study},
 pages = {47--54},
 year = {2022},
 doi = {10.1109/ICDMW58026.2022.00015},
 file = {Elshawi, Sakr 2022 - cSmartML-Glassbox:Attachments/Elshawi, Sakr 2022 - cSmartML-Glassbox.pdf:application/pdf}
}


@inproceedings{Esteves.2016,
 abstract = {Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: {\textquotedbl}What is the cleanest and lowest-impact solution, i.e., the minimal effort to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?{\textquotedbl}. We introduce a novel and low-impact methodology specifically designed for code built in that context, combining Semantic Web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: MEX Vocabulary. {\copyright} 2016 ACM.},
 author = {Esteves, D. and Mendes, P. N. and Moussallem, D. and Duarte, J. C. and Zaveri, A. and Lehmann, J. and Neto, C. B. and Costa, I. and Cavalcanti, M. C.},
 title = {MEX interfaces: Automating machine learning metadata generation},
 keywords = {Artefact Design},
 pages = {17--24},
 year = {2016},
 doi = {10.1145/2993318.2993320},
 file = {Esteves, Mendes et al 2016 - MEX interfaces:Attachments/Esteves, Mendes et al 2016 - MEX interfaces.pdf:application/pdf}
}


@article{Farina.2022,
 abstract = {Background: Obesity is chronic health problem. Screening for the obesity phenotype is limited by the availability of practical methods. Methods: We determined the reproducibility and accuracy of an automated machine-learning method using smartphone camera-enabled capture and analysis of single, two-dimensional (2D) standing lateral digital images to estimate fat mass (FM) compared to dual X-ray absorptiometry (DXA) in females and males. We also report the first model to predict abdominal FM using 2D digital images. Results: Gender-specific 2D estimates of FM were significantly correlated (p {\textless} 0.001) with DXA FM values and not different (p {\textgreater} 0.05). Reproducibility of FM estimates was very high (R2 = 0.99) with high concordance (R2 = 0.99) and low absolute pure error (0.114 to 0.116 kg) and percent error (1.3 and 3{\%}). Bland--Altman plots revealed no proportional bias with limits of agreement of 4.9 to $-$4.3 kg and 3.9 to $-$4.9 kg for females and males, respectively. A novel 2D model to estimate abdominal (lumbar 2--5) FM produced high correlations (R2 = 0.99) and concordance (R2 = 0.99) compared to DXA abdominal FM values. Conclusions: A smartphone camera trained with machine learning and automated processing of 2D lateral standing digital images is an objective and valid method to estimate FM and, with proof of concept, to determine abdominal FM. It can facilitate practical identification of the obesity phenotype in adults. {\copyright} 2022 by the authors.},
 author = {Farina, G. L. and Orlandi, C. and Lukaski, H. and Nescolarde, L.},
 year = {2022},
 title = {Digital Single-Image Smartphone Assessment of Total Body Fat and Abdominal Fat Using Machine Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141640084&doi=10.3390%2fs22218365&partnerID=40&md5=3b567237dd6add2a72d6d2677c800d86},
 keywords = {ML implementation},
 volume = {22},
 number = {21},
 journal = {Sensors},
 doi = {10.3390/s22218365},
 file = {Farina, Orlandi et al. 2022 - Digital Single-Image Smartphone Assessment:Attachments/Farina, Orlandi et al. 2022 - Digital Single-Image Smartphone Assessment.pdf:application/pdf}
}


@article{Garouani.2022,
 abstract = {The Machine Learning(ML) based solutions in manufacturing industrial contexts often require skilled resources. More practical non-expert software solutions are then desired to enhance the usability of ML algorithms. The algorithm selection and configuration is one of the most difficult tasks for users like manufacturing specialists. The identification of the most appropriate algorithm in an automatic manner is among the major research challenges to achieve optimal performance of ML tools. In this paper, we present an auto-explained Automated Machine Learning tool for Big Industrial Data(AMLBID) to better cope with the prominent challenges posed by the evolution of Big Industrial Data. It is a meta-learning based decision support system for the automated selection and tuning of implied hyperparameters for ML algorithms. Moreover, the framework is equipped with an explainer module that makes the outcomes transparent and interpretable for well-performing ML systems. {\copyright} 2021 The Authors},
 author = {Garouani, M. and Ahmad, A. and Bouneffa, M. and Hamlich, M.},
 year = {2022},
 title = {AMLBID: An auto-explained Automated Machine Learning tool for Big Industrial Data},
 keywords = {Artefact Design},
 volume = {17},
 journal = {SoftwareX},
 doi = {10.1016/j.softx.2021.100919},
 file = {Garouani, Ahmad et al 2022 - AMLBID An auto-explained Automated Machine:Attachments/Garouani, Ahmad et al 2022 - AMLBID An auto-explained Automated Machine.pdf:application/pdf}
}


@inproceedings{Garouani.2023,
 abstract = {Automated machine learning (AutoML) has transformed the process of selecting optimal machine learning (ML) models by autonomously searching for the most appropriate ones and fine-tuning associated hyperparameters. This eliminates the burdensome task of trial-and-error selection and parametrization of ML algorithms. Nonetheless, the lack of transparency and explainability poses a significant challenge when using AutoML, as it hampers user trust in the system's recommendations. Consequently, users often allocate more resources to the search process, resulting in reduced efficiency of the AutoML systems. To address this challenge, we propose an interactive and explainable AutoML framework that enables users to understand the reasoning behind the recommendations and diagnose any limitations of the suggested models using various explainable AI methods. Additionally, our framework provides the possibility of automated performance refinement. To operationalize the framework, we introduce AMLExplainer, an XAI system for interactive and interpretable AutoML that visualizes and performs all stages of the proposed pipeline(s) within the widely used Bootstrap Dash environment. {\copyright} The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
 author = {Garouani, M. and Bouneffa, M.},
 title = {Unlocking the Black Box: Towards Interactive Explainable Automated Machine Learning},
 keywords = {Artefact Design},
 pages = {458--469},
 year = {2023},
 doi = {10.1007/978-3-031-48232-8{\textunderscore }42},
 file = {Garouani, Bouneffa 2023 - Unlocking the Black Box:Attachments/Garouani, Bouneffa 2023 - Unlocking the Black Box.pdf:application/pdf}
}


@article{Garouani.2023b,
 abstract = {We report a new release of the self-explainable AutoML software AMLBID. The software package is a meta-learning based AutoML decision support system that assists machine learning(ML) experts and neophytes in building well performing ML pipelines and makes the outcomes transparent and interpretable. The release 2.0 of the software introduces an enhanced efficiency of algorithms recommendation. The performance improvement is mainly achieved by the integration of the AeKNN meta-model. This implementation makes datasets meta-features more informative and further improve the accuracy. The release 2.0 also introduces the standalone AMLBIDesc, a desktop version of the AMLBID software which makes the tool more accessible to non-expert users. {\&} COPY; 2023 The Author(s). Published by Elsevier B.V. All rights reserved.},
 author = {Garouani, M. and Bouneffa, M. and Ahmad, A. and Hamlich, M.},
 year = {2023},
 title = {Version [2.0]- [AMLBID: An auto-explained Automated Machine Learning tool for Big Industrial Data]},
 keywords = {Artefact Design},
 volume = {23},
 journal = {SoftwareX},
 file = {Garouani, Bouneffa et al 2023 - Version [20]- [AMLBID:Attachments/Garouani, Bouneffa et al 2023 - Version [20]- [AMLBID.pdf:application/pdf}
}


@article{Giovanelli.2024,
 abstract = {This work is a companion reproducibility paper of the experiments and results reported in Giovanelli et al. (2022), where data pre-processing pipelines are evaluated in order to find pipeline prototypes that reduce the classification error of supervised learning algorithms. With the recent shift towards data-centric approaches, where instead of the model, the dataset is systematically changed for better model performance, data pre-processing is receiving a lot of attention. Yet, its impact over the final analysis is not widely recognized, primarily due to the lack of publicly available experiments that quantify it. To bridge this gap, this work introduces a set of reproducible experiments on the impact of data pre-processing by providing a detailed reproducibility protocol together with a software tool and a set of extensible datasets, which allow for all the experiments and results of our aforementioned work to be reproduced. We introduce a set of strongly reproducible experiments based on a collection of intermediate results, and a set of weakly reproducible experiments (Lastra-D{\i}az, 0000) that allows reproducing our end-to-end optimization process and evaluation of all the methods reported in our primary paper. The reproducibility protocol is created in Docker and tested in Windows and Linux. In brief, our primary work (i) develops a method for generating effective prototypes, as templates or logical sequences of pre-processing transformations, and (ii) instantiates the prototypes into pipelines, in the form of executable or physical sequences of actual operators that implement the respective transformations. For the first, a set of heuristic rules learned from extensive experiments are used, and for the second techniques from Automated Machine Learning (AutoML) are applied. {\copyright} 2023 Elsevier Ltd},
 author = {Giovanelli, J. and Bilalli, B. and Abell{\'o}, A. and Silva-Coira, F. and de Bernardo, G.},
 year = {2024},
 title = {Reproducible experiments for generating pre-processing pipelines for AutoETL},
 keywords = {Artefact Design;Experiment},
 volume = {120},
 journal = {Information Systems},
 file = {Giovanelli, Bilalli et al 2024 - Reproducible experiments for generating pre-processing:Attachments/Giovanelli, Bilalli et al 2024 - Reproducible experiments for generating pre-processing.pdf:application/pdf}
}


@article{Gopagoni.2020,
 abstract = {Machine learning techniques are designed to derive knowledge out of existing data. Increased computational power, use of natural language processing, image processing methods made easy creation of rich data. Good domain knowledge is required to build useful models. Uncertainty remains around choosing the right sample data, variables reduction and selection of statistical algorithm. A suitable statistical method coupled with explaining variables is critical for model building and analysis. There are multiple choices around each parameter. An automated system which could help the scientists to select an appropriate data set coupled with learning algorithm will be very useful. A freely available web-based platform, named automated machine learning tool (AMLT), is developed in this study. AMLT will automate the entire model building process. AMLT is equipped with all most commonly used variable selection methods, statistical methods both for supervised and unsupervised learning. AMLT can also do the clustering. AMLT uses statistical principles like R2 to rank the models and automatic test set validation. Tool is validated for connectivity and capability by reproducing two published works. {\copyright} Science and Information Organization.},
 author = {Gopagoni, D. and Lakshmi, P. V.},
 year = {2020},
 title = {Automated machine learning tool: The first stop for data science and statistical model building},
 keywords = {Artefact Design},
 pages = {410--418},
 number = {2},
 journal = {International Journal of Advanced Computer Science and Applications},
 file = {Gopagoni, Lakshmi 2020 - Automated machine learning tool:Attachments/Gopagoni, Lakshmi 2020 - Automated machine learning tool.pdf:application/pdf}
}


@article{Gosiewska.2021,
 abstract = {Machine learning has proved to generate useful predictive models that can and should support decision makers in many areas. The availability of tools for AutoML makes it possible to quickly create an effective but complex predictive model. However, the complexity of such models is often a major obstacle in applications, especially in terms of high-stake decisions. We are experiencing a growing number of examples where the use of black boxes leads to decisions that are harmful, unfair or simply wrong. In this paper, we show that very often we can simplify complex models without compromising their performance; however, with the benefit of much needed transparency. We propose a framework that uses elastic black boxes as supervisor models to create simpler, less opaque, yet still accurate and interpretable glass box models. The new models were created using newly engineered features extracted with the help of a supervisor model. We supply the analysis using a large-scale benchmark on several tabular data sets from the OpenML database. There are tree main results of this paper: 1) we show that extracting information from complex models may improve the performance of simpler models, 2) we question a common myth that complex predictive models outperform simpler predictive models, 3) we present a real-life application of the proposed method. {\copyright} 2021 The Authors},
 author = {Gosiewska, A. and Kozak, A. and Biecek, P.},
 year = {2021},
 title = {Simpler is better: Lifting interpretability-performance trade-off via automated feature engineering},
 keywords = {Artefact Design},
 volume = {150},
 journal = {Decision Support Systems},
 file = {Gosiewska, Kozak et al 2021 - Simpler is better:Attachments/Gosiewska, Kozak et al 2021 - Simpler is better.pdf:application/pdf}
}


@article{Guevara.2023,
 abstract = {In this research, we describe the MazeGen framework (as a maze generator), which generates navigation scenarios using Grammatical Evolution for robots or drones to navigate. The maze generator uses evolutionary algorithms to create robotic navigation scenarios with different semantic levels along a scenario profile. Grammatical Evolution is a Machine Learning technique from the Evolutionary Computing branch that uses a BNF grammar to describe the language of the possible scenario universe and a numerical encoding of individual scenarios along that grammar. Through a mapping process, it converts new numerical individuals obtained by operations on the parents' encodings to a new solution by means of grammar. In this context, the grammar describes the scenario elements and some composition rules. We also analyze associated concepts of complexity, understanding complexity as the cost of production of the scenario and skill levels needed to move around the maze. Preliminary results and statistics evidence a low correlation between complexity and the number of obstacles placed, as configurations with more difficult obstacle dispositions were found in the early stages of the evolution process and also when analyzing mazes taking into account their semantic meaning, earlier versions of the experiment not only resulted as too simplistic for the Smart Manufacturing domain, but also lacked correlation with possible real-world scenarios, as was evidenced in our experiments, where the most semantic meaning results had the lowest fitness score. They also show the emerging technology status of this approach, as we still need to find out how to reliably find solvable scenarios and characterize those belonging to the same class of equivalence. Despite being an emerging technology, MazeGen allows users to simplify the process of building configurations for smart manufacturing environments, by making it faster, more efficient, and reproducible, and it also puts the non-expert programmer in the center of the development process, as little boilerplate code is needed. {\copyright} 2023 by the authors.},
 author = {Guevara, I. H. and Margaria, T.},
 year = {2023},
 title = {MazeGen: A Low-Code Framework for Bootstrapping Robotic Navigation Scenarios for Smart Manufacturing Contexts},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159171741&doi=10.3390%2felectronics12092058&partnerID=40&md5=ea56f8e81911b6b4a5bbf3f19869c0f3},
 keywords = {Artefact Design},
 volume = {12},
 number = {9},
 journal = {Electronics (Switzerland)},
 doi = {10.3390/electronics12092058},
 file = {Guevara, Margaria 2023 - MazeGen A Low-Code Framework:Attachments/Guevara, Margaria 2023 - MazeGen A Low-Code Framework.pdf:application/pdf}
}


@article{Gundersen.2022,
 abstract = {Science is experiencing an ongoing reproducibility crisis. In light of this crisis, our objective is to investigate whether machine learning platforms provide out-of-the-box reproducibility. Our method is twofold: First, we survey machine learning platforms for whether they provide features that simplify making experiments reproducible out-of-the-box. Second, we conduct the exact same experiment on four different machine learning platforms, and by this varying the processing unit and ancillary software only. The survey shows that no machine learning platform supports the feature set described by the proposed framework while the experiment reveals statstically significant difference in results when the exact same experiment is conducted on different machine learning platforms. The surveyed machine learning platforms do not on their own enable users to achieve the full reproducibility potential of their research. Also, the machine learning platforms with most users provide less functionality for achieving it. Furthermore, results differ when executing the same experiment on the different platforms. Wrong conclusions can be inferred at the at 95{\%} confidence level. Hence, we conclude that machine learning platforms do not provide reproducibility out-of-the-box and that results generated from one machine learning platform alone cannot be fully trusted. {\copyright} 2021 The Author(s)},
 author = {Gundersen, O. E. and Shamsaliei, S. and Isdahl, R. J.},
 year = {2022},
 title = {Do machine learning platforms provide out-of-the-box reproducibility?},
 keywords = {Technical Review},
 pages = {34--47},
 volume = {126},
 journal = {Future Generation Computer Systems},
 file = {Gundersen, Shamsaliei et al. 2022 - Do machine learning platforms provide:Attachments/Gundersen, Shamsaliei et al. 2022 - Do machine learning platforms provide.pdf:application/pdf}
}


@inproceedings{Haagen.2023,
 abstract = {Machine learning (ML) algorithms are increasingly used in high-stake domains like healthcare. While ML systems frequently outperform humans in specific tasks, ensuring safety and transparency is critical in these domains. Interpretability, therefore, plays a crucial role in understanding the decision-making process, auditing and correction of ML models and establishing trust. Furthermore, there is a growing demand for automated machine learning (AutoML) to facilitate model development without expert intervention. However, the combination of interpretability and AutoML has received limited attention thus far. In this study, we propose two objective model-agnostic measures of interpretability to quantify model compactness and explanation stability, embedded within an automated interpretable ML pipeline. We experiment with a set of interpretable models on medical classification tasks reporting the proposed measures along with the predictive performances. We further conduct a user study with domain experts to evaluate the correlation between these measures and the subjective concept of interpretability. Our findings demonstrate the effectiveness of the proposed measures, affirming their success and validating their utility in creating an interpretable automated pipeline. {\copyright} 2023 CEUR-WS. All rights reserved.},
 author = {Haagen, T. and Kaya, H. and Snijder, J. and Nierman, M.},
 title = {AutoXplain: Towards Automated Interpretable Model Selection},
 keywords = {Artefact Design;Case Study},
 pages = {18--23},
 year = {2023},
 file = {Haagen, Kaya et al 2023 - AutoXplain:Attachments/Haagen, Kaya et al 2023 - AutoXplain.pdf:application/pdf}
}


@inproceedings{HerreraSanchez.2024,
 abstract = {Medical imaging classification is an area that has taken relevance in recent years due to the capability to support the medical specialist at the time of diagnosis. However, there are different instruments to obtain images from the body, and each body organ is captured differently due to its chemical composition. In this way, there are some difficulties in working with different imaging modalities. Firstly, using different functions or methods to extract features from the images is necessary. Secondly, the classification performance depends on the relevant features extracted from the images, and thirdly, it is necessary to find the classifier that performs with the minimum error. Following the concept of Auto-Machine Learning (AutoML), where the feature engineering and the hyperparameter tuning of the classifier are done automatically, this work proposes an automated approach for feature extraction and image classification based on Genetic Programming. The approach modifies the functions and their parameters and the hyperparameters for the classifier. The results show that the approach can deal with different imaging modalities, demonstrating that feature extraction is necessary to increase the classification performance. For X-ray images, it achieves a classification accuracy of 0.99, and for computerized tomography, it achieves an accuracy of 0.96. On the other hand, the solutions given by the approach are easily reproducible and easy to interpret. {\copyright} 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
 author = {Herrera-S{\'a}nchez, D. and Acosta-Mesa, H.-G. and Mezura-Montes, E.},
 title = {Auto Machine Learning Based on Genetic Programming for Medical Image Classification},
 keywords = {Artefact Design},
 pages = {349--359},
 year = {2024},
 doi = {10.1007/978-3-031-51940-6{\textunderscore }26},
 file = {Herrera-S{\'a}nchez, Acosta-Mesa et al 2024 - Auto Machine Learning Based:Attachments/Herrera-S{\'a}nchez, Acosta-Mesa et al 2024 - Auto Machine Learning Based.pdf:application/pdf}
}


@incollection{Isdahl.2019,
 abstract = {Even machine learning experiments that are fully conducted on computers are not necessarily reproducible. An increasing number of open source and commercial, closed source machine learning platforms are being developed that help address this problem. However, there is no standard for assessing and comparing which features are required to fully support reproducibility. We propose a quantitative method that alleviates this problem. Based on the proposed method we assess and compare the current state of the art machine learning platforms for how well they support making empirical results reproducible. Our results show that BEAT and Floydhub have the best support for reproducibility with Codalab and Kaggle as close contenders. The most commonly used machine learning platforms provided by the big tech companies have poor support for reproducibility. {\copyright} 2019 IEEE.},
 author = {Isdahl, R. and Gundersen, O. E.},
 title = {Out-of-the-box reproducibility: A survey of machine learning platforms},
 keywords = {Technical Review},
 pages = {86--95},
 booktitle = {2019 15th International Conference on eScience (eScience)},
 doi = {10.1109/eScience.2019.00017},
 file = {Isdahl, Gundersen 2019 - Out-of-the-box reproducibility:Attachments/Isdahl, Gundersen 2019 - Out-of-the-box reproducibility.pdf:application/pdf}
}


@proceedings{Jarrett.2021,
 abstract = {Time-series learning is the bread and butter of data-driven clinical decision support, and the recent explosion in ML research has demonstrated great potential in various healthcare settings. At the same time, medical time-series problems in the wild are challenging due to their highly composite nature: They entail design choices and interactions among components that preprocess data, impute missing values, select features, issue predictions, estimate uncertainty, and interpret models. Despite exponential growth in electronic patient data, there is a remarkable gap between the potential and realized utilization of ML for clinical research and decision support. In particular, orchestrating a real-world project lifecycle poses challenges in engineering (i.e. hard to build), evaluation (i.e. hard to assess), and efficiency (i.e. hard to optimize). Designed to address these issues simultaneously, Clairvoyance proposes a unified, end-to-end, autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical standard, and (iii) interface for optimization. Our ultimate goal lies in facilitating transparent and reproducible experimentation with complex inference workflows, providing integrated pathways for (1) personalized prediction, (2) treatment-effect estimation, and (3) information acquisition. Through illustrative examples on real-world data in outpatient, general wards, and intensive-care settings, we illustrate the applicability of the pipeline paradigm on core tasks in the healthcare journey. To the best of our knowledge, Clairvoyance is the first to demonstrate viability of a comprehensive and automatable pipeline for clinical time-series ML. {\copyright} 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.},
 year = {2021},
 title = {Clairvoyance: A pipeline toolkit for medical time series},
 keywords = {Artefact Design},
 editor = {Jarrett, D. and Yoon, J. and Bica, I. and Qian, Z. and Ercole, A. and {van der Schaar}, M.},
 file = {Jarrett, Yoon et al (Hg) 2021 - CLAIRVOYANCE:Attachments/Jarrett, Yoon et al (Hg) 2021 - CLAIRVOYANCE.pdf:application/pdf}
}


@article{Kim.2022,
 abstract = {Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to adapt the given neural architecture to improve performance while maintaining computational costs. In the architecture adaptation task, we can utilize the known high-performance architectures, and the architecture adaptation results of NAT showed performance improvements on various architectures in their experiments. However, we verified that NAT lacks reproducibility through multiple trials of experiments. Moreover, it requires an additional architecture adaptation process before network weight training. In this paper, we propose proxyless neural architecture adaptation that is reproducible and efficient. The proposed method doesn't need a proxy task for architecture adaptation. It directly improves the architecture during the conventional training process, and we can directly use the trained neural network. Moreover, the proposed method can be applied to both supervised learning and self-supervised learning. The proposed method shows stable performance improvements on various architectures and various datasets. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to various models and datasets.  {\copyright} 2013 IEEE.},
 author = {Kim, D.-G. and Lee, H.-C.},
 year = {2022},
 title = {Proxyless Neural Architecture Adaptation at Once},
 keywords = {Artefact Design},
 pages = {99745--99753},
 volume = {10},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2022.3206765},
 file = {Kim, Lee 2022 - Proxyless Neural Architecture Adaptation:Attachments/Kim, Lee 2022 - Proxyless Neural Architecture Adaptation.pdf:application/pdf}
}


@article{Lautenschlager.2020,
 abstract = {To assess the exposure of citizens to pollutants like NOx or particulate matter in urban areas, land use regression (LUR) models are a well established method. LUR models leverage information about environmental and anthropogenic factors such as cars, heating, or industry to predict air pollution in areas where no measurements have been made. However, existing approaches are often not globally applicable and require tedious hyper-parameter tuning to enable high quality predictions. In this work, we tackle these issues by introducing OpenLUR, an off-the-shelf approach for modeling air pollution that (i) works on a set of novel features solely extracted from the globally and openly available data source OpenStreetMap and (ii) is based on state-of-the-art machine learning featuring automated hyper-parameter tuning in order to minimize manual effort. We show that our proposed features are able to outperform their counterparts from local and closed sources, and illustrate how automated hyper parameter tuning can yield competitve results while alleviating the need for expert knowledge in machine learning and manual effort. Importantly, we further demonstrate the potential of the global availability of our features by applying cross-learning across different cities in order to reduce the need for a large amount of training samples. Overall, OpenLUR represents an off-the-shelf approach that facilitates easily reproducible experiments and the development of globally applicable models. {\copyright} 2020 Elsevier Ltd},
 author = {Lautenschlager, F. and Becker, M. and Kobs, K. and Steininger, M. and Davidson, P. and Krause, A. and Hotho, A.},
 year = {2020},
 title = {OpenLUR: Off-the-shelf air pollution modeling with open features and machine learning},
 keywords = {Artefact Design},
 volume = {233},
 journal = {Atmospheric Environment},
 file = {Lautenschlager, Becker et al 2020 - OpenLUR Off-the-shelf air pollution modeling:Attachments/Lautenschlager, Becker et al 2020 - OpenLUR Off-the-shelf air pollution modeling.pdf:application/pdf}
}


@proceedings{Lin.2022,
 abstract = {This paper presents some experiments to validate the design of an Automated Computer Vision (AutoCV) library for applications in scientific image understanding. AutoCV attempts to define a search space of algorithms used in common image analysis workflows and then uses a fitness function to automatically select individual algorithmic workflows for a given problem. The final goal is a semi-automated system that can assist researchers in finding specific computer vision algorithms that work for their specific research questions. As an example of this method the researchers have built the SEE-Insight tool which uses genetic algorithms to search for image analysis workflows. This tool has been used to implement an image segmentation workflow (SEE-Segment) and is being updated and modified to work with other image analysis workflows such as anchor point detection and counting. This work is motivated by analogous work being done in Automated Machine Learning (AutoML). As a way to validate the approach, this paper uses the SEE-Insight tool to recreate an AutoML solution (called SEE-Classify) and compares results to an existing AutoML solution (TPOT). As expected the existing AutoML tool worked better than the prototype SEE-Classify tool. However, the goal of this work was to learn from these well-established tools and possibly identify one of them that could be modified as a mature replacement for the core SEE-Insight search algorithm. Although this drop-in replacement was not found, reproducing the AutoML experiments in the SEE-Insight framework provided quite a few insights into best practices for moving forward with this research. {\copyright} 2022 ACM.},
 year = {2022},
 title = {Validating new Automated Computer Vision workflows to traditional Automated Machine Learning},
 keywords = {Artefact Design},
 editor = {Lin, D. and Prabagaran, P. and Colbry, D.},
 doi = {10.1145/3491418.3535174},
 file = {Lin, Prabagaran et al (Hg) 2022 - Validating new Automated Computer Vision:Attachments/Lin, Prabagaran et al (Hg) 2022 - Validating new Automated Computer Vision.pdf:application/pdf}
}


@article{Makarious.2022,
 abstract = {Personalized medicine promises individualized disease prediction and treatment. The convergence of machine learning (ML) and available multimodal data is key moving forward. We build upon previous work to deliver multimodal predictions of Parkinson's disease (PD) risk and systematically develop a model using GenoML, an automated ML package, to make improved multi-omic predictions of PD, validated in an external cohort. We investigated top features, constructed hypothesis-free disease-relevant networks, and investigated drug--gene interactions. We performed automated ML on multimodal data from the Parkinson's progression marker initiative (PPMI). After selecting the best performing algorithm, all PPMI data was used to tune the selected model. The model was validated in the Parkinson's Disease Biomarker Program (PDBP) dataset. Our initial model showed an area under the curve (AUC) of 89.72{\%} for the diagnosis of PD. The tuned model was then tested for validation on external data (PDBP, AUC 85.03{\%}). Optimizing thresholds for classification increased the diagnosis prediction accuracy and other metrics. Finally, networks were built to identify gene communities specific to PD. Combining data modalities outperforms the single biomarker paradigm. UPSIT and PRS contributed most to the predictive power of the model, but the accuracy of these are supplemented by many smaller effect transcripts and risk SNPs. Our model is best suited to identifying large groups of individuals to monitor within a health registry or biobank to prioritize for further testing. This approach allows complex predictive models to be reproducible and accessible to the community, with the package, code, and results publicly available. {\copyright} 2022, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.},
 author = {Makarious, M. B. and Leonard, H. L. and Vitale, D. and Iwaki, H. and Sargent, L. and Dadu, A. and Violich, I. and Hutchins, E. and Saffo, D. and Bandres-Ciga, S. and Kim, J. J. and Song, Y. and Maleknia, M. and Bookman, M. and Nojopranoto, W. and Campbell, R. H. and Hashemi, S. H. and Botia, J. A. and Carter, J. F. and Craig, D. W. and {van Keuren-Jensen}, K. and Morris, H. R. and Hardy, J. A. and Blauwendraat, C. and Singleton, A. B. and Faghri, F. and Nalls, M. A.},
 year = {2022},
 title = {Multi-modality machine learning predicting Parkinson's disease},
 keywords = {ML implementation},
 volume = {8},
 number = {1},
 journal = {npj Parkinson's Disease},
 doi = {10.1038/s41531-022-00288-w},
 file = {Makarious, Leonard et al. 2022 - Multi-modality machine learning predicting Parkinson's:Attachments/Makarious, Leonard et al. 2022 - Multi-modality machine learning predicting Parkinson's.pdf:application/pdf}
}


@article{Makino.2023,
 abstract = {The Stochastic Schemata Exploiter (SSE), one of the Evolutionary Algorithms, is designed to find the optimal solution of a function. SSE extracts common schemata from individual sets with high fitness and generates individuals from the common schemata. For hyper-parameter optimization, the initialization method, the schema extraction method, and the new individual generation method, which are characteristic processes in SSE, are extended. In this paper, an SSE-based multi-objective optimization for AutoML is proposed. AutoML gives good results in terms of model accuracy. However, if only model accuracy is considered, the model may be too complex. Such complex models cannot always be allowed because of the long computation time. The proposed method maximizes the stacking model accuracy and minimizes the model complexity simultaneously. When compared with existing methods, SSE has interesting features such as fewer control parameters and faster convergence properties. The visualization method makes the optimization process transparent and helps users understand the process.},
 author = {Makino, H. and Kita, E.},
 year = {2023},
 title = {Application of a Stochastic Schemata Exploiter for Multi-Objective Hyper-parameter Optimization of Machine Learning},
 keywords = {Artefact Design},
 pages = {179--213},
 volume = {17},
 number = {2},
 issn = {1867-3236},
 journal = {REVIEW OF SOCIONETWORK STRATEGIES},
 file = {Makino, Kita 2023 - Application of a Stochastic Schemata:Attachments/Makino, Kita 2023 - Application of a Stochastic Schemata.pdf:application/pdf}
}


@proceedings{Mehta.2022,
 abstract = {The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. Recently, several new NAS benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these NAS benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do not generalize to other benchmarks. To help remedy this problem, we introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research. Our code is available at https://github.com/automl/naslib. {\copyright} 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.},
 year = {2022},
 title = {Nas-bench-suite: Nas evaluation is (now) surprisingly easy},
 keywords = {Artefact Design},
 editor = {Mehta, Y. and White, C. and Zela, A. and Krishnakumar, A. and Zabergja, G. and Moradian, S. and Safari, M. and Yu, K. and Hutter, F.},
 file = {Mehta, White et al (Hg) 2022 - NAS-BENCH-SUITE:Attachments/Mehta, White et al (Hg) 2022 - NAS-BENCH-SUITE.pdf:application/pdf}
}


@incollection{Narayan.2020,
 abstract = {We present Ultron-AutoML, an open-source, distributed framework for efficient hyper-parameter optimization (HPO) of ML models. Considering that hyper-parameter optimization is compute intensive and time-consuming, the framework has been designed for reliability - the ability to successfully complete an HPO Job in a multi-tenant, failure prone environment, as well as efficiency - completing the job with minimum compute cost and wall-clock time. From a user's perspective, the framework emphasizes ease of use and customizability. The user can declaratively specify and execute an HPO Job, while ancillary tasks - containerizing and running the user's scripts, model checkpointing, monitoring progress, parallelization - are handled by the framework. At the same time, the user has complete flexibility in composing the code-base for specifying the ML model training algorithm as well as, optionally, any custom HPO algorithm. The framework supports the creation of data-pipelines to stream batches of shuffled and augmented data from a distributed file system. This comes in handy for training Deep Learning models based on self-supervised, semi-supervised or representation learning algorithms over large training datasets. We demonstrate the framework's reliability and efficiency by running a BERT pre-training job over a large training corpus using pre-emptible GPU compute targets. Despite the inherent unreliability of the underlying compute nodes, the framework is able to complete such long running jobs at 30{\%} of the cost with a marginal increase in wall-clock time. The framework also comes with a service to monitor jobs and ensures reproducibility of any result. {\copyright} 2020 IEEE.},
 author = {Narayan, S. and Krishna, C. S. and Mishra, V. and Rai, A. and Rai, H. and Bharti, C. and Sodhi, G. S. and Gupta, A. and Singh, N.},
 title = {Ultron-AutoML: An open-source, distributed, scalable framework for efficient hyper-parameter optimization},
 keywords = {Artefact Design},
 pages = {1584--1593},
 booktitle = {2020 IEEE International Conference on Big Data (Big Data)},
 doi = {10.1109/BigData50022.2020.9378071},
 file = {Narayan, Krishna et al 2020 - Ultron-AutoML:Attachments/Narayan, Krishna et al 2020 - Ultron-AutoML.pdf:application/pdf}
}


@inproceedings{Patron.2020,
 abstract = {Credit risk prediction is one of the most recurrent problems in the financial industry. While machine learning techniques such as Neural Networks can have a stunning power of prediction accuracy when done right, the results of such models are not easily interpretable and hence, are difficult to explain and to integrate into financial regulation. Building strong and robust models requires a high degree of expertise, time and testing, and as the list of the available model grows, their complexity also increases. This is why meta-heuristic search and optimization techniques are being built to tackle this task. However, this often means that such models may not be easily interpretable. This work proposes a fast, reproducible pipeline that targets these two salient needs: solid, comparable model-building and reliable interpretability. An automated machine learning process is implemented via Genetic Algorithms to obtain a locally optimal model for our data that is comparable to top Kagglers' performance for the same classification problem and then, an interpretation engine is added on top to perform sanity checks on our results and identify the most important causals of prediction. This process greatly reduces time, cost and barrier of entry for model-building while providing the reasons for prediction, which can be easily contrasted with expert knowledge to check for correctness and extracting key insights. {\copyright} 2020, Springer Nature Switzerland AG.},
 author = {Patron, G. and Leon, D. and Lopez, E. and Hernandez, G.},
 title = {An Interpretable Automated Machine Learning Credit Risk Model},
 keywords = {Artefact Design},
 pages = {16--23},
 year = {2020},
 doi = {10.1007/978-3-030-61834-6{\textunderscore }2},
 file = {Patron, Leon et al 2020 - An Interpretable Automated Machine Learning:Attachments/Patron, Leon et al 2020 - An Interpretable Automated Machine Learning.pdf:application/pdf}
}


@article{Pham.2021,
 abstract = {The success of machine learning (ML) techniques implemented in different industries heavily rely on operator expertise and domain knowledge, which is used in manually choosing an algorithm and setting up the specific algorithm parameters for a problem. Due to the manual nature of model selection and parameter tuning, it is impossible to quantify or evaluate the quality of this manual process, which in turn limits the ability to perform comparison studies between different algorithms. In this study, we propose a new hybrid approach for developing machine learning workflows to help automated algorithm selection and hyperparameter optimization. The proposed approach provides a robust, reproducible, and unbiased workflow that can be quantified and validated using different scoring metrics. We have used the most common workflows implemented in the application of artificial intelligence (AI) and ML in engineering problems including grid/random search, Bayesian search and optimization, genetic programming, and compared that with our new hybrid approach that includes the integration of Tree-based Pipeline Optimization Tool (TPOT) and Bayesian optimization. The performance of each workflow is quantified using different scoring metrics such as Pearson correlation (i.e., R2 correlation) and Mean Square Error (i.e., MSE). For this purpose, actual field data obtained from 1567 gas wells in Marcellus Shale, with 121 features from reservoir, drilling, completion, stimulation, and operation is tested using different proposed workflows. A proposed new hybrid workflow is then used to evaluate the type well used for evaluation of Marcellus shale gas production. In conclusion, our automated hybrid approach showed significant improvement in comparison to other proposed workflows using both scoring matrices. The new hybrid approach provides a practical tool that supports the automated model and hyperparameter selection, which is tested using real field data that can be implemented in solving different engineering problems using artificial intelligence and machine learning. The new hybrid model is tested in a real field and compared with conventional type wells developed by field engineers. It is found that the type well of the field is very close to P50 predictions of the field, which shows great success in the completion design of the field performed by field engineers. It also shows that the field average production could have been improved by 8{\%} if shorter cluster spacing and higher proppant loading per cluster were used during the frac jobs.},
 author = {Pham, V. V. and Fathi, E. and Belyadi, F.},
 year = {2021},
 title = {New Hybrid Approach for Developing Automated Machine Learning Workflows: A Real Case Application in Evaluation of Marcellus Shale Gas Production},
 keywords = {Artefact Design;Empirical Study},
 pages = {286--303},
 volume = {2},
 number = {3},
 issn = {2673-3994},
 journal = {FUELS},
 file = {Pham, Fathi et al. 2021 - New Hybrid Approach for Developing:Attachments/Pham, Fathi et al. 2021 - New Hybrid Approach for Developing.pdf:application/pdf}
}


@article{Rahman.2023,
 abstract = {Machine learning has been very promising in solving real problems, but the implementation involved difficulties mainly for the inexpert data scientists. Therefore, this paper presents an automated machine learning (AutoML) to simplify and accelerate the modeling tasks. Focused on Python and RapidMiner rapid modeling tools, Tree-based Pipeline Optimization Tool (TPOT) and AutoModel were used. This paper presents a comprehensive comparison between these tools with regard to the prediction accuracy and Area Under Curve (AUC) in classifying real cases of whistleblowing academic dishonesty among undergraduate students of two universities in Indonesia. Additionally, the correlations weight from demographic and Theory of Planned Behavior (TOB) attributes in the different machine learning models are also discussed. All the machine learning algorithms from TPOT and AutoModel are considerable powerful to generate good accuracy level (between 70--93{\%} of AUC) in classifying both cases of whistleblowing and non-whistleblowing on the hold-out samples from the testing process. Generally, based on the validation results of the prediction models, demographic attributes presented more importance than the TBP attributes. The findings of this study will be a great interest of many research scholars to conduct a more in-depth analysis on AutoML for many domains mainly in education and academic misconduct fields. • AutoML is the first of its kind to be empirically compared between TPOT and AutoModel in an application to predict academic dishonesty whistleblowing. • Besides accuracy performances of the AutoML, the proportion of the variance of each attribute from demographic and Theory of Planned Behavior (TPB) is also presented in the prediction models of academic dishonesty whistleblowing. • AutoML is a convenient and reproducible rapid modeling method of machine learning to be used in many kinds of prediction problem. {\copyright} 2023},
 author = {Rahman, R. A. and Masrom, S. and Mohamad, M. and Sari, E. N. and Saragih, F. and Rahman, A.S.A.},
 year = {2023},
 title = {Comparisons of automated machine learning (AutoML) in predicting whistleblowing of academic dishonesty with demographic and theory of planned behavior},
 keywords = {Case Study;ML implementation;Technical Review},
 volume = {11},
 journal = {MethodsX},
 doi = {10.1016/j.mex.2023.102364},
 file = {Rahman, Masrom et al. 2023 - Comparisons of automated machine learning:Attachments/Rahman, Masrom et al. 2023 - Comparisons of automated machine learning.pdf:application/pdf}
}


@proceedings{Saleem.2023,
 abstract = {Although numerous open-source tools exist for machine learning with tabular data, there is a scarcity of comparable resources tailored specifically for NLP. The lack of transparency in the inner workings of existing AutoNLP tools is a significant obstacle in scientific research. AutoML tools are known for their ability to perform model selection with little human intervention, improving the accuracy and reliability of the results. However, conducting a model space search among pre-trained NLP models can be computationally infeasible, making it challenging to determine the optimal NLP model for a given dataset. This research aims to enhance the performance of NLP model selection. Our approach has resulted in higher accuracy than existing methods on the dataset that was created. In our future work, we plan to benchmark our algorithm against datasets created by other researchers to validate its effectiveness. Additionally, we intend to use the same system to perform model selection among popular large language Transformer models.  {\copyright} 2023 ITMA.},
 year = {2023},
 title = {AutoNLP: A Framework for Automated Model Selection in Natural Language Processing},
 keywords = {Artefact Design},
 volume = {2023-June},
 editor = {Saleem, S. and Kumarapathirage, S.},
 doi = {10.23919/CISTI58278.2023.10212030},
 file = {Saleem, Kumarapathirage (Hg) 2023 - AutoNLP A Framework for Automated:Attachments/Saleem, Kumarapathirage (Hg) 2023 - AutoNLP A Framework for Automated.pdf:application/pdf}
}


@article{Schwen.2022,
 abstract = {Image analysis tasks in computational pathology are commonly solved using convolutional neural networks (CNNs). The selection of a suitable CNN architecture and hyperparameters is usually done through exploratory iterative optimization, which is computationally expensive and requires substantial manual work. The goal of this article is to evaluate how generic tools for neural network architecture search and hyperparameter optimization perform for common use cases in computational pathology. For this purpose, we evaluated one on-premises and one cloud-based tool for three different classification tasks for histological images: tissue classification, mutation prediction, and grading. We found that the default CNN architectures and parameterizations of the evaluated AutoML tools already yielded classification performance on par with the original publications. Hyperparameter optimization for these tasks did not substantially improve performance, despite the additional computational effort. However, performance varied substantially between classifiers obtained from individual AutoML runs due to non-deterministic effects. Generic CNN architectures and AutoML tools could thus be a viable alternative to manually optimizing CNN architectures and parametrizations. This would allow developers of software solutions for computational pathology to focus efforts on harder-to-automate tasks such as data curation. {\copyright} 2022 The Author(s)},
 author = {Schwen, L. O. and Schacherer, D. and Gei{\ss}ler, C. and Homeyer, A.},
 year = {2022},
 title = {Evaluating generic AutoML tools for computational pathology},
 keywords = {Technical Review},
 volume = {29},
 journal = {Informatics in Medicine Unlocked},
 file = {Schwen, Schacherer et al 2022 - Evaluating generic AutoML tools:Attachments/Schwen, Schacherer et al 2022 - Evaluating generic AutoML tools.pdf:application/pdf}
}


@inproceedings{Shirazi.2022,
 abstract = {Designing Computer Vision (CV) algorithms for production applications requires extensive knowledge of Machine Learning (ML), programming, and software architecture. Accordingly, keeping humans, especially domain users, in the loop of model decisions is a challenge. The algorithms created by the CV community usually omit to provide clear interpretable information on the finally selected model, except for numerical metrics. This leads to a transparency challenge, known as black-box ML, which may negatively impact the users' trust in the sensitive use cases. Furthermore, the majority of interpretable methods for giving feedback to domain users are usually made after training, i.e., proposing a final model without the user participation.In this paper, we present a Visual Interpretation-based Control (VIC) technique, which is a simple but principled model evaluation criteria. VIC offers a decision-making strategy for enabling non-experts to dictate their intuition from the most important areas in an image and incorporate this within CV pipelines. Specifically, we supervise a generative adversarial network to penalize its generator for the specified region of interest. We further validate our method through an architecture selection strategy in one of the common AutoML benchmarks.  {\copyright} 2022 IEEE.},
 author = {Shirazi, M. and Safronov, G. and Rizk, A.},
 title = {Rethinking of Domain Users Control in Computer Vision Pipelines by Customized Attention},
 keywords = {Artefact Design},
 pages = {1018--1025},
 year = {2022},
 doi = {10.1109/ICMLA55696.2022.00170},
 file = {Shirazi, Safronov et al 2022 - Rethinking of Domain Users Control:Attachments/Shirazi, Safronov et al 2022 - Rethinking of Domain Users Control.pdf:application/pdf}
}


@article{Stalring.2011,
 abstract = {Background: Machine learning has a vast range of applications. In particular, advanced machine learning methods are routinely and increasingly used in quantitative structure activity relationship (QSAR) modeling. QSAR data sets often encompass tens of thousands of compounds and the size of proprietary, as well as public data sets, is rapidly growing. Hence, there is a demand for computationally efficient machine learning algorithms, easily available to researchers without extensive machine learning knowledge. In granting the scientific principles of transparency and reproducibility, Open Source solutions are increasingly acknowledged by regulatory authorities. Thus, an Open Source state-of-the-art high performance machine learning platform, interfacing multiple, customized machine learning algorithms for both graphical programming and scripting, to be used for large scale development of QSAR models of regulatory quality, is of great value to the QSAR community. Results: This paper describes the implementation of the Open Source machine learning package AZOrange. AZOrange is specially developed to support batch generation of QSAR models in providing the full work flow of QSAR modeling, from descriptor calculation to automated model building, validation and selection. The automated work flow relies upon the customization of the machine learning algorithms and a generalized, automated model hyper-parameter selection process. Several high performance machine learning algorithms are interfaced for efficient data set specific selection of the statistical method, promoting model accuracy. Using the high performance machine learning algorithms of AZOrange does not require programming knowledge as flexible applications can be created, not only at a scripting level, but also in a graphical programming environment. Conclusions: AZOrange is a step towards meeting the needs for an Open Source high performance machine learning platform, supporting the efficient development of highly accurate QSAR models fulfilling regulatory requirements. {\copyright} 2011 St{\aa}lring et al; licensee Chemistry Central Ltd.},
 author = {St{\aa}lring, J. C. and Carlsson, L. A. and Almeida, P. and Boyer, S.},
 year = {2011},
 title = {AZOrange - High performance Open Source machine learning for QSAR modeling in a graphical programming environment},
 keywords = {Artefact Design;ML implementation},
 volume = {3},
 number = {7},
 journal = {Journal of Cheminformatics},
 file = {St{\aa}lring, Carlsson et al. 2011 - AZOrange:Attachments/St{\aa}lring, Carlsson et al. 2011 - AZOrange.pdf:application/pdf}
}


@proceedings{Sun.2023b,
 abstract = {Automated machine learning (AutoML) is envisioned to make ML techniques accessible to ordinary users. Recent work has investigated the role of humans in enhancing AutoML functionality throughout a standard ML workflow. However, it is also critical to understand how users adopt existing AutoML solutions in complex, real-world settings from a holistic perspective. To fill this gap, this study conducted semi-structured interviews of AutoML users (N = 19) focusing on understanding (1) the limitations of AutoML encountered by users in their real-world practices, (2) the strategies users adopt to cope with such limitations, and (3) how the limitations and workarounds impact their use of AutoML. Our findings reveal that users actively exercise user agency to overcome three major challenges arising from customizability, transparency, and privacy. Furthermore, users make cautious decisions about whether and how to apply AutoML on a case-by-case basis. Finally, we derive design implications for developing future AutoML solutions. {\copyright} 2023 ACM.},
 year = {2023},
 title = {AutoML in The Wild: Obstacles, Workarounds, and Expectations},
 keywords = {Empirical Study},
 editor = {Sun, Y. and Song, Q. and Gui, X. and Ma, F. and Wang, T.},
 doi = {10.1145/3544548.3581082},
 file = {Sun, Song et al (Hg) 2023 - AutoML in The Wild:Attachments/Sun, Song et al (Hg) 2023 - AutoML in The Wild.pdf:application/pdf}
}


@article{TamayoVera.2024,
 abstract = {The interplay of machine learning (ML) and deep learning (DL) within the agroclimatic domain is pivotal for addressing the multifaceted challenges posed by climate change on agriculture. This paper embarks on a systematic review to dissect the current utilization of ML and DL in agricultural research, with a pronounced emphasis on agroclimatic impacts and adaptation strategies. Our investigation reveals a dominant reliance on conventional ML models and uncovers a critical gap in the documentation of methodologies. This constrains the replicability, scalability, and adaptability of these technologies in agroclimatic research. In response to these challenges, we advocate for a strategic pivot toward Automated Machine Learning (AutoML) frameworks. AutoML not only simplifies and standardizes the model development process but also democratizes ML expertise, thereby catalyzing the advancement in agroclimatic research. The incorporation of AutoML stands to significantly enhance research scalability, adaptability, and overall performance, ushering in a new era of innovation in agricultural practices tailored to mitigate and adapt to climate change. This paper underscores the untapped potential of AutoML in revolutionizing agroclimatic research, propelling forward the development of sustainable and efficient agricultural solutions that are responsive to the evolving climate dynamics. {\copyright} 2024 by the authors.},
 author = {Tamayo-Vera, D. and Wang, X. and Mesbah, M.},
 year = {2024},
 title = {A Review of Machine Learning Techniques in Agroclimatic Studies},
 keywords = {Literature Review},
 volume = {14},
 number = {3},
 journal = {Agriculture (Switzerland)},
 file = {Tamayo-Vera, Wang et al. 2024 - A Review of Machine Learning:Attachments/Tamayo-Vera, Wang et al. 2024 - A Review of Machine Learning.pdf:application/pdf}
}


@article{Tesfaye.2022,
 abstract = {Field-scale prediction methods that use remote sensing are significant in many global projects; however, the existing methods have several limitations. In particular, the characteristics of smallholder systems pose a unique challenge in the development of reliable prediction methods. Therefore, in this study, a fast and reproducible new approach to wheat prediction is developed by combining predictors derived from optical (Sentinel-2) and radar (Sentinel-1) sensors using a diverse set of machine learning and deep learning methods under a small dataset domain. This study takes place in the wheat belt region of Ethiopia and evaluates forty-two predictors that represent the major vegetation index categories of green, water, chlorophyll, dry biomass, and VH polarization SAR indices. The study also applies field-collected agronomic data from 165 farm fields for training and validation. According to results, compared to other methods, a combined automated machine learning (AutoML) approach with a generalized linear model (GLM) showed higher performance. AutoML, which reduces training time, delivered ten influential parameters. For the combined approach, the mean RMSE of wheat yield was from 0.84 to 0.98 ton/ha using ten predictors from the test dataset, achieving a 99{\%} confidence interval. It also showed a correlation coefficient as high as 0.69 between the estimated yield and measured yield, and it was less sensitive to the small datasets used for model training and validation. A deep neural network with three hidden layers using the ten influential parameters was the second model. For this model, the mean RMSE of wheat yield was between 1.31 and 1.36 ton/ha on the test dataset, achieving a 99{\%} confidence interval. This model used 55 neurons with respective values of 0.1, 0.5, and 1 $\times$ 10$-$4 for the hidden dropout ratio, input dropout ratio, and l2 regularization. The approaches implemented in this study are fast and reproducible and beneficial to predict yield at scale. These approaches could be adapted to predict grain yields of other cereal crops grown under smallholder systems in similar global production systems. {\copyright} 2022 by the authors.},
 author = {Tesfaye, A. A. and Awoke, B. G. and Sida, T. S. and Osgood, D. E.},
 year = {2022},
 title = {Enhancing Smallholder Wheat Yield Prediction through Sensor Fusion and Phenology with Machine Learning and Deep Learning Methods},
 keywords = {ML implementation},
 volume = {12},
 number = {9},
 journal = {Agriculture (Switzerland)},
 file = {Tesfaye, Awoke et al. 2022 - Enhancing Smallholder Wheat Yield Prediction:Attachments/Tesfaye, Awoke et al. 2022 - Enhancing Smallholder Wheat Yield Prediction.pdf:application/pdf}
}


@proceedings{Wang.2019,
 abstract = {To relieve the pain of manually selecting machine learning algorithms and tuning hyperparameters, automated machine learning (AutoML) methods have been developed to automatically search for good models. Due to the huge model search space, it is impossible to try all models. Users tend to distrust automatic results and increase the search budget as much as they can, thereby undermining the efficiency of AutoML. To address these issues, we design and implement ATMSeer, an interactive visualization tool that supports users in refining the search space of AutoML and analyzing the results. To guide the design of ATMSeer, we derive a workflow of using AutoML based on interviews with machine learning experts. A multi-granularity visualization is proposed to enable users to monitor the AutoML process, analyze the searched models, and refine the search space in real time. We demonstrate the utility and usability of ATMSeer through two case studies, expert interviews, and a user study with 13 end users. {\copyright} 2019 Copyright held by the owner/author(s).},
 year = {2019},
 title = {AtmSeer: Increasing transparency and controllability in automated machine learning},
 keywords = {Artefact Design},
 editor = {Wang, Q. and Ming, Y. and Jin, Z. and Shen, Q. and Liu, D. and Smith, M. J. and Veeramachaneni, K. and Qu, H.},
 doi = {10.1145/3290605.3300911},
 file = {Wang, Ming et al (Hg) 2019 - AtmSeer Increasing transparency and controllability:Attachments/Wang, Ming et al (Hg) 2019 - AtmSeer Increasing transparency and controllability.pdf:application/pdf}
}


@proceedings{Xin.2021,
 abstract = {Efforts to make machine learning more widely accessible have led to a rapid increase in Auto-ML tools that aim to automate the process of training and deploying machine learning. To understand how Auto-ML tools are used in practice today, we performed a qualitative study with participants ranging from novice hobbyists to industry researchers who use Auto-ML tools.We present insights into the benefits and deficiencies of existing tools, as well as the respective roles of the human and automation in ML workflows. Finally, we discuss design implications for the future of Auto-ML tool development. We argue that instead of full automation being the ultimate goal of Auto-ML, designers of these tools should focus on supporting a partnership between the user and the Auto-ML tool. This means that a range of Auto-ML tools will need to be developed to support varying user goals such as simplicity, reproducibility, and reliability. {\copyright} 2021 ACM.},
 year = {2021},
 title = {Whither automl? understanding the role of automation in machine learningworkflows},
 keywords = {Empirical Study},
 editor = {Xin, D. and Wu, E. Y. and Lee, D.J.-L. and Salehi, N. and Parameswaran, A.},
 doi = {10.1145/3411764.3445306},
 file = {Xin, Wu et al (Hg) 2021 - Whither automl:Attachments/Xin, Wu et al (Hg) 2021 - Whither automl.pdf:application/pdf}
}


@inproceedings{Yan.2021,
 abstract = {While early research in neural architecture search (NAS) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-fidelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the final validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-fidelity algorithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11. {\copyright} 2021 Neural information processing systems foundation. All rights reserved.},
 author = {Yan, S. and White, C. and Savani, Y. and Hutter, F.},
 title = {NAS-Bench-x11 and the Power of Learning Curves},
 keywords = {Artefact Design},
 pages = {22534--22549},
 year = {2021},
 file = {Yan, White et al 2021 - NAS-Bench-x11 and the Power:Attachments/Yan, White et al 2021 - NAS-Bench-x11 and the Power.pdf:application/pdf}
}


@article{Yang.2020,
 abstract = {Due to the concerted efforts to utilize the microbial features to improve disease prediction capabilities, automated machine learning (AutoML) systems aiming to get rid of the tediousness in manually performing ML tasks are in great demand. Here we developed mAML, an ML model-building pipeline, which can automatically and rapidly generate optimized and interpretable models for personalized microbiome-based classification tasks in a reproducible way. The pipeline is deployed on a web-based platform, while the server is user-friendly and flexible and has been designed to be scalable according to the specific requirements. This pipeline exhibits high performance for 13 benchmark datasets including both binary and multi-class classification tasks. In addition, to facilitate the application of mAML and expand the human disease-related microbiome learning repository, we developed GMrepo ML repository (GMrepo Microbiome Learning repository) from the GMrepo database. The repository involves 120 microbiome-based classification tasks for 85 human-disease phenotypes referring to 12 429 metagenomic samples and 38 643 amplicon samples. The mAML pipeline and the GMrepo ML repository are expected to be important resources for researches in microbiology and algorithm developments. Database URL: http://lab.malab.cn/soft/mAML  {\copyright} 2020 The Author(s) 2020. Published by Oxford University Press.},
 author = {Yang, F. and Zou, Q.},
 year = {2020},
 title = {MAML: An automated machine learning pipeline with a microbiome repository for human disease classification},
 keywords = {Artefact Design},
 volume = {2020},
 journal = {Database},
 file = {Yang, Zou 2020 - MAML An automated machine learning:Attachments/Yang, Zou 2020 - MAML An automated machine learning.pdf:application/pdf}
}


@article{Yang.2024,
 abstract = {A chest X-ray radiography is still the global standard for diagnosing pneumonia. Despite several studies, doctors still have trouble correctly diagnosing and classifying pneumonia. Neural architecture search (NAS) has the potential to enhance diagnostic efficiency and accuracy. However, NAS methods fail to account for the security of data sources, and the result of model prediction cannot be communicated safely and consistently. To tackle these issues, we propose a trustworthy NAS method for pneumonia image classification using blockchain technology, which provides secure, reliable, and high-performance model automatic search and efficient data prediction capabilities. By synergistically combining NAS with blockchain technology, we enhance the transparency and interpretability of NAS-driven image classification processes, thereby safeguarding the confidentiality and integrity of sensitive medical information. Moreover, our approach automates the model construction process for pneumonia image classification, markedly reducing the reliance on manual intervention. Experimental results demonstrate that our method achieves comparable performance to state-of-the-art methods for pneumonia image classification while ensuring security. This provides a new solution for promoting medical aided diagnosis. {\copyright} 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
 author = {Yang, Y. and Wei, J. and Yu, Z. and Zhang, R.},
 year = {2024},
 title = {A trustworthy neural architecture search framework for pneumonia image classification utilizing blockchain technology},
 keywords = {Artefact Design},
 pages = {1694--1727},
 volume = {80},
 number = {2},
 journal = {Journal of Supercomputing},
 file = {Yang, Wei et al. 2024 - A trustworthy neural architecture search:Attachments/Yang, Wei et al. 2024 - A trustworthy neural architecture search.pdf:application/pdf}
}


@article{Zoller.2023,
 abstract = {In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML, an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab, experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML. We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML, leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself. {\copyright} 2023 Copyright held by the owner/author(s)},
 author = {Z{\"o}ller, M.-A. and Titov, W. and Schlegel, T. and Huber, M. F.},
 year = {2023},
 title = {XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning},
 keywords = {Artefact Design},
 volume = {13},
 number = {4},
 journal = {ACM Transactions on Interactive Intelligent Systems},
 doi = {10.1145/3625240},
 file = {Z{\"o}ller, Titov et al 2023 - XAutoML A Visual Analytics Tool:Attachments/Z{\"o}ller, Titov et al 2023 - XAutoML A Visual Analytics Tool.pdf:application/pdf}
}


