% This file was created with Citavi 6.18.0.1

@article{AbdelHafez.2022,
 abstract = {Background: Unfractionated heparin (UFH) is an anticoagulant drug that is considered a high-risk medication because an excessive dose can cause bleeding, whereas an insufficient dose can lead to a recurrent embolic event. Therapeutic response to the initiation of intravenous UFH is monitored using activated partial thromboplastin time (aPTT) as a measure of blood clotting time. Clinicians iteratively adjust the dose of UFH toward a target, indication-defined therapeutic aPTT range using nomograms, but this process can be imprecise and can take {\textgreater}= 36 hours to achieve the target range. Thus, a more efficient approach is required.Objective: In this study, we aimed to develop and validate a machine learning (ML) algorithm to predict aPTT within 12 hours after a specified bolus and maintenance dose of UFH.Methods: This was a retrospective cohort study of 3019 patient episodes of care from January 2017 to August 2020 using data collected from electronic health records of 5 hospitals in Queensland, Australia. Data from 4 hospitals were used to build and test ensemble models using cross-validation, whereas data from the fifth hospital were used for external validation. We built 2 ML models: a regression model to predict the aPTT value after a UFH bolus dose and a multiclass model to predict the aPTT, classified as subtherapeutic (aPTT {\textless}70 seconds), therapeutic (aPTT 70-100 seconds), or supratherapeutic (aPTT {\textgreater}100 seconds). Modeling was performed using Driverless AI (H2O), an automated ML tool, and 17 different experiments were iteratively conducted to optimize model accuracy.Results: In predicting aPTT, the best performing model was an ensemble with 4x LightGBM models with a root mean square error of 31.35 (SD 1.37). In predicting the aPTT class using a repurposed data set, the best performing ensemble model achieved an accuracy of 0.599 (SD 0.0289) and an area under the receiver operating characteristic curve of 0.735. External validation yielded similar results: root mean square error of 30.52 (SD 1.29) for the aPTT prediction model, and accuracy of 0.568 (SD 0.0315) and area under the receiver operating characteristic curve of 0.724 for the aPTT multiclassification model.Conclusions: To the best of our knowledge, this is the first ML model applied to intravenous UFH dosing that has been developed and externally validated in a multisite adult general medical and surgical inpatient setting. We present the processes of data collection, preparation, and feature engineering for replication.},
 author = {Abdel-Hafez, A. and Scott, I. A. and Falconer, N. and Canaris, S. and Bonilla, O. and Marxen, S. and {van Garderen}, A. and Barras, M.},
 year = {2022},
 title = {Predicting Therapeutic Response to Unfractionated Heparin Therapy: Machine Learning Approach},
 keywords = {ML implementation},
 volume = {11},
 number = {2},
 issn = {1929-073X},
 journal = {INTERACTIVE JOURNAL OF MEDICAL RESEARCH},
 doi = {10.2196/34533}
}


@article{Abecasis.2023,
 abstract = {AIMS: Myocardial fibrosis (MF) is a common pathological process in a wide range of cardiovascular diseases. Its quantity has diagnostic and prognostic relevance. We aimed to assess if the complementary use of an automated artificial intelligence software might improve the precision of the pathologist´s quantification of MF on endomyocardial biopsies (EMB). Methods and results: Intraoperative EMB samples from 30 patients with severe aortic stenosis submitted to surgical aortic valve replacement were analysed. Tissue sections were stained with Masson´s trichrome for collagen/fibrosis and whole slide images (WSI) from the experimental glass slides were obtained at a resolution of 0.5 \textgreek{m}m using a digital microscopic scanner. Three experienced pathologists made a first quantification of MF excluding the subendocardium. After two weeks, an algorithm for Masson´s trichrome brightfield WSI (at QuPath software) was applied and the automatic quantification was revealed to the pathologists, who were asked to reassess MF, blinded to their first evaluation. The impact of the automatic algorithm on the inter-observer agreement was evaluated using Bland-Altman type methodology. Median values of MF on EMB were 8.33{\%} [IQR 5.00-12.08{\%}] and 13.60{\%} [IQR 7.32-21.2{\%}], respectively for the first pathologist´s and automatic algorithm quantification, being highly correlated (R2: 0.79; p {\textless} 0.001). Interobserver discordance was relevant, particularly for higher percentages of MF. The knowledge of the automatic quantification significantly improved the overall pathologist´s agreement, which became unaffected by the degree of MF severity. Conclusions: The use of an automated artificial intelligence software for MF quantification on EMB samples improves the reproducibility of measurements by experienced pathologists. By improving the reliability of the quantification of myocardial tissue components, this adjunctive tool may facilitate the implementation of imaging-pathology correlation studies. {\copyright} 2023 Elsevier Inc.},
 author = {Abecasis, J. and Cortez-Dias, N. and Pinto, D. G. and Lopes, P. and Madeira, M. and Ramos, S. and Gil, V. and Cardim, N. and F{\'e}lix, A.},
 year = {2023},
 title = {Quantitative assessment of myocardial fibrosis by digital image analysis: An adjunctive tool for pathologist ``ground truth''},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159617310&doi=10.1016%2fj.carpath.2023.107541&partnerID=40&md5=0eda2208bfe2b1049b48bdb9cb0f7aae},
 keywords = {ML implementation},
 volume = {65},
 journal = {Cardiovascular Pathology},
 doi = {10.1016/j.carpath.2023.107541},
 file = {Abecasis, Cortez-Dias et al 2023 - Quantitative assessment of myocardial fibrosis:Attachments/Abecasis, Cortez-Dias et al 2023 - Quantitative assessment of myocardial fibrosis.pdf:application/pdf}
}


@article{Adje.2022,
 abstract = {Direct-acting antivirals (DAAs) achieve high hepatitis C virus (HCV) cure rates and are forgiving to missed doses, but adherence--efficacy relationships have not been well defined. Traditional adherence measures (e.g. pill counts, self-report and pharmacy refills) over-estimate medication adherence. Newer technology-based tools have been used to provide more objective adherence data. Herein, electronic medication diaries (e-diaries), medication events monitoring system (MEMS{\circledR}) caps, electronic blister packs, electronic pill boxes, video-based directly observed therapy (vDOT), artificial intelligence platforms (AIPs), and ingestible sensor systems are described, and compared based on existing studies using DAA. Percent adherence, predictors of adherence, and HCV cure rates utilizing these technologies are included. DAA adherence with e-diaries was 95--96{\%}, MEMS{\circledR} caps and ingestible biosensors were between 95{\%} and 97{\%}, blister pack weekly dosing ranged 73--98{\%}, and daily dosing 73--94{\%}, whereas electronic pill boxes ranged between 39{\%} and 89{\%}, vDOT was 98{\%} and AIP 91--96{\%}. Despite a wide range of adherence, high sustained virologic response (SVR) rates (86--100{\%}) were observed across all studies utilizing these different technology-based tools. Current data support the forgiveness of DAA therapies to missed doses using tools that provide more quantitative adherence measures compared with self-report and provide insight on adherence--efficacy relationships for contemporary DAA. {\copyright} The Author(s), 2022.},
 author = {Adje, Y. H. and Brooks, K. M. and Castillo-Mancilla, J. R. and Wyles, D. L. and Anderson, P. L. and Kiser, J. J.},
 year = {2022},
 title = {The use of technology-based adherence monitoring in the treatment of hepatitis C virus},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130303834&doi=10.1177%2f20499361221095664&partnerID=40&md5=6d0ebdc8ef6b74c95b1e35c0a8905c7d},
 keywords = {Empirical Study},
 volume = {9},
 journal = {Therapeutic Advances in Infectious Disease},
 doi = {10.1177/20499361221095664},
 file = {Adje, Brooks et al. 2022 - The use of technology-based adherence:Attachments/Adje, Brooks et al. 2022 - The use of technology-based adherence.pdf:application/pdf}
}


@article{Adlung.2021,
 abstract = {Machine learning is increasingly integrated into clinical practice, with applications ranging from pre-clinical data processing, bedside diagnosis assistance, patient stratification, treatment decision making, and early warning as part of primary and secondary prevention. However, a multitude of technological, medical, and ethical considerations are critical in machine-learning utilization, including the necessity for careful validation of machine-learning-based technologies in real-life contexts, unbiased evaluation of benefits and risks, and avoidance of technological over-dependence and associated loss of clinical, ethical, and social-related decision-making capacities. Other challenges include the need for careful benchmarking and external validations, dissemination of end-user knowledge from computational experts to field users, and responsible code and data sharing, enabling transparent assessment of pipelines. In this review, we highlight key promises and achievements in integration of machine-learning platforms into clinical medicine while highlighting limitations, pitfalls, and challenges toward enhanced integration of learning systems into the medical realm. {\copyright} 2021 Elsevier Inc.},
 author = {Adlung, L. and Cohen, Y. and Mor, U. and Elinav, E.},
 year = {2021},
 title = {Machine learning in clinical decision making},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113773331&doi=10.1016%2fj.medj.2021.04.006&partnerID=40&md5=96e058558560599a6a627147b48301e7},
 keywords = {Literature Review},
 pages = {642--665},
 volume = {2},
 number = {6},
 journal = {Med},
 doi = {10.1016/j.medj.2021.04.006},
 file = {Adlung, Cohen et al 2021 - Machine learning in clinical decision:Attachments/Adlung, Cohen et al 2021 - Machine learning in clinical decision.pdf:application/pdf}
}


@inproceedings{Afzal.2021,
 abstract = {Data exploration and quality analysis is an important yet tedious process in the AI pipeline. Current data cleaning and data readiness assessment practices for machine learning tasks are mostly conducted in an arbitrary manner which limits their reuse and often results in loss of productivity. We introduce the concept of a Data Readiness Report as accompanying documentation to a dataset that allows data consumers to get detailed insights into the quality of data. Data characteristics and challenges on various quality dimensions are identified and documented, keeping in mind the principles of transparency and explainability. The Data Readiness Report also serves as a record of all data assessment operations, including applied transformations. This provides a detailed lineage for data governance and management. In effect, the report captures and documents the actions taken by various personas in a data readiness and assessment workflow. Over time this becomes a repository of best practices and can potentially drive a recommendation system for building automated data readiness workflows on the lines of AutoML [1]. The data readiness report could serve as a valuable asset for organizing and operationalizing data in a Data-as-a-service model as it augments the trust and reliability of the datasets. We anticipate that together with the Datasheets [2], Dataset Nutrition Label [3], FactSheets [4] and Model Cards [5], the Data Readiness Report completes the AI documentation pipeline and increases trust and re-useability of data. {\copyright}2021 IEEE},
 author = {Afzal, S. and Rajmohan, C. and Kesarwani, M. and Mehta, S. and Patel, H.},
 title = {Data readiness report},
 keywords = {Artefact Design},
 pages = {42--51},
 year = {2021},
 doi = {10.1109/SMDS53860.2021.00016},
 file = {Afzal, Rajmohan et al 2021 - Data readiness report:Attachments/Afzal, Rajmohan et al 2021 - Data readiness report.pdf:application/pdf}
}


@article{Alaa.2019,
 abstract = {Background Identifying people at risk of cardiovascular diseases (CVD) is a cornerstone of preventative cardiology. Risk prediction models currently recommended by clinical guidelines are typically based on a limited number of predictors with sub-optimal performance across all patient groups. Data-driven techniques based on machine learning (ML) might improve the performance of risk predictions by agnostically discovering novel risk predictors and learning the complex interactions between them. We tested (1) whether ML techniques based on a state-of-the-art automated ML framework (AutoPrognosis) could improve CVD risk prediction compared to traditional approaches, and (2) whether considering non-traditional variables could increase the accuracy of CVD risk predictions. Methods and findings Using data on 423,604 participants without CVD at baseline in UK Biobank, we developed a ML-based model for predicting CVD risk based on 473 available variables. Our ML-based model was derived using AutoPrognosis, an algorithmic tool that automatically selects and tunes ensembles of ML modeling pipelines (comprising data imputation, feature processing, classification and calibration algorithms). We compared our model with a well-established risk prediction algorithm based on conventional CVD risk factors (Framingham score), a Cox proportional hazards (PH) model based on familiar risk factors (i.e, age, gender, smoking status, systolic blood pressure, history of diabetes, reception of treatments for hypertension and body mass index), and a Cox PH model based on all of the 473 available variables. Predictive performances were assessed using area under the receiver operating characteristic curve (AUC-ROC). Overall, our AutoPrognosis model improved risk prediction (AUC-ROC: 0.774, 95{\%} CI: 0.768-0.780) compared to Framingham score (AUC-ROC: 0.724, 95{\%} CI: 0.720-0.728, p {\textless} 0.001), Cox PH model with conventional risk factors (AUC-ROC: 0.734, 95{\%} CI: 0.729-0.739, p {\textless} 0.001), and Cox PH model with all UK Biobank variables (AUC-ROC: 0.758, 95{\%} CI: 0.753-0.763, p {\textless} 0.001). Out of 4,801 CVD cases recorded within 5 years of baseline, AutoPrognosis was able to correctly predict 368 more cases compared to the Framingham score. Our AutoPrognosis model included predictors that are not usually considered in existing risk prediction models, such as the individuals' usual walking pace and their self-reported overall health rating. Furthermore, our model improved risk prediction in potentially relevant sub-populations, such as in individuals with history of diabetes. We also highlight the relative benefits accrued from including more information into a predictive model (information gain) as compared to the benefits of using more complex models (modeling gain). Conclusions Our AutoPrognosis model improves the accuracy of CVD risk prediction in the UK Biobank population. This approach performs well in traditionally poorly served patient subgroups. Additionally, AutoPrognosis uncovered novel predictors for CVD disease that may now be tested in prospective studies. We found that the ``information gain'' achieved by considering more risk factors in the predictive model was significantly higher than the ``modeling gain'' achieved by adopting complex predictive models. {\copyright} 2019 Alaa et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
 author = {Alaa, A. M. and Bolton, T. and Angelantonio, E. D. and Rudd, J.H.F. and {van der Schaar}, M.},
 year = {2019},
 title = {Cardiovascular disease risk prediction using automated machine learning: A prospective study of 423,604 UK Biobank participants},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065908050&doi=10.1371%2fjournal.pone.0213653&partnerID=40&md5=273441d8abae9d1d6e8193aab9b73223},
 keywords = {ML implementation},
 volume = {14},
 number = {5},
 journal = {PLoS ONE},
 doi = {10.1371/journal.pone.0213653},
 file = {Alaa, Bolton et al. 2019 - Cardiovascular disease risk prediction using:Attachments/Alaa, Bolton et al. 2019 - Cardiovascular disease risk prediction using.pdf:application/pdf}
}


@article{Alabed.2022,
 abstract = {Background: There has been a rapid increase in the number of Artificial Intelligence (AI) studies of cardiac MRI (CMR) segmentation aiming to automate image analysis. However, advancement and clinical translation in this field depend on researchers presenting their work in a transparent and reproducible manner. This systematic review aimed to evaluate the quality of reporting in AI studies involving CMR segmentation. Methods: MEDLINE and EMBASE were searched for AI CMR segmentation studies in April 2022. Any fully automated AI method for segmentation of cardiac chambers, myocardium or scar on CMR was considered for inclusion. For each study, compliance with the Checklist for Artificial Intelligence in Medical Imaging (CLAIM) was assessed. The CLAIM criteria were grouped into study, dataset, model and performance description domains. Results: 209 studies published between 2012 and 2022 were included in the analysis. Studies were mainly published in technical journals (58{\%}), with the majority (57{\%}) published since 2019. Studies were from 37 different countries, with most from China (26{\%}), the United States (18{\%}) and the United Kingdom (11{\%}). Short axis CMR images were most frequently used (70{\%}), with the left ventricle the most commonly segmented cardiac structure (49{\%}). Median compliance of studies with CLAIM was 67{\%} (IQR 59--73{\%}). Median compliance was highest for the model description domain (100{\%}, IQR 80--100{\%}) and lower for the study (71{\%}, IQR 63--86{\%}), dataset (63{\%}, IQR 50--67{\%}) and performance (60{\%}, IQR 50--70{\%}) description domains. Conclusion: This systematic review highlights important gaps in the literature of CMR studies using AI. We identified key items missing---most strikingly poor description of patients included in the training and validation of AI models and inadequate model failure analysis---that limit the transparency, reproducibility and hence validity of published AI studies. This review may support closer adherence to established frameworks for reporting standards and presents recommendations for improving the quality of reporting in this field. Systematic Review Registration: [www.crd.york.ac.uk/prospero/], identifier [CRD42022279214]. Copyright {\copyright} 2022 Alabed, Maiter, Salehi, Mahmood, Daniel, Jenkins, Goodlad, Sharkey, Mamalakis, Rakocevic, Dwivedi, Assadi, Wild, Lu, O'Regan, van der Geest, Garg and Swift.},
 author = {Alabed, S. and Maiter, A. and Salehi, M. and Mahmood, A. and Daniel, S. and Jenkins, S. and Goodlad, M. and Sharkey, M. and Mamalakis, M. and Rakocevic, V. and Dwivedi, K. and Assadi, H. and Wild, J. M. and Lu, H. and O'Regan, D. P. and {van der Geest}, R. J. and Garg, P. and Swift, A. J.},
 year = {2022},
 title = {Quality of reporting in AI cardiac MRI segmentation studies -- A systematic review and recommendations for future studies},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135194839&doi=10.3389%2ffcvm.2022.956811&partnerID=40&md5=975b0f3572eebcde9c33c29a551a685b},
 keywords = {Literature Review},
 volume = {9},
 journal = {Frontiers in Cardiovascular Medicine},
 doi = {10.3389/fcvm.2022.956811},
 file = {Alabed, Maiter et al. 2022 - Quality of reporting in AI:Attachments/Alabed, Maiter et al. 2022 - Quality of reporting in AI.pdf:application/pdf}
}


@article{Alcobaca.2020,
 abstract = {Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments. {\copyright} 2020 Edesio Alcoba{\c{c}}a, Felipe Siqueira, Adriano Rivolli, Lu{\'i}s P. F. Garcia, Jefferson T. Oliva, Andr{\'e} C. P. L. F. de Carvalho.},
 author = {Alcoba{\c{c}}a, E. and Siqueira, F. and Rivolli, A. and Garcia, L.P.F. and Oliva, J. T. and de Carvalho, A.C.P.L.F.},
 year = {2020},
 title = {MFE: Towards reproducible meta-feature extraction},
 keywords = {Artefact Design},
 volume = {21},
 journal = {Journal of Machine Learning Research},
 file = {Alcoba{\c{c}}a, Siqueira et al 2020 - MFE Towards reproducible meta-feature extraction:Attachments/Alcoba{\c{c}}a, Siqueira et al 2020 - MFE Towards reproducible meta-feature extraction.pdf:application/pdf}
}


@proceedings{Ali.2023,
 abstract = {Recently, blockchain technology has become popular. Through a highly secure, decentralised system enabled by this technology, anyone can interact securely without the need for a middleman. Machine learning can help with many of the constraints that blockchain-based systems have in addition to its own strengths. Blockchain technology and machine learning together have the potential to produce very effective and beneficial outcomes. Hence, when blockchain and ML converge, they surely would benefit from each other. Blockchain can enhance the security of ML platforms, and ML can provide automation and optimization to blockchain solutions. In this work, we advocate the importance of enhancing blockchain with ML algorithms, as a proof of principle we address the issue of secure and intelligent e-voting. The use of blockchain technology has brought tremendous different application domains, e-voting is one of them. Most existing e-Voting systems require central authority during the process of authentication and verification of the voter. In this paper, we propose a safe online voting approach based on blockchain and ML to provide a solution to this issue. We use blockchain to ensure integrity and transparency of the votes, and ML for automating the verification process of eligible voters based on AI-for face authentication. The proposed solution offers automation, security, and mobility to the voting system. {\copyright} 2023 IEEE.},
 year = {2023},
 title = {Blockchain-Based Online E-voting System},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153878258&doi=10.1109%2fICSCA57840.2023.10087767&partnerID=40&md5=f6a89e84b9b09b517f051f14d5e38d0b},
 editor = {Ali, Y.A.F. and Ahmed, O.T.M. and Diab, M.A.M. and Sayed, M.A.E. and {Abd Elaziz}, M. K. and Aboshosha, B. W.},
 doi = {10.1109/ICSCA57840.2023.10087767}
}


@article{AlRashdan.2020,
 abstract = {Mobile terminal (MT) localization based on the fingerprint approach is a strong contender solution for utilization in microcells urban environments and indoor settings that suffer from severe multipath and signal degradation. In this paper, we investigate and evaluate the performance of thirteen machine learning (ML) algorithms (including multi-target algorithms) employed in conjunction with fingerprint based MT localization for distributed massive multiple input multiple-output (DM-MIMO) wireless systems configurations. The fingerprints will rely solely on the received signal strengths (RSS) from the single-antenna MT collected at each of the receive antenna elements of the massive MIMO base station. The performance is evaluated through numerical simulations incorporating practical millimeter-wave signal propagation models suited for 5G wireless systems in combination with ray-tracing techniques, and in conjunction with the 3D OpenStreetMap to replicate real-life environments. In addition, the ML computational platform, and implementation of the proposed framework was selected with a focus on efficiently handling the anticipated big data that could be generated from a typical 5G network with expected large subscriber cell density (1 million/km2). To that end, an Apache Spark based ML platform is proposed and employed. Several DM-MIMO system topologies and configuration parameters combinations affecting MT localization were investigated to analyze performance. Numerical simulation results demonstrated that the location of a MT could be effectively predicted by means of a subset of the collection of considered ML algorithms. The obtained results of MT localization performance evaluation metrics served to identify an optimum ML algorithm and methodology for employment in DM-MIMO systems.  {\copyright} 2013 IEEE.},
 author = {Al-Rashdan, W. Y. and Tahat, A.},
 year = {2020},
 title = {A Comparative Performance Evaluation of Machine Learning Algorithms for Fingerprinting Based Localization in DM-MIMO Wireless Systems Relying on Big Data Techniques},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087274100&doi=10.1109%2fACCESS.2020.3001912&partnerID=40&md5=acdf43a020d1ee4847d635cc639307e3},
 keywords = {ML implementation},
 pages = {109522--109534},
 volume = {8},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2020.3001912},
 file = {Al-Rashdan, Tahat 2020 - A Comparative Performance Evaluation:Attachments/Al-Rashdan, Tahat 2020 - A Comparative Performance Evaluation.pdf:application/pdf}
}


@proceedings{Andrei.2018,
 abstract = {Optimization theory is a widely-used field of mathematics that can be applied to different tasks: pure engineering problems (e.g., obtaining optimal wing shape), control synthesis tasks (e.g., determination of optimal guidance of aircraft), and even machine learning (e.g., training procedures of neural networks). Currently mostly all applied software systems support optimization procedures in a very limited form. This fact leads to several problems: black-box effect (i.e., there is no opportunity to explore source code, modify it, or simply verify), no code reuse (i.e., implemented procedures are accessible only within software that includes it), limitation modern optimization algorithm application (i.e., number of optimization algorithms increases but most of them were verified only on synthetic tests). Also, it should be noted that all mentioned problems lead to so-called reproducibility crisis. The main idea of this work is to suggest an Open-Source Optimization Library Extremum (OSOL Extremum) with wide API features and to show how it can be applied to find optimal control of a Simulink model that represents behavior of a satellite. {\copyright} 2018 IEEE.},
 year = {2018},
 title = {Extremum-Open-Source Optimization Library and its Application to Optimal Control Synthesis Problem},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060892910&doi=10.1109%2fINFORINO.2018.8581745&partnerID=40&md5=dcc9513daba4245809ead611eeb8a617},
 keywords = {Artefact Design},
 editor = {Andrei, P. and Valentin, P.},
 doi = {10.1109/INFORINO.2018.8581745}
}


@article{Apostolova.2014,
 abstract = {Biomarkers are the only feasible way to detect and monitor presymptomatic Alzheimer's disease (AD). No single biomarker can predict future cognitive decline with an acceptable level of accuracy. In addition to designing powerful multimodal diagnostic platforms, a careful investigation of the major sources of disease heterogeneity and their influence on biomarker changes is needed. Here we investigated the accuracy of a novel multimodal biomarker classifier for differentiating cognitively normal (NC), mild cognitive impairment (MCI) and AD subjects with and without stratification by ApoE4 genotype. 111 NC, 182 MCI and 95 AD ADNI participants provided both structural MRI and CSF data at baseline. We used an automated machine-learning classifier to test the ability of hippocampal volume and CSF A\textgreek{b}, t-tau and p-tau levels, both separately and in combination, to differentiate NC, MCI and AD subjects, and predict conversion. We hypothesized that the combined hippocampal/CSF biomarker classifier model would achieve the highest accuracy in differentiating between the three diagnostic groups and that ApoE4 genotype will affect both diagnostic accuracy and biomarker selection. The combined hippocampal/CSF classifier performed better than hippocampus-only classifier in differentiating NC from MCI and NC from AD. It also outperformed the CSF-only classifier in differentiating NC vs. AD. Our amyloid marker played a role in discriminating NC from MCI or AD but not for MCI vs. AD. Neurodegenerative markers contributed to accurate discrimination of AD from NC and MCI but not NC from MCI. Classifiers predicting MCI conversion performed well only after ApoE4 stratification. Hippocampal volume and sex achieved AUC = 0.68 for predicting conversion in the ApoE4-positive MCI, while CSF p-tau, education and sex achieved AUC = 0.89 for predicting conversion in ApoE4-negative MCI. These observations support the proposed biomarker trajectory in AD, which postulates that amyloid markers become abnormal early in the disease course while markers of neurodegeneration become abnormal later in the disease course and suggests that ApoE4 could be at least partially responsible for some of the observed disease heterogeneity. {\copyright} 2013 The Authors.},
 author = {Apostolova, L. G. and Hwang, K. S. and Kohannim, O. and Avila, D. and Elashoff, D. and {Jack Jr.}, C. R. and Shaw, L. and Trojanowski, J. Q. and Weiner, M. W. and Thompson, P. M.},
 year = {2014},
 title = {ApoE4 effects on automated diagnostic classifiers for mild cognitive impairment and Alzheimer's disease},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896512293&doi=10.1016%2fj.nicl.2013.12.012&partnerID=40&md5=2ef9ca6d97a431a1eb8f73e24c768ef3},
 keywords = {ML implementation},
 pages = {461--472},
 volume = {4},
 journal = {NeuroImage: Clinical},
 doi = {10.1016/j.nicl.2013.12.012},
 file = {Apostolova, Hwang et al. 2014 - ApoE4 effects on automated diagnostic:Attachments/Apostolova, Hwang et al. 2014 - ApoE4 effects on automated diagnostic.pdf:application/pdf}
}


@article{Araki.2017,
 abstract = {Stroke risk stratification based on grayscale morphology of the ultrasound carotid wall has recently been shown to have a promise in classification of high risk versus low risk plaque or symptomatic versus asymptomatic plaques. In previous studies, this stratification has been mainly based on analysis of the far wall of the carotid artery. Due to the multifocal nature of atherosclerotic disease, the plaque growth is not restricted to the far wall alone. This paper presents a new approach for stroke risk assessment by integrating assessment of both the near and far walls of the carotid artery using grayscale morphology of the plaque. Further, this paper presents a scientific validation system for stroke risk assessment. Both these innovations have never been presented before. The methodology consists of an automated segmentation system of the near wall and far wall regions in grayscale carotid B-mode ultrasound scans. Sixteen grayscale texture features are computed, and fed into the machine learning system. The training system utilizes the lumen diameter to create ground truth labels for the stratification of stroke risk. The cross-validation procedure is adapted in order to obtain the machine learning testing classification accuracy through the use of three sets of partition protocols: (5, 10, and Jack Knife). The mean classification accuracy over all the sets of partition protocols for the automated system in the far and near walls is 95.08{\%} and 93.47{\%}, respectively. The corresponding accuracies for the manual system are 94.06{\%} and 92.02{\%}, respectively. The precision of merit of the automated machine learning system when compared against manual risk assessment system are 98.05{\%} and 97.53{\%} for the far and near walls, respectively. The ROC of the risk assessment system for the far and near walls is close to 1.0 demonstrating high accuracy. {\copyright} 2016 Elsevier Ltd},
 author = {Araki, T. and Jain, P. K. and Suri, H. S. and Londhe, N. D. and Ikeda, N. and El-Baz, A. and Shrivastava, V. K. and Saba, L. and Nicolaides, A. and Shafique, S. and Laird, J. R. and Gupta, A. and Suri, J. S.},
 year = {2017},
 title = {Stroke Risk Stratification and its Validation using Ultrasonic Echolucent Carotid Wall Plaque Morphology: A Machine Learning Paradigm},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000741772&doi=10.1016%2fj.compbiomed.2016.11.011&partnerID=40&md5=09bfe72f8d9672185e45b3773a91664c},
 keywords = {ML implementation},
 pages = {77--96},
 volume = {80},
 journal = {Computers in Biology and Medicine},
 doi = {10.1016/j.compbiomed.2016.11.011}
}


@article{Archila.2022,
 abstract = {Background: In an attempt to provide quantitative, reproducible, and standardized analyses in cases of eosinophilic esophagitis (EoE), we have developed an artificial intelligence (AI) digital pathology model for the evaluation of histologic features in the EoE/esophageal eosinophilia spectrum. Here, we describe the development and technical validation of this novel AI tool. Methods: A total of 10 726 objects and 56.2 mm2 of semantic segmentation areas were annotated on whole-slide images, utilizing a cloud-based, deep learning artificial intelligence platform (Aiforia Technologies, Helsinki, Finland). Our training set consisted of 40 carefully selected digitized esophageal biopsy slides which contained the full spectrum of changes typically seen in the setting of esophageal eosinophilia, ranging from normal mucosa to severe abnormalities with regard to each specific features included in our model. A subset of cases was reserved as independent ``test sets'' in order to assess the validity of the AI model outside the training set. Five specialized experienced gastrointestinal pathologists scored each feature blindly and independently of each other and of AI model results. Results: The performance of the AI model for all cell type features was similar/non-inferior to that of our group of GI pathologists (F1-scores: 94.5--94.8 for AI vs human and 92.6--96.0 for human vs human). Segmentation area features were rated for accuracy using the following scale: 1. ``perfect or nearly perfect'' (95{\%}--100{\%}, no significant errors), 2. ``very good'' (80{\%}--95{\%}, only minor errors), 3. ``good'' (70{\%}--80{\%}, significant errors but still captures the feature well), 4. ``insufficient'' (less than 70{\%}, significant errors compromising feature recognition). Rating scores for tissue (1.01), spongiosis (1.15), basal layer (1.05), surface layer (1.04), lamina propria (1.15), and collagen (1.11) were in the ``very good'' to ``perfect or nearly perfect'' range, while degranulation (2.23) was rated between ``good'' and ``very good''. Conclusion: Our newly developed AI-based tool showed an excellent performance (non-inferior to a group of experienced GI pathologists) for the recognition of various histologic features in the EoE/esophageal mucosal eosinophilia spectrum. This tool represents an important step in creating an accurate and reproducible method for semi-automated quantitative analysis to be used in the evaluation of esophageal biopsies in this clinical context. {\copyright} 2022},
 author = {Archila, L. R. and Smith, L. and Sihvo, H.-K. and Westerling-Bui, T. and Koponen, V. and O'Sullivan, D. M. and Fernandez, M.C.C. and Alexander, E. E. and Wang, Y. and Sivasubramaniam, P. and Patil, A. and Hopson, P. E. and Absah, I. and Ravi, K. and Mounajjed, T. and Pai, R. and Hagen, C. and Hartley, C. and Graham, R. P. and Moreira, R. K.},
 year = {2022},
 title = {Development and technical validation of an artificial intelligence model for quantitative analysis of histopathologic features of eosinophilic esophagitis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139347122&doi=10.1016%2fj.jpi.2022.100144&partnerID=40&md5=305a2f637f61f6c6af0e2a65137a104a},
 keywords = {ML implementation},
 volume = {13},
 journal = {Journal of Pathology Informatics},
 doi = {10.1016/j.jpi.2022.100144},
 file = {Archila, Smith et al. 2022 - Development and technical validation:Attachments/Archila, Smith et al. 2022 - Development and technical validation.pdf:application/pdf}
}


@article{Argesanu.2023,
 abstract = {The integration of machine learning (ML) in various organizations has become an essential aspect with a wide range of applications. However, the development and deployment of machine learning models can be time-consuming and prone to errors due to the iterative nature of the process and the constant testing and retraining of models. As ML becomes more integrated with industrial systems, the demand for controlled, reproducible and repeatable processes rises. This paper proposes a novel approach for the automation of various workflows of the ML model lifecycle via custom Command Line Interfaces (CLI) and Continuous Integration/Continuous Deployment (CI/CD) pipelines. We discuss the challenges and pitfalls of non-automated ML workflows, as well as the benefits of using the proposed toolset. We introduce our bespoke approach to CLI and CI/CD automation, highlighting timesaving as well as consistency-improvement aspects for Process {\&} Pipeline Services' use-case of detecting mechanically induced stress cracking in pipelines. This paper adds to the limited literature on ML lifecycle automation.},
 author = {Argesanu, A. I. and Andreescu, G. D.},
 year = {2023},
 title = {Streamlining Machine Learning Workflows in Industrial Applications with CLI's and CI/CD Pipelines},
 keywords = {Artefact Design},
 pages = {389--396},
 volume = {66},
 number = {3},
 journal = {Acta Technica Napocensis Series - Applied Mathematics Mechanics and Engineering},
 file = {Argesanu, Andreescu 2023 - STREAMLINING MACHINE LEARNING WORKFLOWS:Attachments/Argesanu, Andreescu 2023 - STREAMLINING MACHINE LEARNING WORKFLOWS.pdf:application/pdf}
}


@article{Asch.2019,
 abstract = {Background: Echocardiographic quantification of left ventricular (LV) ejection fraction (EF) relies on either manual or automated identification of endocardial boundaries followed by model-based calculation of end-systolic and end-diastolic LV volumes. Recent developments in artificial intelligence resulted in computer algorithms that allow near automated detection of endocardial boundaries and measurement of LV volumes and function. However, boundary identification is still prone to errors limiting accuracy in certain patients. We hypothesized that a fully automated machine learning algorithm could circumvent border detection and instead would estimate the degree of ventricular contraction, similar to a human expert trained on tens of thousands of images. Methods: Machine learning algorithm was developed and trained to automatically estimate LVEF on a database of {\textgreater}50 000 echocardiographic studies, including multiple apical 2-and 4-chamber views (AutoEF, BayLabs). Testing was performed on an independent group of 99 patients, whose automated EF values were compared with reference values obtained by averaging measurements by 3 experts using conventional volume-based technique. Inter-Technique agreement was assessed using linear regression and Bland-Altman analysis. Consistency was assessed by mean absolute deviation among automated estimates from different combinations of apical views. Finally, sensitivity and specificity of detecting of EF $\leq$35{\%} were calculated. These metrics were compared side-by-side against the same reference standard to those obtained from conventional EF measurements by clinical readers. Results: Automated estimation of LVEF was feasible in all 99 patients. AutoEF values showed high consistency (mean absolute deviation =2.9{\%}) and excellent agreement with the reference values: r=0.95, bias=1.0{\%}, limits of agreement =$\pm$11.8{\%}, with sensitivity 0.90 and specificity 0.92 for detection of EF $\leq$35{\%}. This was similar to clinicians' measurements: r=0.94, bias=1.4{\%}, limits of agreement =$\pm$13.4{\%}, sensitivity 0.93, specificity 0.87. Conclusions: Machine learning algorithm for volume-independent LVEF estimation is highly feasible and similar in accuracy to conventional volume-based measurements, when compared with reference values provided by an expert panel. {\copyright} 2019 American Heart Association, Inc.},
 author = {Asch, F. M. and Poilvert, N. and Abraham, T. and Jankowski, M. and Cleve, J. and Adams, M. and Romano, N. and Hong, H. and Mor-Avi, V. and Martin, R. P. and Lang, R. M.},
 year = {2019},
 title = {Automated Echocardiographic Quantification of Left Ventricular Ejection Fraction Without Volume Measurements Using a Machine Learning Algorithm Mimicking a Human Expert},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072202596&doi=10.1161%2fCIRCIMAGING.119.009303&partnerID=40&md5=064fe6a0d3c6164e3e5c0e5fd900dc04},
 keywords = {ML implementation},
 volume = {12},
 number = {9},
 journal = {Circulation: Cardiovascular Imaging},
 doi = {10.1161/CIRCIMAGING.119.009303},
 file = {Asch, Poilvert et al. 2019 - Automated Echocardiographic Quantification of Left:Attachments/Asch, Poilvert et al. 2019 - Automated Echocardiographic Quantification of Left.pdf:application/pdf}
}


@article{Asch.2021,
 abstract = {Background: We have recently tested an automated machine-learning algorithm that quantifies left ventricular (LV) ejection fraction (EF) from guidelines-recommended apical views. However, in the point-of-care (POC) setting, apical 2-chamber views are often difficult to obtain, limiting the usefulness of this approach. Since most POC physicians often rely on visual assessment of apical 4-chamber and parasternal long-Axis views, our algorithm was adapted to use either one of these 3 views or any combination. This study aimed to (1) test the accuracy of these automated estimates; (2) determine whether they could be used to accurately classify LV function. Methods: Reference EF was obtained using conventional biplane measurements by experienced echocardiographers. In protocol 1, we used echocardiographic images from 166 clinical examinations. Both automated and reference EF values were used to categorize LV function as hyperdynamic (EF{\textgreater}73{\%}), normal (53{\%}-73{\%}), mildly-To-moderately (30{\%}-52{\%}), or severely reduced ({\textless}30{\%}). Additionally, LV function was visually estimated for each view by 10 experienced physicians. Accuracy of the detection of reduced LV function (EF{\textless}53{\%}) by the automated classification and physicians' interpretation was assessed against the reference classification. In protocol 2, we tested the new machine-learning algorithm in the POC setting on images acquired by nurses using a portable imaging system. Results: Protocol 1: The agreement with the reference EF values was good (intraclass correlation, 0.86-0.95), with biases {\textless}2{\%}. Machine-learning classification of LV function showed similar accuracy to that by physicians in most views, with only 10{\%} to 15{\%} cases where it was less accurate. Protocol 2: The agreement with the reference values was excellent (intraclass correlation=0.84) with a minimal bias of 2.5$\pm$6.4{\%}. Conclusions: The new machine-learning algorithm allows accurate automated evaluation of LV function from echocardiographic views commonly used in the POC setting. This approach will enable more POC personnel to accurately assess LV function. {\copyright} 2021 Lippincott Williams and Wilkins. All rights reserved.},
 author = {Asch, F. M. and Mor-Avi, V. and Rubenson, D. and Goldstein, S. and Saric, M. and Mikati, I. and Surette, S. and Chaudhry, A. and Poilvert, N. and Hong, H. and Horowitz, R. and Park, D. and Diaz-Gomez, J. L. and Boesch, B. and Nikravan, S. and Liu, R. B. and Philips, C. and Thomas, J. D. and Martin, R. P. and Lang, R. M.},
 year = {2021},
 title = {Deep Learning-Based Automated Echocardiographic Quantification of Left Ventricular Ejection Fraction: A Point-of-Care Solution},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108092474&doi=10.1161%2fCIRCIMAGING.120.012293&partnerID=40&md5=330902b0e440442c3353f0417c55ce54},
 keywords = {ML implementation},
 pages = {E012293},
 volume = {14},
 number = {6},
 journal = {Circulation: Cardiovascular Imaging},
 doi = {10.1161/CIRCIMAGING.120.012293}
}


@article{Asif.2022,
 abstract = {Objectives: To assess the diagnostic accuracy of an automated algorithm to detect left ventricular (LV) dilatation on non-ECG gated CT, using cardiac magnetic resonance (CMR) as reference standard. Methods Consecutive patients with contrast-enhanced CT thorax and CMR within 31 days (2016--2020) were analysed (n = 84). LV dilatation was defined against age-, sex-and body surface area-specific values for CMR. CTs underwent automated artificial intelligence(AI)derived analysis that segmented ventricular chambers, presenting maximal LV diameter and volume. Area under the receiver operator curve (AUC-ROC) analysis identified CT thresholds with $\geq$90{\%} sensitivity and highest specificity and $\geq$90{\%} specificity with highest sensitivity. Youden's Index was used to identify thresholds with optimised sensitivity and specificity. Results: Automated diameter analysis was feasible in 92{\%} of cases (77/84; 45 men, age 61 $\pm$ 14 years, mean CT to CMR interval 10 $\pm$ 8 days). Relative to CMR as a reference standard, 45{\%} had LV dilatation. In males, an automated LV diameter measurement of $\geq$55.5 mm was $\geq$90{\%} specific for CMR-defined LV dilatation (positive predictive value (PPV) 85.7{\%}, negative predictive value (NPV) 61.2{\%}, accuracy 68.9{\%}). In females, an LV diameter of $\geq$49.7 mm was $\geq$90{\%} specific for CMR-defined LV dilatation (PPV 66.7{\%}, NPV 73.1{\%}, accuracy 71.9{\%}). AI CT volumetry data did not significantly improve AUC performance. Conclusion: Fully automated AI-derived analysis LV dilatation on routine unselected non-gated contrastenhanced CT thorax studies is feasible. We have defined thresholds for the detection of LV dilatation on CT relative to CMR, which could be used to routinely screen for dilated cardiomyopathy at the time of CT. Advances in knowledge: We show, for the first time, that a fully-automated AI-derived analysis of maximal LV chamber axial diameter on non-ECG-gated thoracic CT is feasible in unselected real-world cases and that the derived measures can predict LV dilatation relative to cardiac magnetic resonance imaging, the non-invasive reference standard for determining cardiac chamber size. We have derived sex-specific cut-off values to screen for LV dilatation on routine contrast-enhanced thoracic CT. Future work should validate these thresholds and determine if technology can alter clinical outcomes in a costeffective manner. {\copyright} 2022, British Institute of Radiology. All rights reserved.},
 author = {Asif, A. and Charters, P.F.P. and Thompson, C.A.S. and Komber, H.M.E.I. and Hudson, B. J. and Rodrigues, J.C.L.},
 year = {2022},
 title = {Artificial intelligence can detect left ventricular dilatation on contrast-enhanced thoracic computer tomography relative to cardiac magnetic resonance imaging},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138460930&doi=10.1259%2fbjr.20210852&partnerID=40&md5=b7ff376d92895a8c4bbb8872ce47b438},
 keywords = {ML implementation},
 volume = {95},
 number = {1138},
 journal = {British Journal of Radiology},
 doi = {10.1259/bjr.20210852},
 file = {Asif, Charters et al. 2022 - Artificial intelligence can detect left:Attachments/Asif, Charters et al. 2022 - Artificial intelligence can detect left.pdf:application/pdf}
}


@article{Augusto.2021,
 abstract = {Background: Left ventricular maximum wall thickness (MWT) is central to diagnosis and risk stratification of hypertrophic cardiomyopathy, but human measurement is prone to variability. We developed an automated machine learning algorithm for MWT measurement and compared precision (reproducibility) with that of 11 international experts, using a dataset of patients with hypertrophic cardiomyopathy. Methods: 60 adult patients with hypertrophic cardiomyopathy, including those carrying hypertrophic cardiomyopathy gene mutations, were recruited at three institutes in the UK from August, 2018, to September, 2019: Barts Heart Centre, University College London Hospital (The Heart Hospital), and Leeds Teaching Hospitals NHS Trust. Participants had two cardiovascular magnetic resonance scans (test and retest) on the same day, ensuring no biological variability, using four cardiac MRI scanner models represented across two manufacturers and two field strengths. End-diastolic short-axis MWT was measured in test and retest by 11 international experts (from nine centres in six countries) and an automated machine learning method, which was trained to segment endocardial and epicardial contours on an independent, multicentre, multidisease dataset of 1923 patients. Machine learning MWT measurement was done with a method based on solving Laplace's equation. To assess test--retest reproducibility, we estimated the absolute test--retest MWT difference (precision), the coefficient of variation (CoV) for duplicate measurements, and the number of patients reclassified between test and retest according to different thresholds (MWT {\textgreater}15 mm and {\textgreater}30 mm). We calculated the sample size required to detect a prespecified MWT change between pairs of scans for machine learning and each expert. Findings: 1440 MWT measurements were analysed, corresponding to two scans from 60 participants by 12 observers (11 experts and machine learning). Experts differed in the MWT they measured, ranging from 14·9 mm (SD 4·2) to 19·0 mm (4·7; p{\textless}0·0001 for trend). Machine learning-measured mean MWT was 16·8 mm (4·1). Machine learning precision was superior, with a test--retest difference of 0·7 mm (0·6) compared with experts, who ranged from 1·1 mm (0·9) to 3·7 mm (2·0; p values for machine learning vs expert comparison ranging from {\textless}0·0001 to 0·0073) and a significantly lower CoV than for all experts (4·3{\%} [95{\%} CI 3·3--5·1] vs 5·7--12·1{\%} across experts). On average, 38 (64{\%}) patients were designated as having MWT greater than 15 mm by machine learning compared with 27 (45{\%}) to 50 (83{\%}) patients by experts; five (8{\%}) patients were reclassified in test--retest by machine learning compared with four (7{\%}) to 12 (20{\%}) by experts. With a cutoff point of more than 30 mm for implantable cardioverter-defibrillator, three experts would have changed recommendations between tests a total of four times, but machine learning was consistent. Using machine learning, a clinical trial to detect a 2 mm MWT change would need 2·3 times (range 1·6--4·6) fewer patients. Interpretation: In this preliminary study, machine learning MWT measurement in hypertrophic cardiomyopathy is superior to human experts with potential implications for diagnosis, risk stratification, and clinical trials. Funding: European Regional Development Fund and Barts Charity. {\copyright} 2021 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license},
 author = {Augusto, J. B. and Davies, R. H. and Bhuva, A. N. and Knott, K. D. and Seraphim, A. and Alfarih, M. and Lau, C. and Hughes, R. K. and Lopes, L. R. and Shiwani, H. and Treibel, T. A. and Gerber, B. L. and Hamilton-Craig, C. and Ntusi, N.A.B. and Pontone, G. and Desai, M. Y. and Greenwood, J. P. and Swoboda, P. P. and Captur, G. and Cavalcante, J. and Bucciarelli-Ducci, C. and Petersen, S. E. and Schelbert, E. and Manisty, C. and Moon, J. C.},
 year = {2021},
 title = {Diagnosis and risk stratification in hypertrophic cardiomyopathy using machine learning wall thickness measurement: a comparison with human test-retest performance},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098873583&doi=10.1016%2fS2589-7500%2820%2930267-3&partnerID=40&md5=8d679745e46eed5133eb37f3d507cfb5},
 keywords = {ML implementation},
 pages = {e20-e28},
 volume = {3},
 number = {1},
 journal = {The Lancet Digital Health},
 doi = {10.1016/S2589-7500(20)30267-3},
 file = {Augusto, Davies et al 2021 - Diagnosis and risk stratification:Attachments/Augusto, Davies et al 2021 - Diagnosis and risk stratification.pdf:application/pdf}
}


@article{Baccolo.2021,
 abstract = {Gas chromatography -- mass spectrometry (GC-MS) is an important tool in contemporary untargeted chemical analysis, where the batch analysis of sample series and subsequent generation of peak tables are still commonly subject to software-uncertainty leading to issues in reproducibility and hypothesis testing. Using tensor-based modelling in combination with other machine learning tools, we were able to provide a completely automated method for turning GC-MS data into a peak-table that is absent of user-interactions, avoiding user induced differences in the peak tables. The developed tools are integrated into the software package called PARADISe. The results of using the fully automated version of PARADISe are illustrated using experimental GC-MS data. The presented approach still has room for improvement, especially when the data collinearity is broken, such as in the case of peak saturation. The proposed automated approach provides marked improvements over current analysis, including but not limited to the analysis time and reproducibility. {\copyright} 2021 The Authors},
 author = {Baccolo, G. and Quintanilla-Casas, B. and Vichi, S. and Augustijn, D. and Bro, R.},
 year = {2021},
 title = {From untargeted chemical profiling to peak tables -- A fully automated AI driven approach to untargeted GC-MS},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118694746&doi=10.1016%2fj.trac.2021.116451&partnerID=40&md5=bfe6ebdc2c0062e0f44dee33461105f4},
 keywords = {Artefact Design},
 volume = {145},
 journal = {TrAC - Trends in Analytical Chemistry},
 doi = {10.1016/j.trac.2021.116451},
 file = {Baccolo, Quintanilla-Casas et al 2021 - From untargeted chemical profiling:Attachments/Baccolo, Quintanilla-Casas et al 2021 - From untargeted chemical profiling.pdf:application/pdf}
}


@article{Bachmann.2023,
 abstract = {Current conventional methods of evaluating microstructures are characterized by a high degree of subjectivity and a lack of reproducibility. Modern machine learning (ML) approaches have already shown great potential in overcoming these challenges. Once trained with representative data in combination with objective ground truth, the ML model is able to perform a task properly in a reproducible and automated manner. However, in highly complex use cases, it is often not possible to create a definite ground truth. This study addresses this problem using the underlying showcase of microstructures of highly complex quenched and quenched and tempered (Q/QT) steels. A patch-wise classification approach combined with a sliding window technique provides a solution for segmenting entire microphotographs where pixel-wise segmentation is not applicable since it is hardly feasible to create reproducible training masks. Using correlative microscopy, consisting of light optical microscope (LOM) and scanning electron microscope (SEM) micrographs, as well as corresponding data from electron backscatter diffraction (EBSD), a training dataset of reference states that covers a wide range of microstructures was acquired in order to train accurate and robust ML models in order to classify LOM or SEM images. Despite the enormous complexity associated with the steels treated here, classification accuracies of 88.8{\%} in the case of LOM images and 93.7{\%} for high-resolution SEM images were achieved. These high accuracies are close to super-human performance, especially in consideration of the reproducibility of the automated ML approaches compared to conventional methods based on subjective evaluations through experts. {\copyright} 2023 by the authors.},
 author = {Bachmann, B.-I. and M{\"u}ller, M. and Britz, D. and Staudt, T. and M{\"u}cklich, F.},
 year = {2023},
 title = {Reproducible Quantification of the Microstructure of Complex Quenched and Quenched and Tempered Steels Using Modern Methods of Machine Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169082526&doi=10.3390%2fmet13081395&partnerID=40&md5=3e798e59b6fb75b40f042210add2cebb},
 keywords = {ML implementation},
 volume = {13},
 number = {8},
 journal = {Metals},
 doi = {10.3390/met13081395},
 file = {Bachmann, M{\"u}ller et al. 2023 - Reproducible Quantification of the Microstructure:Attachments/Bachmann, M{\"u}ller et al. 2023 - Reproducible Quantification of the Microstructure.pdf:application/pdf}
}


@article{Bains.2023,
 abstract = {Monitoring the activity of mice within their home cage is proving to be a powerful tool for revealing subtle and early-onset phenotypes in mouse models. Video-tracking, in particular, lends itself to automated machine-learning technologies that have the potential to improve the manual annotations carried out by humans. This type of recording and analysis is particularly powerful in objective phenotyping, monitoring behaviors with no experimenter intervention. Automated home-cage testing allows the recording of non-evoked voluntary behaviors, which do not require any contact with the animal or exposure to specialist equipment. By avoiding stress deriving from handling, this approach, on the one hand, increases the welfare of experimental animals and, on the other hand, increases the reliability of results excluding confounding effects of stress on behavior. In this study, we show that the monitoring of climbing on the wire cage lid of a standard individually ventilated cage (IVC) yields reproducible data reflecting complex phenotypes of individual mouse inbred strains and of a widely used model of neurodegeneration, the N171-82Q mouse model of Huntington's disease (HD). Measurements in the home-cage environment allowed for the collection of comprehensive motor activity data, which revealed sexual dimorphism, daily biphasic changes, and aging-related decrease in healthy C57BL/6J mice. Furthermore, home-cage recording of climbing allowed early detection of motor impairment in the N171-82Q HD mouse model. Integrating cage-floor activity with cage-lid activity (climbing) has the potential to greatly enhance the characterization of mouse strains, detecting early and subtle signs of disease and increasing reproducibility in preclinical studies. Copyright {\copyright} 2023 Bains, Forrest, Sillito, Armstrong, Stewart, Nolan and Wells.},
 author = {Bains, R. S. and Forrest, H. and Sillito, R. R. and Armstrong, J. D. and Stewart, M. and Nolan, P. M. and Wells, S. E.},
 year = {2023},
 title = {Longitudinal home-cage automated assessment of climbing behavior shows sexual dimorphism and aging-related decrease in C57BL/6J healthy mice and allows early detection of motor impairment in the N171-82Q mouse model of Huntington's disease},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152590333&doi=10.3389%2ffnbeh.2023.1148172&partnerID=40&md5=2c18da1962c83315ccf054db1df5fd92},
 keywords = {ML implementation},
 volume = {17},
 journal = {Frontiers in Behavioral Neuroscience},
 doi = {10.3389/fnbeh.2023.1148172},
 file = {Bains, Forrest et al. 2023 - Longitudinal home-cage automated assessment:Attachments/Bains, Forrest et al. 2023 - Longitudinal home-cage automated assessment.pdf:application/pdf}
}


@article{Bangaru.2019,
 abstract = {The scanning electron microscopy (SEM) images are commonly used to understand the microstructure of the concrete. With the advancements in the field of computer vision, many researchers have adopted the image processing technique for the microstructure analysis. Most of the previous methods are not adaptable, non-reproducible, semi-automated, and most importantly all these methods are highly influenced by image magnification. Therefore, to overcome these challenges, this paper presents a machine learning based image segmentation method for microstructure analysis and degree of hydration measurement using SEM images. In addition, the authors looked into the impact of magnification of SEM images on the model accuracy and classifier training for the degree of hydration measurement considering two scenarios. First, the image segmentation was performed using a classifier of specific magnification, and then a common classifier is trained using the image of different magnification. The results show that the Random Forest classifier algorithm is suitable for microstructure analysis using SEM images. Through the statistical analysis, it has been proved that there is no significant effect of magnification on model training and accuracy for the degree of hydration measurement. So, a single classifier can be used to process the images of different magnification of a specimen which reduces the effort of training and computational time. The proposed method can generate highly accurate and reliable results in a shorter time and lower cost. Moreover, the findings in this research can be useful for researchers to determine the optimum magnification required for the microstructure analysis. {\copyright} 2019},
 author = {Bangaru, S. S. and Wang, C. and Hassan, M. and Jeon, H. W. and Ayiluri, T.},
 year = {2019},
 title = {Estimation of the degree of hydration of concrete through automated machine learning based microstructure analysis -- A study on effect of image magnification},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070666335&doi=10.1016%2fj.aei.2019.100975&partnerID=40&md5=84ddc4a7ca3284d98d9b698ecad829f4},
 keywords = {ML implementation},
 volume = {42},
 journal = {Advanced Engineering Informatics},
 doi = {10.1016/j.aei.2019.100975}
}


@inproceedings{Baptista.2013,
 abstract = {It has been pointed that public goods games lack experimental research in topics such as the study of the interaction of groups versus the interaction of individuals and effects of social identity and decision framing. Furthermore, the number of computational frameworks proposed to date to deploy these type of experiments is reduced. In this context, we propose the INVITE computational framework to serve as a useful research tool. The motivation behind the proposed framework is therefore straightforward: to allow researchers to be able to configure without difficulty public goods experiments where they can test their hypothesis regarding the behaviour of individuals, simulate behaviour of automated artificial intelligence and study the interaction between virtual agents and real persons. The greatest advantage of the INVITE framework is that it provides a high level of flexibility in the configuration of game theoretical paradigms. It is possible to configure simple structures such as the 2-player prisoner's dilemma or stag hunt as well as some of the most complex forms of inter-group conflicts such as team games. Accordingly, this framework allows an effortless parametrization, assisted by a tailored configuration tool, of a myriad of public goods games. Moreover, the 3D video game configured in the framework places the players in an immersive and engaging virtual environment where real-life conditions can be replicated and some circumstances, difficult to reproduce in real-life, such as life-threatening situations, can also be simulated. Given its characteristics, the proposed framework aims to represent an important contribution to the study of behaviour of both virtual and human players in scenarios of social conflict. {\copyright} 2013 IEEE.},
 author = {Baptista, M. and Damas, H. and Dimas, J. and Raimundo, G. and Prada, R. and Martinho, C. and Santos, P. A. and Pe{\~n}a, J.},
 title = {A serious game based on a public goods experiment},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893558522&doi=10.1109%2fSocialCom.2013.116&partnerID=40&md5=a90f1a3245eee2ef03013c96bda6a0e4},
 keywords = {ML implementation},
 pages = {774--781},
 year = {2013},
 doi = {10.1109/SocialCom.2013.116}
}


@inproceedings{Baresi.2021,
 abstract = {TensorFlow, a popular machine learning (ML) platform, allows users to transparently exploit both GPUs and CPUs to run their applications. Since GPUs are optimized for compute-intensive workloads (e.g., matrix calculus), they help boost executions, but introduce resource heterogeneity. TensorFlow neither provides efficient heterogeneous resource management nor allows for the enforcement of user-defined constraints on the execution time. Most of the works address these issues in the context of creating models on existing data sets (training phase), and only focus on scheduling algorithms. This paper focuses on the inference phase, that is, on the application of created models to predict the outcome on new data interactively, and presents a comprehensive resource management solution called ROMA (Resource Constrained ML Applications). ROMA is an extension of TensorFlow that (a) provides means to easily deploy multiple TensorFlow models in containers using Kubernetes b) allows users to set constraints on response times, (c) schedules the execution of requests on GPUs and CPUs using heuristics, and (d) dynamically refines the CPU core allocation by exploiting control theory. The assessment conducted on four real-world benchmark applications compares ROMA against four different systems and demonstrates a significant reduction (75 {\%} ) in constraint violations and 24 {\%} saved resources on average. {\copyright} 2021, Springer Nature Switzerland AG.},
 author = {Baresi, L. and Quattrocchi, G. and Rasi, N.},
 title = {Resource Management for TensorFlow Inference},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120523618&doi=10.1007%2f978-3-030-91431-8_15&partnerID=40&md5=4da7dc718ea3822fd304338d87ffb1e5},
 pages = {238--253},
 year = {2021},
 doi = {10.1007/978-3-030-91431-8{\textunderscore }15}
}


@article{Barr.2017,
 abstract = {Background: Providing patients with recordings of their clinic visits enhances patient and family engagement, yet few organizations routinely offer recordings. Challenges exist for organizations and patients, including data safety and navigating lengthy recordings. A secure system that allows patients to easily navigate recordings may be a solution. Objective: The aim of this project is to develop and test an interoperable system to facilitate routine recording, the Open Recording Automated Logging System (ORALS), with the aim of increasing patient and family engagement. ORALS will consist of (1) technically proficient software using automated machine learning technology to enable accurate and automatic tagging of in-clinic audio recordings (tagging involves identifying elements of the clinic visit most important to patients [eg, treatment plan] on the recording) and (2) a secure, easy-to-use Web interface enabling the upload and accurate linkage of recordings to patients, which can be accessed at home. Methods: We will use a mixed methods approach to develop and formatively test ORALS in 4 iterative stages: case study of pioneer clinics where recordings are currently offered to patients, ORALS design and user experience testing, ORALS software and user interface development, and rapid cycle testing of ORALS in a primary care clinic, assessing impact on patient and family engagement. Dartmouth's Informatics Collaboratory for Design, Development and Dissemination team, patients, patient partners, caregivers, and clinicians will assist in developing ORALS. Results: We will implement a publication plan that includes a final project report and articles for peer-reviewed journals. In addition to this work, we will regularly report on our progress using popular relevant Tweet chats and online using our website, www.openrecordings.org. We will disseminate our work at relevant conferences (eg, Academy Health, Health Datapalooza, and the Institute for Healthcare Improvement Quality Forums). Finally, Iora Health, a US-wide network of primary care practices (www.iorahealth.com), has indicated a willingness to implement ORALS on a larger scale upon completion of this development project. Conclusions: Upon the completion of this project we will have developed a novel recording system that will be ready for large-scale testing. Our long-term goal is for ORALS to seamlessly fit into a clinic's and patient's daily routine, increasing levels of patient engagement and transparency of care. {\copyright} Paul J Barr, Michelle D Dannenberg, Craig H Ganoe, William Haslett, Rebecca Faill, Saeed Hassanpour, Amar Das, Roger Arend, Meredith C Masel, Sheryl Piper, Haley Reicher, James Ryan, Glyn Elwyn. Originally published in JMIR Research Protocols (http://www.researchprotocols.org), 06.07.2017. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Research Protocols, is properly cited. The complete bibliographic information, a link to the original publication on http://www.researchprotocols.org, as well as this copyright and license information must be included.},
 author = {Barr, P. J. and Dannenberg, M. D. and Ganoe, C. H. and Haslett, W. and Faill, R. and Hassanpour, S. and Das, A. and Arend, R. and Masel, M. C. and Piper, S. and Reicher, H. and Ryan, J. and Elwyn, G.},
 year = {2017},
 title = {Sharing annotated audio recordings of clinic visits with patients-development of the open recording automated logging system (ORALS): Study protocol},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027840416&doi=10.2196%2fresprot.7735&partnerID=40&md5=b95fce13ef751222d7bbfa83b25e4043},
 keywords = {ML implementation},
 volume = {6},
 number = {7},
 journal = {JMIR Research Protocols},
 doi = {10.2196/resprot.7735}
}


@proceedings{Bathen.2022,
 abstract = {This paper introduces the concept of Trustless AutoML, and proposes a framework that combines AutoML techniques with blockchain to fully decentralize the design and training process of machine learning models. The goal is to introduce full transparency and trust in the model design pipeline to establish a root-of-trust.  {\copyright} 2022 IEEE.},
 year = {2022},
 title = {Trustless AutoML for the Age of Internet of Things},
 keywords = {Artefact Design},
 editor = {Bathen, L.A.D. and Jadav, D.},
 doi = {10.1109/ICBC54727.2022.9805535},
 file = {Bathen, Jadav (Hg) 2022 - Trustless AutoML for the Age:Attachments/Bathen, Jadav (Hg) 2022 - Trustless AutoML for the Age.pdf:application/pdf}
}


@inproceedings{Bathen.2022b,
 abstract = {Machine learning (ML) and artificial intelligence (AI) technologies are becoming intrinsic in our every-day life. These technologies are present in our vehicles, our customer service departments, the applications in our smart-phones, our hospitals, to name a few. The tasks they are trying to solve have grown in complexity, thus, it is imperative we can trust not just the accuracy of the models, but the process used to design, train, and deploy them. This paper introduces the idea of TrustlessNAS, a platform for decentralized Network Architecture Search (NAS), which serves as the root-of-trust for the design, training, and validation process of NAS-based models. We propose an architecture that relies on permissioned blockchain technology to make the entire design process transparent, auditable, and trusted without the need to fully trust all the participants.  {\copyright} 2022 IEEE.},
 author = {Bathen, L.A.D. and Jadav, D.},
 title = {TrustlessNAS: Towards Trustless Network Architecture Search},
 keywords = {Artefact Design},
 pages = {117--125},
 year = {2022},
 doi = {10.1109/SOSE55356.2022.00020},
 file = {Bathen, Jadav 2022 - TrustlessNAS:Attachments/Bathen, Jadav 2022 - TrustlessNAS.pdf:application/pdf}
}


@article{Beecy.2020,
 abstract = {Purpose: Echocardiography (echo) is widely used for right ventricular (RV) assessment. Current techniques for RV evaluation require additional imaging and manual analysis; machine learning (ML) approaches have the potential to provide efficient, fully automated quantification of RV function. Methods: An automated ML model was developed to track the tricuspid annulus on echo using a convolutional neural network approach. The model was trained using 7791 image frames, and automated linear and circumferential indices quantifying annular displacement were generated. Automated indices were compared to an independent reference of cardiac magnetic resonance (CMR) defined RV dysfunction (RVEF~{\textless}~50{\%}). Results: A total of 101 patients prospectively underwent echo and CMR: Fully automated annular tracking was uniformly successful; analyses entailed minimal processing time ({\textless}1~second for all) and no user editing. Findings demonstrate all automated annular shortening indices to be lower among patients with CMR-quantified RV dysfunction (all P~{\textless}.001). Magnitude of ML annular displacement decreased stepwise in relation to population-based tertiles of TAPSE, with similar results when ML analyses were localized to the septal or lateral annulus (all P~$\leq$.001). Automated segmentation techniques provided good diagnostic performance (AUC 0.69--0.73) in relation to CMR reference and compared to conventional RV indices (TAPSE and S$\prime$) with high negative predictive value (NPV 84{\%}--87{\%} vs 83{\%}--88{\%}). Reproducibility was higher for ML algorithm as compared to manual segmentation with zero inter- and intra-observer variability and ICC 1.0 (manual ICC: 0.87--0.91). Conclusions: This study provides an initial validation of a deep learning system for RV assessment using automated tracking of the tricuspid annulus. {\copyright} 2020 The Authors. Echocardiography published by Wiley Periodicals LLC.},
 author = {Beecy, A. N. and Bratt, A. and Yum, B. and Sultana, R. and Das, M. and Sherifi, I. and Devereux, R. B. and Weinsaft, J. W. and Kim, J.},
 year = {2020},
 title = {Development of novel machine learning model for right ventricular quantification on echocardiography---A multimodality validation study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084415058&doi=10.1111%2fecho.14674&partnerID=40&md5=c34ce14bb8bcf6d9407951472a81e492},
 keywords = {ML implementation},
 pages = {688--697},
 volume = {37},
 number = {5},
 journal = {Echocardiography},
 doi = {10.1111/echo.14674},
 file = {Beecy, Bratt et al. 2020 - Development of novel machine learning:Attachments/Beecy, Bratt et al. 2020 - Development of novel machine learning.pdf:application/pdf}
}


@article{Benlala.2022,
 abstract = {Objective: This study aimed to develop and validate an automated artificial intelligence (AI)-driven quantification of pleural plaques in a population of retired workers previously occupationally exposed to asbestos. Methods: CT scans of former workers previously occupationally exposed to asbestos who participated in the multicenter APEXS (Asbestos PostExposure Survey) study were collected retrospectively between 2010 and 2017 during the second and the third rounds of the survey. A hundred and forty-one participants with pleural plaques identified by expert radiologists at the 2nd and the 3rd CT screenings were included. Maximum Intensity Projection (MIP) with 5 mm thickness was used to reduce the number of CT slices for manual delineation. A Deep Learning AI algorithm using 2D-convolutional neural networks was trained with 8280 images from 138 CT scans of 69 participants for the semantic labeling of Pleural Plaques (PP). In all, 2160 CT images from 36 CT scans of 18 participants were used for AI testing versus ground-truth labels (GT). The clinical validity of the method was evaluated longitudinally in 54 participants with pleural plaques. Results: The concordance correlation coefficient (CCC) between AI-driven and GT was almost perfect ({\textgreater}0.98) for the volume extent of both PP and calcified PP. The 2D pixel similarity overlap of AI versus GT was good (DICE = 0.63) for PP, whether they were calcified or not, and very good (DICE = 0.82) for calcified PP. A longitudinal comparison of the volumetric extent of PP showed a significant increase in PP volumes (p {\textless} 0.001) between the 2nd and the 3rd CT screenings with an average delay of 5 years. Conclusions: AI allows a fully automated volumetric quantification of pleural plaques showing volumetric progression of PP over a five-year period. The reproducible PP volume evaluation may enable further investigations for the comprehension of the unclear relationships between pleural plaques and both respiratory function and occurrence of thoracic malignancy. {\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
 author = {Benlala, I. and de Senneville, B. D. and Dournes, G. and Menant, M. and Gramond, C. and Thaon, I. and Clin, B. and Brochard, P. and Gislard, A. and Andujar, P. and Chammings, S. and Gallet, J. and Lacourt, A. and Delva, F. and Paris, C. and Ferretti, G. and Pairon, J.-C. and Laurent, F.},
 year = {2022},
 title = {Deep Learning for the Automatic Quantification of Pleural Plaques in Asbestos-Exposed Subjects},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123604247&doi=10.3390%2fijerph19031417&partnerID=40&md5=725b5c1d76f85a505579d6a290fd75a9},
 keywords = {ML implementation},
 volume = {19},
 number = {3},
 journal = {International Journal of Environmental Research and Public Health},
 doi = {10.3390/ijerph19031417},
 file = {Benlala, Senneville et al. 2022 - Deep Learning for the Automatic:Attachments/Benlala, Senneville et al. 2022 - Deep Learning for the Automatic.pdf:application/pdf}
}


@article{Bess.2023,
 abstract = {Purpose: This study introduces a sophisticated computational pipeline, eVir, designed for the discovery of antiviral drugs based on their interactions within the human protein network. There is a pressing need for cost-effective therapeutics for infectious diseases (e.g., COVID-19), particularly in resource-limited countries. Therefore, our team devised an Artificial Intelligence (AI) system to explore repurposing opportunities for currently used oral therapies. The eVir system operates by identifying pharmaceutical compounds that mirror the effects of antiviral peptides (AVPs)---fragments of human proteins known to interfere with fundamental phases of the viral life cycle: entry, fusion, and replication. eVir extrapolates the probable antiviral efficacy of a given compound by analyzing its established and predicted impacts on the human protein-protein interaction network. This innovative approach provides a promising platform for drug repurposing against SARS-CoV-2 or any virus for which peptide data is available. Methods: The eVir AI software pipeline processes drug-protein and protein-protein interaction networks generated from open-source datasets. eVir uses Node2Vec, a graph embedding technique, to understand the nuanced connections among drugs and proteins. The embeddings are input a Siamese Network (SNet) and MLPs, each tailored for the specific mechanisms of entry, fusion, and replication, to evaluate the similarity between drugs and AVPs. Scores generated from the SNet and MLPs undergo a Platt probability calibration and are combined into a unified score that gauges the potential antiviral efficacy of a drug. This integrated approach seeks to boost drug identification confidence, offering a potential solution for detecting therapeutic candidates with pronounced antiviral potency. Once identified a number of compounds were tested for efficacy and toxicity in lung carcinoma cells (Calu-3) infected with SARS-CoV-2. A lead compound was further identified to determine its efficacy and toxicity in K18-hACE2 mice infected with SARS-CoV-2. Computational Predictions: The SNet confidently differentiated between similar and dissimilar drug pairs with an accuracy of 97.28{\%} and AUC of 99.47{\%}. Key compounds identified through these networks included Zinc, Mebendazole, Levomenol, Gefitinib, Niclosamide, and Imatinib. Notably, Mebendazole and Zinc showcased the highest similarity scores, while Imatinib, Levemenol, and Gefitinib also ranked within the top 20, suggesting their significant pharmacological potentials. Further examination of protein binding analysis using explainable AI focused on reverse engineering the causality of the networks. Protein interaction scores for Mebendazole and Imatinib revealed their effects on notable proteins such as CDPK1, VEGF2, ABL1, and several tyrosine protein kinases. Laboratory Studies: This study determined that Mebendazole, Gefitinib, Topotecan and to some extent Carfilzomib showed conventional drug-response curves, with IC50 values near or below that of Remdesivir with excellent confidence all above R2{\textgreater}0.91, and no cytotoxicity at the IC50 concentration in Calu-3 cells. Cyclosporine A showed antiviral activity, but also unconventional drug-response curves and low R2 which are explained by the non-dose dependent toxicity of the compound. Additionally, Niclosamide demonstrated a conventional drug-response curve with high confidence; however, its inherent cytotoxicity may be a confounding element that misrepresents true antiviral efficacy, by reflecting cellular damage rather than a genuine antiviral action. Remdesivir was used as a control compound and was evaluated in parallel with the submitted test article and had conventional drug-response curves validating the overall results of the assay. Mebendazole was identified from the cell studies to have efficacy at non-toxic concentrations and were further evaluated in mice infected with SARS-CoV-2. Mebendazole administered to K18-hACE2 mice infected with SARS-CoV-2, resulted in a 44.2{\%} reduction in lung viral load compared to non-treated placebo control respectively. There were no significant differences in body weight and all clinical chemistry determinations evaluated (i.e., kidney and liver enzymes) between the different treatment groups. Conclusion: This research underscores the potential of repurposing existing compounds for treating COVID-19. Our preliminary findings underscore the therapeutic promise of several compounds, notably Mebendazole, in both in vitro and in vivo settings against SARS-CoV-2. Several of the drugs explored, especially Mebendazole, are off-label medication; their cost-effectiveness position them as economical therapies against SARS-CoV-2. Copyright {\copyright} 2023 Bess, Berglind, Mukhopadhyay, Brylinski, Alvin, Fattah and Wasan.},
 author = {Bess, A. and Berglind, F. and Mukhopadhyay, S. and Brylinski, M. and Alvin, C. and Fattah, F. and Wasan, K. M.},
 year = {2023},
 title = {Identification of oral therapeutics using an AI platform against the virus responsible for COVID-19, SARS-CoV-2},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181582607&doi=10.3389%2ffphar.2023.1297924&partnerID=40&md5=009061b26634e8abd56440229a2a9bad},
 keywords = {ML implementation},
 volume = {14},
 journal = {Frontiers in Pharmacology},
 doi = {10.3389/fphar.2023.1297924},
 file = {Bess, Berglind et al. 2023 - Identification of oral therapeutics using:Attachments/Bess, Berglind et al. 2023 - Identification of oral therapeutics using.pdf:application/pdf}
}


@article{Beyramysoltan.2022,
 abstract = {The widespread abuse of {\textquotedbl}legal high{\textquotedbl}psychoactive plants continues to be of global concern because of their negative impacts on public health and safety. In forensic science, a major challenge in controlling these substances is the paucity of methods to rapidly identify them. We report the development of the Database of Psychoactive Plants (DoPP), a new user-friendly tool featuring an architecture for the identification of plant unknowns, and the necessary regression statistics for the development and validation of psychoactive compound quantification. The application relies on the knowledge that terrestrial plants exhibit species-specific chemical signatures that can be revealed by direct analysis in real time─high-resolution mass spectrometry (DART-HRMS). Subsequent automated machine learning processing of libraries of these spectra enables rapid discrimination and species identification. The chemical signature database includes 57 available plant species. The rapid acquisition of mass spectra and the ability to sample the materials in their native form enabled the generation of the vast amounts of spectral replicates required for database construction. For the identification of sample unknowns, a data analysis workflow was developed and implemented using the DoPP tool. It utilizes a hierarchical classification tree that integrates three machine learning methods, namely, random forest, k-nearest neighbors, and support vector machine, all of which were fused using posterior probabilities. The results show accuracies of 98 and 99{\%} for 10-fold cross-validation and external validation, respectively, which make the classification model suitable for identity prediction of real samples.  {\copyright} 2022 American Chemical Society.},
 author = {Beyramysoltan, S. and Chambers, M. I. and Osborne, A. M. and Ventura, M. I. and Musah, R. A.},
 year = {2022},
 title = {Introducing {\textquotedbl}DoPP{\textquotedbl}: A Graphical User-Friendly Application for the Rapid Species Identification of Psychoactive Plant Materials and Quantification of Psychoactive Small Molecules Using DART-MS Data},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142428255&doi=10.1021%2facs.analchem.2c01614&partnerID=40&md5=f12e2a4453f2d291591ee4c66bbc786a},
 keywords = {ML implementation},
 pages = {16570--16578},
 volume = {94},
 number = {48},
 journal = {Analytical Chemistry},
 doi = {10.1021/acs.analchem.2c01614}
}


@article{Bhuva.2019,
 abstract = {Background: Automated analysis of cardiac structure and function using machine learning (ML) has great potential, but is currently hindered by poor generalizability. Comparison is traditionally against clinicians as a reference, ignoring inherent human inter-and intraobserver error, and ensuring that ML cannot demonstrate superiority. Measuring precision (scan:rescan reproducibility) addresses this. We compared precision of ML and humans using a multicenter, multi-disease, scan:rescan cardiovascular magnetic resonance data set. Methods: One hundred ten patients (5 disease categories, 5 institutions, 2 scanner manufacturers, and 2 field strengths) underwent scan:rescan cardiovascular magnetic resonance (96{\%} within one week). After identification of the most precise human technique, left ventricular chamber volumes, mass, and ejection fraction were measured by an expert, a trained junior clinician, and a fully automated convolutional neural network trained on 599 independent multicenter disease cases. Scan:rescan coefficient of variation and 1000 bootstrapped 95{\%} CIs were calculated and compared using mixed linear effects models. Results: Clinicians can be confident in detecting a 9{\%} change in left ventricular ejection fraction, with greater than half of coefficient of variation attributable to intraobserver variation. Expert, trained junior, and automated scan:rescan precision were similar (for left ventricular ejection fraction, coefficient of variation 6.1 [5.2{\%}-7.1{\%}], P=0.2581; 8.3 [5.6{\%}-10.3{\%}], P=0.3653; 8.8 [6.1{\%}-11.1{\%}], P=0.8620). Automated analysis was 186$\times$ faster than humans (0.07 versus 13 minutes). Conclusions: Automated ML analysis is faster with similar precision to the most precise human techniques, even when challenged with real-world scan:rescan data. Assessment of multicenter, multi-vendor, multi-field strength scan:rescan data (available at www.thevolumesresource.com) permits a generalizable assessment of ML precision and may facilitate direct translation of ML to clinical practice. {\copyright} 2019 American Heart Association, Inc.},
 author = {Bhuva, A. N. and Bai, W. and Lau, C. and Davies, R. H. and Ye, Y. and Bulluck, H. and McAlindon, E. and Culotta, V. and Swoboda, P. P. and Captur, G. and Treibel, T. A. and Augusto, J. B. and Knott, K. D. and Seraphim, A. and Cole, G. D. and Petersen, S. E. and Edwards, N. C. and Greenwood, J. P. and Bucciarelli-Ducci, C. and Hughes, A. D. and Rueckert, D. and Moon, J. C. and Manisty, C. H.},
 year = {2019},
 title = {A Multicenter, Scan-Rescan, Human and Machine Learning CMR Study to Test Generalizability and Precision in Imaging Biomarker Analysis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072566465&doi=10.1161%2fCIRCIMAGING.119.009214&partnerID=40&md5=18cb4a7654f16d0abe958d8dee77b3e5},
 keywords = {Empirical Study;ML implementation},
 volume = {12},
 number = {10},
 journal = {Circulation: Cardiovascular Imaging},
 doi = {10.1161/CIRCIMAGING.119.009214}
}


@article{Bonakdari.2021,
 abstract = {Aim: In osteoarthritis (OA) there is a need for automated screening systems for early detection of structural progressors. We built a comprehensive machine learning (ML) model that bridges major OA risk factors and serum levels of adipokines/related inflammatory factors at baseline for early prediction of at-risk knee OA patient structural progressors over time. Methods: The patient- and gender-based model development used baseline serum levels of six adipokines, three related inflammatory factors and their ratios (36), as well as major OA risk factors [age and bone mass index (BMI)]. Subjects (677) were selected from the Osteoarthritis Initiative (OAI) progression subcohort. The probability values of being structural progressors (PVBSP) were generated using our previously published prediction model, including five baseline structural features of the knee, i.e. two X-rays and three magnetic resonance imaging variables. To identify the most important variables amongst the 47 studied in relation to PVBSP, we employed the ML feature classification methodology. Among five supervised ML algorithms, the support vector machine (SVM) demonstrated the best accuracy and use for gender-based classifiers development. Performance and sensitivity of the models were assessed. A reproducibility analysis was performed with clinical trial OA patients. Results: Feature selections revealed that the combination of age, BMI, and the ratios CRP/MCP-1 and leptin/CRP are the most important variables in predicting OA structural progressors in both genders. Classification accuracies for both genders in the testing stage (OAI) were {\textgreater}80{\%}, with the highest sensitivity of CRP/MCP-1. Reproducibility analysis showed an accuracy ⩾92{\%}; the ratio CRP/MCP-1 demonstrated the highest sensitivity in women and leptin/CRP in men. Conclusion: This is the first time that such a framework was built for predicting knee OA structural progressors. Using this automated ML patient- and gender-based model, early prediction of knee structural OA progression can be performed with high accuracy using only three baseline serum biomarkers and two risk factors. Plain language summary: Machine learning model for early knee osteoarthritis structural progression Knee osteoarthritis is a well-known debilitating disease leading to reduced mobility and quality of life -- the main causes of chronic invalidity. Disease evolution can be slow and span many years; however, for some individuals, the progression/evolution can be fast. Current treatments are only symptomatic and conventional diagnosis of osteoarthritis is not very effective in early identification of patients who will progress rapidly. To improve therapeutic approaches, we need a robust prediction model to stratify osteoarthritis patients at an early stage according to risk of joint structure disease progression. We hypothesize that a prediction model using a machine learning system would enable such an early identification of individuals for whom osteoarthritis knee structure will degrade rapidly. Data were from the Osteoarthritis Initiative, a National Institute of Health (United States) databank, and the robustness and generalizability of the developed model was further evaluated using osteoarthritis patients from an external cohort. Using the supervised machine learning system (support vector machine), we developed an automated patient- and gender-based model enabling an early clinical prognosis for individuals at high risk of structural progressive osteoarthritis. In brief, this model employed at baseline (when the subject sees a physician) easily obtained features consisting of the two main osteoarthritis risk factors, age and bone mass index (BMI), in addition to the serum levels of three molecules. Two of these molecules belong to a family of factors names adipokines and one to a related inflammatory factor. In brief, the model comprising a combination of age, BMI, and the ratios CRP/MCP-1 and leptin/CRP were found very robust for both genders, and the high accuracy persists when tested with an external cohort conferring the gender-based model generalizability. This study offers a new automated system for identifying early knee osteoarthritis structural progressors, which will significantly improve clinical prognosis with real time patient monitoring. {\copyright} The Author(s), 2021.},
 author = {Bonakdari, H. and Jamshidi, A. and Pelletier, J.-P. and Abram, F. and Tardif, G. and Martel-Pelletier, J.},
 year = {2021},
 title = {A warning machine learning algorithm for early knee osteoarthritis structural progressor patient screening},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101545521&doi=10.1177%2f1759720X21993254&partnerID=40&md5=eafb4b5b4a1982558afd0d8dc4f0f46d},
 keywords = {ML implementation},
 volume = {13},
 journal = {Therapeutic Advances in Musculoskeletal Disease},
 doi = {10.1177/1759720X21993254},
 file = {Bonakdari, Jamshidi et al. 2021 - A warning machine learning algorithm:Attachments/Bonakdari, Jamshidi et al. 2021 - A warning machine learning algorithm.pdf:application/pdf}
}


@inproceedings{Bradley.2022,
 abstract = {Explainable Artificial Intelligence (XAI) aims to bridge the understanding between decisions made by an AI interface and the user interacting with the AI. When the goal of the AI is to teach the user how to solve a problem, user-friendly explanations of the AI's decisions must be given to the user so they can learn how to replicate the process for themselves. This paper describes the process of defining explanations in the context of a collaborative AI platform, ALLURE, which teaches the user how to solve a Rubik's Cube. A macro-action in our collaborative AI algorithm refers to a set of moves that takes the cube from initial state to goal state - a process that was not transparent nor accessible when we revealed back-end logic to the front-end for user engagement.~By providing~macro-action explanations to the user in a chatbot as well as a visual representation of the moves being performed on a virtual Rubik's Cube, we created an XAI interface to engage and guide the user through a subset of the solutions that can later be applied to the remaining solutions of the AI. After initial usability testing, our study provides some useful and practical XAI user interface design implications. {\copyright} 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
 author = {Bradley, C. and Wu, D. and Tang, H. and Singh, I. and Wydant, K. and Capps, B. and Wong, K. and Agostinelli, F. and Irvin, M. and Srivastava, B.},
 title = {Explainable Artificial Intelligence (XAI) User Interface Design for Solving a Rubik's Cube},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144633086&doi=10.1007%2f978-3-031-19682-9_76&partnerID=40&md5=7f677d6d8a5da08c168325721e18d66a},
 keywords = {Artefact Design;Empirical Study},
 pages = {605--612},
 year = {2022},
 doi = {10.1007/978-3-031-19682-9{\textunderscore }76}
}


@article{Bratt.2019,
 abstract = {Background: Phase contrast (PC) cardiovascular magnetic resonance (CMR) is widely employed for flow quantification, but analysis typically requires time consuming manual segmentation which can require human correction. Advances in machine learning have markedly improved automated processing, but have yet to be applied to PC-CMR. This study tested a novel machine learning model for fully automated analysis of PC-CMR aortic flow. Methods: A machine learning model was designed to track aortic valve borders based on neural network approaches. The model was trained in a derivation cohort encompassing 150 patients who underwent clinical PC-CMR then compared to manual and commercially-available automated segmentation in a prospective validation cohort. Further validation testing was performed in an external cohort acquired from a different site/CMR vendor. Results: Among 190 coronary artery disease patients prospectively undergoing CMR on commercial scanners (84{\%} 1.5T, 16{\%} 3T), machine learning segmentation was uniformly successful, requiring no human intervention: Segmentation time was {\textless} 0.01 min/case (1.2 min for entire dataset); manual segmentation required 3.96 $\pm$ 0.36 min/case (12.5 h for entire dataset). Correlations between machine learning and manual segmentation-derived flow approached unity (r = 0.99, p {\textless} 0.001). Machine learning yielded smaller absolute differences with manual segmentation than did commercial automation (1.85 $\pm$ 1.80 vs. 3.33 $\pm$ 3.18 mL, p {\textless} 0.01): Nearly all (98{\%}) of cases differed by $\leq$5 mL between machine learning and manual methods. Among patients without advanced mitral regurgitation, machine learning correlated well (r = 0.63, p {\textless} 0.001) and yielded small differences with cine-CMR stroke volume ( 1.3 $\pm$ 17.7 mL, p = 0.36). Among advanced mitral regurgitation patients, machine learning yielded lower stroke volume than did volumetric cine-CMR ( 12.6 $\pm$ 20.9 mL, p = 0.005), further supporting validity of this method. Among the external validation cohort (n = 80) acquired using a different CMR vendor, the algorithm yielded equivalently small differences ( 1.39 $\pm$ 1.77 mL, p = 0.4) and high correlations (r = 0.99, p {\textless} 0.001) with manual segmentation, including similar results in 20 patients with bicuspid or stenotic aortic valve pathology ( 1.71 $\pm$ 2.25 mL, p = 0.25). Conclusion: Fully automated machine learning PC-CMR segmentation performs robustly for aortic flow quantification - yielding rapid segmentation, small differences with manual segmentation, and identification of differential forward/left ventricular volumetric stroke volume in context of concomitant mitral regurgitation. Findings support use of machine learning for analysis of large scale CMR datasets. {\copyright} 2019 The Author(s).},
 author = {Bratt, A. and Kim, J. and Pollie, M. and Beecy, A. N. and Tehrani, N. H. and Codella, N. and Perez-Johnston, R. and Palumbo, M. C. and Alakbarli, J. and Colizza, W. and Drexler, I. R. and Azevedo, C. F. and Kim, R. J. and Devereux, R. B. and Weinsaft, J. W.},
 year = {2019},
 title = {Machine learning derived segmentation of phase velocity encoded cardiovascular magnetic resonance for fully automated aortic flow quantification},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059500747&doi=10.1186%2fs12968-018-0509-0&partnerID=40&md5=852757301e457f1510c96589fc37d48a},
 keywords = {ML implementation},
 volume = {21},
 number = {1},
 journal = {Journal of Cardiovascular Magnetic Resonance},
 doi = {10.1186/s12968-018-0509-0},
 file = {Bratt, Kim et al. 2019 - Machine learning derived segmentation:Attachments/Bratt, Kim et al. 2019 - Machine learning derived segmentation.pdf:application/pdf}
}


@inproceedings{Candel.2023,
 abstract = {Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. {\copyright} 2023 Association for Computational Linguistics.},
 author = {Candel, A. and McKinney, J. and Singer, P. and Pfeiffer, P. and Jeblick, M. and Lee, C. M. and Conde, M. V.},
 title = {H2O Open Ecosystem for State-of-the-art Large Language Models},
 keywords = {Artefact Design},
 pages = {82--89},
 year = {2023},
 file = {Candel, McKinney et al 2023 - H2O Open Ecosystem for State-of-the-art:Attachments/Candel, McKinney et al 2023 - H2O Open Ecosystem for State-of-the-art.pdf:application/pdf}
}


@article{Canfell.2024,
 abstract = {Background: The digital transformation of health care is advancing rapidly. A well-accepted framework for health care improvement is the Quadruple Aim: improved clinician experience, improved patient experience, improved population health, and reduced health care costs. Hospitals are attempting to improve care by using digital technologies, but the effectiveness of these technologies is often only measured against cost and quality indicators, and less is known about the clinician and patient experience. Objective: This study aims to conduct a systematic review and qualitative evidence synthesis to assess the clinician and patient experience of digital hospitals. Methods: The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) and ENTREQ (Enhancing the Transparency in Reporting the Synthesis of Qualitative Research) guidelines were followed. The PubMed, Embase, Scopus, CINAHL, and PsycINFO databases were searched from January 2010 to June 2022. Studies that explored multidisciplinary clinician or adult inpatient experiences of digital hospitals (with a full electronic medical record) were included. Study quality was assessed using the Mixed Methods Appraisal Tool. Data synthesis was performed narratively for quantitative studies. Qualitative evidence synthesis was performed via (1) automated machine learning text analytics using Leximancer (Leximancer Pty Ltd) and (2) researcher-led inductive synthesis to generate themes. Results: A total of 61 studies (n=39, 64{\%} quantitative; n=15, 25{\%} qualitative; and n=7, 11{\%} mixed methods) were included. Most studies (55/61, 90{\%}) investigated clinician experiences, whereas few (10/61, 16{\%}) investigated patient experiences. The study populations ranged from 8 to 3610 clinicians, 11 to 34,425 patients, and 5 to 2836 hospitals. Quantitative outcomes indicated that clinicians had a positive overall satisfaction (17/24, 71{\%} of the studies) with digital hospitals, and most studies (11/19, 58{\%}) reported a positive sentiment toward usability. Data accessibility was reported positively, whereas adaptation, clinician-patient interaction, and workload burnout were reported negatively. The effects of digital hospitals on patient safety and clinicians' ability to deliver patient care were mixed. The qualitative evidence synthesis of clinician experience studies (18/61, 30{\%}) generated 7 themes: inefficient digital documentation, inconsistent data quality, disruptions to conventional health care relationships, acceptance, safety versus risk, reliance on hybrid (digital and paper) workflows, and patient data privacy. There was weak evidence of a positive association between digital hospitals and patient satisfaction scores. Conclusions: Clinicians' experience of digital hospitals appears positive according to high-level indicators (eg, overall satisfaction and data accessibility), but the qualitative evidence synthesis revealed substantive tensions. There is insufficient evidence to draw a definitive conclusion on the patient experience within digital hospitals, but indications appear positive or agnostic. Future research must prioritize equitable investigation and definition of the digital clinician and patient experience to achieve the Quadruple Aim of health care. {\copyright} Oliver J Canfell, Leanna Woods, Yasaman Meshkat, Jenna Krivit, Brinda Gunashanhar, Christine Slade, Andrew Burton-Jones, Clair Sullivan.},
 author = {Canfell, O. J. and Woods, L. and Meshkat, Y. and Krivit, J. and Gunashanhar, B. and Slade, C. and Burton-Jones, A. and Sullivan, C.},
 year = {2024},
 title = {The Impact of Digital Hospitals on Patient and Clinician Experience: Systematic Review and Qualitative Evidence Synthesis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187783754&doi=10.2196%2f47715&partnerID=40&md5=28e0caed4d6ea3ef6df5e2e08ebbbe3f},
 keywords = {Literature Review},
 volume = {26},
 number = {1},
 journal = {Journal of Medical Internet Research},
 doi = {10.2196/47715}
}


@inproceedings{Cardenas.2021,
 abstract = {Purpose: To develop a deep learning model that generates consistent, high-quality lymph node clinical target volumes (CTV) contours for head and neck cancer (HNC) patients, as an integral part of a fully automated radiation treatment planning workflow. Methods and Materials: Computed tomography (CT) scans from 71 HNC patients were retrospectively collected and split into training (n = 51), cross-validation (n = 10), and test (n = 10) data sets. All had target volume delineations covering lymph node levels Ia through V (Ia-V), Ib through V (Ib-V), II through IV (II-IV), and retropharyngeal (RP) nodes, which were previously approved by a radiation oncologist specializing in HNC. Volumes of interest (VOIs) about nodal levels were automatically identified using computer vision techniques. The VOI (cropped CT image) and approved contours were used to train a U-Net autosegmentation model. Each lymph node level was trained independently, with model parameters optimized by assessing performance on the cross-validation data set. Once optimal model parameters were identified, overlap and distance metrics were calculated between ground truth and autosegmentations on the test set. Lastly, this final model was used on 32 additional patient scans (not included in original 71 cases) and autosegmentations visually rated by 3 radiation oncologists as being ``clinically acceptable without requiring edits,'' ``requiring minor edits,'' or ``requiring major edits.'' Results: When comparing ground truths to autosegmentations on the test data set, median Dice Similarity Coefficients were 0.90, 0.90, 0.89, and 0.81, and median mean surface distance values were 1.0 mm, 1.0 mm, 1.1 mm, and 1.3 mm for node levels Ia-V, Ib-V, II-IV, and RP nodes, respectively. Qualitative scoring varied among physicians. Overall, 99{\%} of autosegmented target volumes were either scored as being clinically acceptable or requiring minor edits (ie, stylistic recommendations, {\textless}2 minutes). Conclusions: We developed a fully automated artificial intelligence approach to autodelineate nodal CTVs for patients with intact HNC. Most autosegmentations were found to be clinically acceptable after qualitative review when considering recommended stylistic edits. This promising work automatically delineates nodal CTVs in a robust and consistent manner; this approach can be implemented in ongoing efforts for fully automated radiation treatment planning. {\copyright} 2020 The Author(s)},
 author = {Cardenas, C. E. and Beadle, B. M. and Garden, A. S. and Skinner, H. D. and Yang, J. and Rhee, D. J. and McCarroll, R. E. and Netherton, T. J. and Gay, S. S. and Zhang, L. and Court, L. E.},
 title = {Generating High-Quality Lymph Node Clinical Target Volumes for Head and Neck Cancer Radiation Therapy Using a Fully Automated Deep Learning-Based Approach},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094862316&doi=10.1016%2fj.ijrobp.2020.10.005&partnerID=40&md5=21fc41fc85d1c0e22dccced7cd285971},
 keywords = {ML implementation},
 pages = {801--812},
 year = {2021},
 doi = {10.1016/j.ijrobp.2020.10.005},
 file = {Cardenas, Beadle et al. 2021 - Generating High-Quality Lymph Node Clinical:Attachments/Cardenas, Beadle et al. 2021 - Generating High-Quality Lymph Node Clinical.pdf:application/pdf}
}


@article{Casalicchio.2019,
 abstract = {OpenML is an online machine learning platform where researchers can easily share data, machine learning tasks and experiments as well as organize them online to work and collaborate more efficiently. In this paper, we present an R package to interface with the OpenML platform and illustrate its usage in combination with the machine learning R package mlr~(Bischl et al. J Mach Learn Res 17(170):1--5, 2016). We show how the OpenML package allows R users to easily search, download and upload data sets and machine learning tasks. Furthermore, we also show how to upload results of experiments, share them with others and download results from other users. Beyond ensuring reproducibility of results, the OpenML platform automates much of the drudge work, speeds up research, facilitates collaboration and increases the users' visibility online. {\copyright} 2017, Springer-Verlag GmbH Germany.},
 author = {Casalicchio, G. and Bossek, J. and Lang, M. and Kirchhoff, D. and Kerschke, P. and Hofner, B. and Seibold, H. and Vanschoren, J. and Bischl, B.},
 year = {2019},
 title = {OpenML: An R package to connect to the machine learning platform OpenML},
 keywords = {Artefact Design},
 pages = {977--991},
 volume = {34},
 number = {3},
 journal = {Computational Statistics},
 file = {Casalicchio, Bossek et al. 2019 - OpenML An R package:Attachments/Casalicchio, Bossek et al. 2019 - OpenML An R package.pdf:application/pdf}
}


@article{Chamberlin.2023,
 abstract = {AIM: To evaluate primary and secondary pathologies of interest using an artificial intelligence (AI) platform, AI-Rad Companion, on low-dose computed tomography (CT) series from integrated positron-emission tomography (PET)/CT to detect CT findings that might be overlooked. MATERIALS AND METHODS: One hundred and eighty-nine sequential patients who had undergone PET/CT were included. Images were evaluated using an ensemble of convolutional neural networks (AI-Rad Companion, Siemens Healthineers, Erlangen, Germany). The primary outcome was detection of pulmonary nodules for which the accuracy, identity, and intra-rater reliability was calculated. For secondary outcomes (binary detection of coronary artery calcium, aortic ectasia, vertebral height loss), accuracy and diagnostic performance were calculated. RESULTS: The overall per-nodule accuracy for detection of lung nodules was 0.847. The overall sensitivity and specificity for detection of lung nodules was 0.915 and 0.781. The overall per-patient accuracy for AI detection of coronary artery calcium, aortic ectasia, and vertebral height loss was 0.979, 0.966, and 0.840, respectively. The sensitivity and specificity for coronary artery calcium was 0.989 and 0.969. The sensitivity and specificity for aortic ectasia was 0.806 and 1. CONCLUSION: The neural network ensemble accurately assessed the number of pulmonary nodules and presence of coronary artery calcium and aortic ectasia on low-dose CT series of PET/CT. The neural network was highly specific for the diagnosis of vertebral height loss, but not sensitive. The use of the AI ensemble can help radiologists and nuclear medicine physicians to catch CT findings that might be overlooked. {\copyright} 2023},
 author = {Chamberlin, J. H. and Smith, C. and Schoepf, U. J. and Nance, S. and Elojeimy, S. and O'Doherty, J. and Baruah, D. and Burt, J. R. and Varga-Szemes, A. and Kabakus, I. M.},
 year = {2023},
 title = {A deep convolutional neural network ensemble for composite identification of pulmonary nodules and incidental findings on routine PET/CT},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149702202&doi=10.1016%2fj.crad.2023.01.014&partnerID=40&md5=13c6941f3c5cbbec8f63298a772aab9d},
 keywords = {ML implementation},
 pages = {e368-e376},
 volume = {78},
 number = {5},
 journal = {Clinical Radiology},
 doi = {10.1016/j.crad.2023.01.014}
}


@proceedings{Chastikova.2021,
 abstract = {The article discusses the current state of technologies for automated machine learning. The development trends and the nature of the distribution model - MLaaS - are defined. There is highlighted a number of problems of automating the machine learning process, such as: Excessive simplification and specialization of tools, vagueness of implemented processes, lack of flexibility in the infrastructure hardware, using closed algorithms. As a partial or complete solution to them, we have proposed the architecture, consisting of separate modules: Models, hybridizer, learning algorithms module, testing module, user support module, and a theoretical framework. The main feature of the given architecture is its modularity, transparency and encapsulation of components. Each module is described as a separate element, implemented as an independent microservice. The paper describes the benefits of applying the given approach to the implementation of automated machine learning systems, the need to implement the given or similar standards. For each of the modules, its purposes, the tasks it solves and the implemented functionality, as well as the data necessary for the functioning and their sources are described. A general diagram showing the flows of information exchange between modules is presented. The main scenarios for the resulting system operation, as well as ways of interacting with it and the result of its operation - the generated model - are described. {\copyright} 2021 Institute of Physics Publishing. All rights reserved.},
 year = {2021},
 title = {Developing the platform model for problem solving of automated machine learning},
 keywords = {Artefact Design;Technical Review},
 volume = {2094},
 editor = {Chastikova, V. A. and Zherlitsyn, S. A.},
 doi = {10.1088/1742-6596/2094/3/032049},
 file = {Chastikova, Zherlitsyn (Hg) 2021 - Developing the platform model:Attachments/Chastikova, Zherlitsyn (Hg) 2021 - Developing the platform model.pdf:application/pdf}
}


@inproceedings{Chauhan.2020,
 abstract = {With the explosion in the use of machine learning in various domains, the need for an efficient pipeline for the development of machine learning models has never been more critical. However, the task of forming and training models largely remains traditional with a dependency on domain experts and time-consuming data manipulation operations, which impedes the development of machine learning models in both academia as well as industry. This demand advocates the new research era concerned with fitting machine learning models fully automatically i.e., AutoML. Automated Machine Learning(AutoML) is an end-to-end process that aims at automating this model development pipeline without any external assistance. First, we provide an insights of AutoML. Second, we delve into the individual segments in the AutoML pipeline and cover their approaches in brief. We also provide a case study on the industrial use and impact of AutoML with a focus on practical applicability in a business context. At last, we conclude with the open research issues, and future research directions. {\copyright} 2020 IEEE.},
 author = {Chauhan, K. and Jani, S. and Thakkar, D. and Dave, R. and Bhatia, J. and Tanwar, S. and Obaidat, M. S.},
 title = {Automated Machine Learning: The New Wave of Machine Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084613251&doi=10.1109%2fICIMIA48430.2020.9074859&partnerID=40&md5=93ad4af1cf12d7ae059cf04ea76a9f4c},
 keywords = {Case Study;Literature Review},
 pages = {205--212},
 year = {2020},
 doi = {10.1109/ICIMIA48430.2020.9074859}
}


@article{Chaves.2023,
 abstract = {Through the development of artificial intelligence, some capabilities of human beings have been replicated in computers. Among the developed models, convolutional neural networks stand out considerably because they make it possible for systems to have the inherent capabilities of humans, such as pattern recognition in images and signals. However, conventional methods are based on deterministic models, which cannot express the epistemic uncertainty of their predictions. The alternative consists of probabilistic models, although these are considerably more difficult to develop. To address the problems related to the development of probabilistic networks and the choice of network architecture, this article proposes the development of an application that allows the user to choose the desired architecture with the trained model for the given data. This application, named ``Graphical User Interface for Probabilistic Neural Networks'', allows the user to develop or to use a standard convolutional neural network for the provided data, with networks already adapted to implement a probabilistic model. Contrary to the existing models for generic use, which are deterministic and already pre-trained on databases to be used in transfer learning, the approach followed in this work creates the network layer by layer, with training performed on the provided data, originating a specific model for the data in question. {\copyright} 2023 by the authors.},
 author = {Chaves, A. and Mendon{\c{c}}a, F. and Mostafa, S. S. and Morgado-Dias, F.},
 year = {2023},
 title = {Graphical User Interface for the Development of Probabilistic Convolutional Neural Networks},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180824810&doi=10.3390%2fsignals4020016&partnerID=40&md5=86017e9cb1adaeb180cd9c3d76551f4b},
 keywords = {Artefact Design},
 pages = {297--314},
 volume = {4},
 number = {2},
 journal = {Signals},
 doi = {10.3390/signals4020016},
 file = {Chaves, Mendon{\c{c}}a et al. 2023 - Graphical User Interface:Attachments/Chaves, Mendon{\c{c}}a et al. 2023 - Graphical User Interface.pdf:application/pdf}
}


@article{Chedid.2022,
 abstract = {Although Alzheimer's disease is the most prevalent form of dementia, there are no treatments capable of slowing disease progression. A lack of reliable disease endpoints and/or biomarkers contributes in part to the absence of effective therapies. Using machine learning to analyze EEG offers a possible solution to overcome many of the limitations of current diagnostic modalities. Here we develop a logistic regression model with an accuracy of 81{\%} that addresses many of the shortcomings of previous works. To our knowledge, no other study has been able to solve the following problems simultaneously: (1) a lack of automation and unbiased removal of artifacts, (2) a dependence on a high level of expertise in data pre-processing and ML for non-automated processes, (3) the need for very large sample sizes and accurate EEG source localization using high density systems, (4) and a reliance on black box ML approaches such as deep neural nets with unexplainable feature selection. This study presents a proof-of-concept for an automated and scalable technology that could potentially be used to diagnose AD in clinical settings as an adjunct to conventional neuropsychological testing, thus enhancing efficiency, reproducibility, and practicality of AD diagnosis. {\copyright} 2022, The Author(s).},
 author = {Chedid, N. and Tabbal, J. and Kabbara, A. and Allouch, S. and Hassan, M.},
 year = {2022},
 title = {The development of an automated machine learning pipeline for the detection of Alzheimer's Disease},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140709563&doi=10.1038%2fs41598-022-22979-3&partnerID=40&md5=37b2a883af285f971764ece4f185ce19},
 keywords = {ML implementation},
 volume = {12},
 number = {1},
 journal = {Scientific Reports},
 doi = {10.1038/s41598-022-22979-3},
 file = {Chedid, Tabbal et al. 2022 - The development of an automated:Attachments/Chedid, Tabbal et al. 2022 - The development of an automated.pdf:application/pdf}
}


@inproceedings{Chen.2020,
 abstract = {Automated machine learning (AutoML) strives to establish an appropriate machine learning model for any dataset automatically with minimal human intervention. Although extensive research has been conducted on AutoML, most of it has focused on supervised learning. Research of automated semisupervised learning and active learning algorithms is still limited. Implementation becomes more challenging when the algorithm is designed for a distributed computing environment. With this as motivation, we propose a novel automated learning system for distributed active learning (AutoDAL) to address these challenges. First, automated graph-based semisupervised learning is conducted by aggregating the proposed cost functions from different compute nodes in a distributed manner. Subsequently, automated active learning is addressed by jointly optimizing hyperparameters in both the classification and query selection stages leveraging the graph loss minimization and entropy regularization. Moreover, we propose an efficient distributed active learning algorithm which is scalable for big data by first partitioning the unlabeled data and replicating the labeled data to different worker nodes in the classification stage, and then aggregating the data in the controller in the query selection stage. The proposed Auto- DAL algorithm is applied to multiple benchmark datasets and a real-world electrocardiogram (ECG) dataset for classification. We demonstrate that the proposed AutoDAL algorithm is capable of achieving significantly better performance compared to several state-of-the-art AutoML approaches and active learning algorithms. {\copyright} 2020, Association for the Advancement of Artificial Intelligence.},
 author = {Chen, X. and Wujek, B.},
 title = {Autodal: Distributed active learning with automatic hyperparameter selection},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098188530&partnerID=40&md5=868278b5cb0efebd937364cd75e81125},
 keywords = {Artefact Design},
 pages = {3537--3544},
 year = {2020}
}


@article{Chen.2021,
 abstract = {Sequence-based analysis and prediction are fundamental bioinformatic tasks that facilitate understanding of the sequence(-structure)-function paradigm for DNAs, RNAs and proteins. Rapid accumulation of sequences requires equally pervasive development of new predictive models, which depends on the availability of effective tools that support these efforts. We introduce iLearnPlus, the first machine-learning platform with graphical-and web-based interfaces for the construction of machine-learning pipelines for analysis and predictions using nucleic acid and protein sequences. iLearnPlus provides a comprehensive set of algorithms and automates sequence-based feature extraction and analysis, construction and deployment of models, assessment of predictive performance, statistical analysis, and data visualization; all without programming. iLearnPlus includes a wide range of feature sets which encode information from the input sequences and over twenty machine-learning algorithms that cover several deep-learning approaches, outnumbering the current solutions by a wide margin. Our solution caters to experienced bioinformaticians, given the broad range of options, and biologists with no programming background, given the point-and-click interface and easy-to-follow design process. We showcase iLearnPlus with two case studies concerning prediction of long noncoding RNAs (lncRNAs) from RNA transcripts and prediction of crotonylation sites in protein chains. iLearnPlus is an open-source platform available at https://github.com/Superzchen/iLearnPlus/with the webserver at http://ilearnplus.erc.monash.edu/.  {\copyright} 2021 The Author(s). Published by Oxford University Press on behalf of Nucleic Acids Research.},
 author = {Chen, Z. and Zhao, P. and Li, C. and Li, F. and Xiang, D. and Chen, Y.-Z. and Akutsu, T. and Daly, R. J. and Webb, G. I. and Zhao, Q. and Kurgan, L. and Song, J.},
 year = {2021},
 title = {ILearnPlus: A comprehensive and automated machine-learning platform for nucleic acid and protein sequence analysis, prediction and visualization},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107426139&doi=10.1093%2fnar%2fgkab122&partnerID=40&md5=7e40277a3dd815fb161f3f97f8f1b16e},
 keywords = {Artefact Design},
 volume = {49},
 number = {10},
 journal = {Nucleic Acids Research},
 doi = {10.1093/nar/gkab122},
 file = {Chen, Zhao et al. 2021 - ILearnPlus:Attachments/Chen, Zhao et al. 2021 - ILearnPlus.pdf:application/pdf}
}


@inproceedings{Chen.2022,
 abstract = {As machine learning is applied more widely, it is necessary to have a machine-learning platform for both infrastructure administrators and users including expert data scientists and citizen data scientists [24] to improve their productivity. However, existing machine-learning platforms are ill-equipped to address the {\textquotedbl}Machine Learning tech debts{\textquotedbl}[36] such as glue code, reproducibility, and portability. Furthermore, existing platforms only take expert data scientists into consideration, and thus they are inflexible for infrastructure administrators and non-user-friendly for citizen data scientists. We propose Submarine, a unified machine-learning platform, and takes all infrastructure administrators, expert data scientists, and citizen data scientists into consideration. Submarine has been widely used in many technology companies, including Ke.com and LinkedIn. We present two use cases in Section 5.  {\copyright} 2022 Owner/Author.},
 author = {Chen, K.-H. and Su, H.-P. and Chuang, W.-C. and Hsiao, H.-C. and Tan, W. and Tang, Z. and Liu, X. and Liang, Y. and Lo, W.-C. and Ji, W. and Hsu, B. and Hu, K. and Jian, H. and Zhou, Q. and Wang, C.-M.},
 title = {Apache submarine: A unified machine learning platform made simple},
 keywords = {Artefact Design;Case Study},
 pages = {101--108},
 year = {2022},
 file = {Chen, Su et al 2022 - Apache submarine:Attachments/Chen, Su et al 2022 - Apache submarine.pdf:application/pdf}
}


@article{Chen.2023,
 abstract = {A typical ensemble learning process typically uses a forward integration mechanism to construct the ensemble classifier with a large number of base classifiers. Based on this mechanism, it is difficult to adjust the diversity among base classifiers and optimize the structure inside ensemble since the generation process has a certain amount of randomness, which makes the performance of ensemble classifiers heavily dependent on the human design decisions. To address this issue, we proposed an automatic ensemble classifier construction method based on a dual-layer evolutionary search mechanism, which includes a tree coding-based base classifier population and a binary coding-based ensemble classifier population. Through a collaborative searching process between the two populations, the proposed method can be driven by training data to update the base classifier population and optimize the ensemble classifiers globally. To verify the effectiveness of the dual evolutionary ensemble learning method (DEEL), we tested it on 22 classification tasks from 4 data repositories. The results show that the proposed method can generate a diverse decision tree population on the training data while searching and constructing ensemble classifiers from them. Compared with 9 competitor algorithms, the proposed method achieved the best performance on 17 of 22 test tasks and improved the average accuracies by 0.97--7.65{\%} over the second place. In particular, the generated ensemble classifiers show excellent structure, which involve small number and diverse decision trees. That increases the transparency of ensembles and helps to perform interpretability analysis on them. {\copyright} 2022, The Author(s).},
 author = {Chen, H. and Zhang, G. and Pan, X. and Jia, R.},
 year = {2023},
 title = {Using dual evolutionary search to construct decision tree based ensemble classifier},
 keywords = {Artefact Design},
 pages = {1327--1345},
 volume = {9},
 number = {2},
 journal = {Complex and Intelligent Systems},
 file = {Chen, Zhang et al. 2023 - Using dual evolutionary search:Attachments/Chen, Zhang et al. 2023 - Using dual evolutionary search.pdf:application/pdf}
}


@article{Cheng.2016,
 abstract = {Objective The aim of this study was to evaluate the accuracy of fully automated machine learning methods for detecting intravenous contrast in computed tomography (CT) studies of the abdomen and pelvis. Methods A set of 591 labeled CT image volumes of the abdomen and pelvis was obtained from 5 different CT scanners, of which 434 (73{\%}) were performed with intravenous contrast. A stratified split of this set was performed into training and test sets of 443 and 148 studies, respectively. Subsequently, support vector machine and logistic regression classifiers were trained using 5-fold cross-validation for parameter optimization. Results The best in-sample performance was seen with a support vector machine classifier with a \textgreek{q}2 kernel (98.9{\%} accuracy); however, test set performance was similar across the trained classifiers, with 95{\%} to 97{\%} accuracy. Conclusions Histogram-based automated classifiers for the presence of intravenous contrast are accurate and may be useful for verifying the accurate labeling of the presence of intravenous contrast in CT body studies. {\copyright} 2016 Wolters Kluwer Health, Inc. All rights reserved.},
 author = {Cheng, P. M.},
 year = {2016},
 title = {Histogram-Based Discrimination of Intravenous Contrast in Abdominopelvic Computed Tomography},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954357390&doi=10.1097%2fRCT.0000000000000361&partnerID=40&md5=54e607ee50302eca0fa8b5b694d83c22},
 keywords = {ML implementation},
 pages = {234--237},
 volume = {40},
 number = {2},
 journal = {Journal of Computer Assisted Tomography},
 doi = {10.1097/RCT.0000000000000361}
}


@article{Cho.2022,
 abstract = {Background: Studies have shown that quantitative evaluation of coronary artery plaque on Coronary Computed Tomography Angiography (CCTA) can identify patients at risk of cardiac events. Recent demonstration of artificial intelligence (AI) assisted CCTA shows that it allows for evaluation of CAD and plaque characteristics. Based on publications to date, we are the first group to perform AI augmented CCTA serial analysis of changes in coronary plaque characteristics over 13~years. We evaluated whether AI assisted CCTA can accurately assess changes in coronary plaque progression, which has potential clinical prognostic value in CAD management. Case presentation: 51-year-old male with hypertension, hyperlipidemia and family history of myocardial infarction, underwent CCTA exams for anginal symptom evaluation and CAD assessment. 5 CCTAs were performed between 2008 and 2021. Quantitative atherosclerosis plaque characterization (APC) using an AI platform (Cleerly), was performed to assess CAD burden. Total plaque volume (TPV) change-over-time demonstrated decreasing low-density non-calcified plaque (LD-NCP) with increasing overall NCP and calcified-plaque (CP). Examination of individual segments revealed a proximal-LAD lesion with decreasing NCP over-time and increasing CP. In contrast, although the D2/D1/ramus lesions showed increasing stenosis, CP, and total plaque, there were no significant differences in NCP over-time, with stable NCP and increased CP. Remarkably, we also consistently visualized small plaques, which typically readers may interpret as false positives due to artifacts. But in this case, they reappeared each study in the same locations, generally progressing in size and demonstrating expected plaque transformation over-time. Conclusions: We performed the first AI augmented CCTA based serial analysis of changes in coronary plaque characteristics over 13~years. We were able to consistently assess progression of plaque volumes, stenosis, and APCs with this novel methodology. We found a significant increase in TPV composed of decreasing LD-NCP, and increasing NCP and CP, with variations in the evolution of APCs between vessels. Although the significance of evolving APCs needs to be investigated, this case demonstrates AI-based CCTA analysis can serve as valuable clinical tool to accurately define unique CAD characteristics over time. Prospective trails are needed to assess whether quantification of APCs provides prognostic capabilities to improve clinical care. {\copyright} 2022, The Author(s).},
 author = {Cho, G. W. and Anderson, L. and Quesada, C. G. and Jennings, R. S. and Min, J. K. and Earls, J. P. and Karlsberg, R. P.},
 year = {2022},
 title = {Serial analysis of coronary artery disease progression by artificial intelligence assisted coronary computed tomography angiography: early clinical experience},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142607191&doi=10.1186%2fs12872-022-02951-9&partnerID=40&md5=0c8187b601817b09b6d837595118c41a},
 keywords = {ML implementation},
 volume = {22},
 number = {1},
 journal = {BMC Cardiovascular Disorders},
 doi = {10.1186/s12872-022-02951-9},
 file = {Cho, Anderson et al. 2022 - Serial analysis of coronary artery:Attachments/Cho, Anderson et al. 2022 - Serial analysis of coronary artery.pdf:application/pdf}
}


@article{Chou.2022,
 abstract = {Artificial intelligence and machine learning (AI/ML) is becoming increasingly more accessible to biomedical researchers with significant potential to transform biomedicine through optimization of highly-accurate predictive models and enabling better understanding of disease biology. Automated machine learning (AutoML) in particular is positioned to democratize artificial intelligence (AI) by reducing the amount of human input and ML expertise needed. However, successful translation of AI/ML in biomedicine requires moving beyond optimizing only for prediction accuracy and towards establishing reproducible clinical and biological inferences. This is especially challenging for clinical studies on rare disorders where the smaller patient cohorts and corresponding sample size is an obstacle for reproducible modeling results. Here, we present a model-agnostic framework to reinforce AutoML using strategies and tools of explainable and reproducible AI, including novel metrics to assess model reproducibility. The framework enables clinicians to interpret AutoML-generated models for clinical and biological verifiability and consequently integrate domain expertise during model development. We applied the framework towards spinal cord injury prognostication to optimize the intraoperative hemodynamic range during injury-related surgery and additionally identified a strong detrimental relationship between intraoperative hypertension and patient outcome. Furthermore, our analysis captured how evolving clinical practices such as faster time-to-surgery and blood pressure management affect clinical model development. Altogether, we illustrate how expert-augmented AutoML improves inferential reproducibility for biomedical discovery and can ultimately build trust in AI processes towards effective clinical integration. Copyright: {\copyright} 2022 Chou et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
 author = {Chou, A. and Torres-Espin, A. and Kyritsis, N. and {Russell Huie}, J. and Khatry, S. and Funk, J. and Hay, J. and Lofgreen, A. and Shah, R. and McCann, C. and Pascual, L. U. and Amorim, E. and Weinstein, P. R. and Manley, G. T. and Dhall, S. S. and Pan, J. Z. and Bresnahan, J. C. and Beattie, M. S. and Whetstone, W. D. and Ferguson, A. R.},
 year = {2022},
 title = {Expert-augmented automated machine learning optimizes hemodynamic predictors of spinal cord injury outcome},
 keywords = {Artefact Design},
 volume = {17},
 number = {4 April},
 journal = {PLoS ONE},
 file = {Chou, Torres-Espin et al. 2022 - Expert-augmented automated machine learning optimizes:Attachments/Chou, Torres-Espin et al. 2022 - Expert-augmented automated machine learning optimizes.pdf:application/pdf}
}


@article{Christy.2024,
 abstract = {Purpose: Chat Generative Pre-Trained Transformer (ChatGPT) is a novel artificial intelligence chatbot that is changing the way humans gather information online. The purpose of this study was to investigate ChatGPT's ability to appropriately and reliably answer common questions regarding distal radius fractures. Methods: Thirty common questions regarding distal radius fractures were presented in an identical manner to the online ChatGPT-3.5 interface three separate times, yielding 90 unique responses because ChatGPT produces an original answer with each query. All responses were graded as ``appropriate,'' ``appropriate but incomplete,'' or ``inappropriate'' by a consensus discussion among three hand surgeon reviewers. The questions were additionally subcategorized into one of four domains based on Bloom's cognitive learning taxonomy, and descriptive statistics were reported. Results: Seventy of the 90 total responses (78{\%}) produced by ChatGPT were ``appropriate,'' and 29 of the 30 questions (97{\%}) had at least one response considered appropriate (of the three possible). However, only 17 of the 30 questions (57{\%}) were answered appropriately on all three iterations. The test--retest reliability of ChatGPT was poor with an intraclass correlation coefficient of 0.12. Finally, ChatGPT performed best answering questions requiring lower-order thinking skills (Bloom's levels 1--3) and less well on level 4 questions. Conclusions: This study found that although ChatGPT has the capability to answer common questions regarding distal radius fractures, caution should be taken before implementing its use, given ChatGPT's inconsistency in providing a complete and accurate response to the same question every time. Clinical relevance: As the popularity and technology of ChatGPT continue to grow, it is important to understand the potential and limitations of this platform to determine how it may be best implemented to improve patient care. {\copyright} 2024 American Society for Surgery of the Hand},
 author = {Christy, M. and Morris, M. T. and Goldfarb, C. A. and Dy, C. J.},
 year = {2024},
 title = {Appropriateness and Reliability of an Online Artificial Intelligence Platform's Responses to Common Questions Regarding Distal Radius Fractures},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179492138&doi=10.1016%2fj.jhsa.2023.10.019&partnerID=40&md5=525437df62bdc8e349acfecad982c533},
 keywords = {Empirical Study},
 pages = {91--98},
 volume = {49},
 number = {2},
 journal = {Journal of Hand Surgery},
 doi = {10.1016/j.jhsa.2023.10.019}
}


@article{Corcoran.2018,
 abstract = {Language and speech are the primary source of data for psychiatrists to diagnose and treat mental disorders. In psychosis, the very structure of language can be disturbed, including semantic coherence (e.g., derailment and tangentiality) and syntactic complexity (e.g., concreteness). Subtle disturbances in language are evident in schizophrenia even prior to first psychosis onset, during prodromal stages. Using computer-based natural language processing analyses, we previously showed that, among English-speaking clinical (e.g., ultra) high-risk youths, baseline reduction in semantic coherence (the flow of meaning in speech) and in syntactic complexity could predict subsequent psychosis onset with high accuracy. Herein, we aimed to cross-validate these automated linguistic analytic methods in a second larger risk cohort, also English-speaking, and to discriminate speech in psychosis from normal speech. We identified an automated machine-learning speech classifier -- comprising decreased semantic coherence, greater variance in that coherence, and reduced usage of possessive pronouns -- that had an 83{\%} accuracy in predicting psychosis onset (intra-protocol), a cross-validated accuracy of 79{\%} of psychosis onset prediction in the original risk cohort (cross-protocol), and a 72{\%} accuracy in discriminating the speech of recent-onset psychosis patients from that of healthy individuals. The classifier was highly correlated with previously identified manual linguistic predictors. Our findings support the utility and validity of automated natural language processing methods to characterize disturbances in semantics and syntax across stages of psychotic disorder. The next steps will be to apply these methods in larger risk cohorts to further test reproducibility, also in languages other than English, and identify sources of variability. This technology has the potential to improve prediction of psychosis outcome among at-risk youths and identify linguistic targets for remediation and preventive intervention. More broadly, automated linguistic analysis can be a powerful tool for diagnosis and treatment across neuropsychiatry. {\copyright} 2018 World Psychiatric Association},
 author = {Corcoran, C. M. and Carrillo, F. and Fern{\'a}ndez-Slezak, D. and Bedi, G. and Klim, C. and Javitt, D. C. and Bearden, C. E. and Cecchi, G. A.},
 year = {2018},
 title = {Prediction of psychosis across protocols and risk cohorts using automated language analysis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040731923&doi=10.1002%2fwps.20491&partnerID=40&md5=16a7dfd011f84310f5bb2bdb2bc1ed74},
 keywords = {ML implementation},
 pages = {67--75},
 volume = {17},
 number = {1},
 journal = {World Psychiatry},
 doi = {10.1002/wps.20491},
 file = {Corcoran, Carrillo et al. 2018 - Prediction of psychosis across protocols:Attachments/Corcoran, Carrillo et al. 2018 - Prediction of psychosis across protocols.pdf:application/pdf}
}


@article{Cotella.2023,
 abstract = {Background: Assessment of left ventricular ejection fraction (LVEF) and global longitudinal strain (GLS) plays a key role in the diagnosis of cardiac amyloidosis (CA). However, manual measurements are time consuming and prone to variability. We aimed to assess whether fully automated artificial intelligence (AI) calculation of LVEF and GLS provide similar estimates and can identify abnormalities in agreement with conventional manual methods, in patients with pre-clinical and clinical CA. Methods: We identified 51 patients (age 80~$\pm$~10 years, 53{\%} male) with confirmed CA according to guidelines, who underwent echocardiography before and/or at the time of CA diagnosis (median (IQR) time between observations 3.87 (1.93, 5.44 years). LVEF and GLS were quantified from the apical 2- and 4-chamber views using both manual and fully automated methods (EchoGo Core 2.0, Ultromics). Inter-technique agreement was assessed using linear regression and Bland-Altman analyses and two-way ANOVA. The diagnostic accuracy and time for detecting abnormalities (defined as LVEF $\leq$ 50{\%} and GLS $\geq$ $-$15.1{\%}, respectively) using AI was assessed by comparisons to manual measurements as a reference. Results: There were no significant differences in manual and automated LVEF and GLS values in either pre-CA (p~=.791 and p~=.105, respectively) or at diagnosis (p~=.463 and p~=.722). The two methods showed strong correlation on both the pre-CA (r~=.78 and r~=.83) and CA echoes (r~=.74 and r~=.80) for LVEF and GLS, respectively. The sensitivity and specificity of AI-derived indices for detecting abnormal LVEF were 83{\%} and 86{\%}, respectively, in the pre-CA echo and 70{\%} and 79{\%} at CA diagnosis. The sensitivity and specificity of AI-derived indices for detecting abnormal GLS was 82{\%} and 86{\%} in the pre-CA echo and 100{\%} and 67{\%} at the time of CA diagnosis. There was no significant difference in the relationship between LVEF (p~=.99) and GLS (p~=.19) and time to abnormality between the two methods. Conclusion: Fully automated AI-calculated LVEF and GLS are comparable to manual measurements in patients pre-CA and at the time of CA diagnosis. The widespread implementation of automated LVEF and GLS may allow for more rapid assessment in different disease states with comparable accuracy and reproducibility to manual methods. {\copyright} 2023 Wiley Periodicals LLC.},
 author = {Cotella, J. I. and Slivnick, J. A. and Sanderson, E. and Singulane, C. and O'Driscoll, J. and Asch, F. M. and Addetia, K. and Woodward, G. and Lang, R. M.},
 year = {2023},
 title = {Artificial intelligence based left ventricular ejection fraction and global longitudinal strain in cardiac amyloidosis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146089016&doi=10.1111%2fecho.15516&partnerID=40&md5=4f0480164dba17b693a8da8adb286b74},
 keywords = {ML implementation},
 pages = {188--195},
 volume = {40},
 number = {3},
 journal = {Echocardiography},
 doi = {10.1111/echo.15516}
}


@article{Crook.2023,
 abstract = {Purpose: The purpose of this study was to analyze the quality and readability of the information generated by an online artificial intelligence (AI) platform regarding 4 common hand surgeries and to compare AI-generated responses to those provided in the informational articles published by the American Society for Surgery of the Hand (ASSH) HandCare website. Methods: An open AI model (ChatGPT) was used to answer questions commonly asked by patients on 4 common hand surgeries (carpal tunnel release, cubital tunnel release, trigger finger release, and distal radius fracture fixation). These answers were evaluated for medical accuracy, quality and readability and compared to answers derived from the ASSH HandCare materials. Results: For the AI model, the Journal of the American Medical Association benchmark criteria score was 0/4, and the DISCERN score was 58 (considered good). The areas in which the AI model lost points were primarily related to the lack of attribution, reliability and currency of the source material. For AI responses, the mean Flesch Kinkaid Reading Ease score was 15, and the Flesch Kinkaid Grade Level was 34, which is considered to be college level. For comparison, ASSH HandCare materials scored 3/4 on the Journal of the American Medical Association Benchmark, 71 on DISCERN (excellent), 9 on Flesch Kinkaid Grade Level, and 60 on Flesch Kinkaid Reading Ease score (eighth/ninth grade level). Conclusion: An AI language model (ChatGPT) provided generally high-quality answers to frequently asked questions relating to the common hand procedures queried, but it is unclear when or where these answers came from without citations to source material. Furthermore, a high reading level was required to comprehend the information presented. The AI software repeatedly referenced the need to discuss these questions with a surgeon, the importance of shared decision-making and individualized care, and compliance with surgeon treatment recommendations. Clinical relevance: As novel AI applications become increasingly mainstream, hand surgeons must understand the limitations and ramifications these technologies have for patient care. {\copyright} 2023 American Society for Surgery of the Hand},
 author = {Crook, B. S. and Park, C. N. and Hurley, E. T. and Richard, M. J. and Pidgeon, T. S.},
 year = {2023},
 title = {Evaluation of Online Artificial Intelligence-Generated Information on Common Hand Procedures},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171136570&doi=10.1016%2fj.jhsa.2023.08.003&partnerID=40&md5=3d0573ff1338c128fab13eb7400b5a4c},
 keywords = {Empirical Study},
 pages = {1122--1127},
 volume = {48},
 number = {11},
 journal = {Journal of Hand Surgery},
 doi = {10.1016/j.jhsa.2023.08.003}
}


@article{Crowley.2021,
 abstract = {Biomarkers predict World Trade Center-Lung Injury (WTC-LI); however, there remains unaddressed multicollinearity in our serum cytokines, chemokines, and high-throughput platform datasets used to phenotype WTC-disease. To address this concern, we used automated, machine-learning, high-dimensional data pruning, and validated identified biomarkers. The parent cohort consisted of male, never-smoking firefighters with WTC-LI (FEV1, {\%}Pred{\textless} lower limit of normal (LLN); n = 100) and controls (n = 127) and had their biomarkers assessed. Cases and controls (n = 15/group) underwent untargeted metabolomics, then feature selection performed on metabolites, cytokines, chemokines, and clinical data. Cytokines, chemokines, and clinical biomarkers were validated in the non-overlapping parent- cohort via binary logistic regression with 5-fold cross validation. Random forests of metabolites (n = 580), clinical biomarkers (n = 5), and previously assayed cytokines, chemokines (n = 106) identified that the top 5{\%} of biomarkers important to class separation included pigment epithelium-derived factor (PEDF), macrophage derived chemokine (MDC), systolic blood pressure, macrophage inflammatory protein-4 (MIP-4), growth-regulated oncogene protein (GRO), monocyte chemoattractant protein-1 (MCP-1), apolipoprotein- AII (Apo-AII), cell membrane metabolites (sphingolipids, phospholipids), and branchedchain amino acids. Validated models via confounder-adjusted (age on 9/11, BMI, exposure, and pre-9/11 FEV1, {\%}Pred) binary logistic regression had AUCROC [0.90(0.84-0.96)]. Decreased PEDF and MIP-4, and increased Apo-AII were associated with increased odds of WTC-LI. Increased GRO, MCP-1, and simultaneously decreased MDC were associated with decreased odds of WTC-LI. In conclusion, automated data pruning identified novel WTC-LI biomarkers; performance was validated in an independent cohort. One biomarker -PEDF, an antiangiogenic agent-is a novel, predictive biomarker of particulate-matterrelated lung disease. Other biomarkers-GRO, MCP-1, MDC, MIP-4-reveal immune cell involvement in WTC-LI pathogenesis. Findings of our automated biomarker identification warrant further investigation into these potential pharmacotherapy targets. {\copyright} 2021 Public Library of Science. All rights reserved.},
 author = {Crowley, G. and Kim, J. and Kwon, S. and Lam, R. and Prezant, D. J. and Liu, M. and Nolan, A.},
 year = {2021},
 title = {PEDF, a pleiotropic WTC-LI biomarker: Machine learning biomarker identification and validation},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111240205&doi=10.1371%2fjournal.pcbi.1009144&partnerID=40&md5=94c45e10e51bebedda94c7341e90da11},
 keywords = {ML implementation},
 volume = {17},
 number = {7},
 journal = {PLoS Computational Biology},
 doi = {10.1371/journal.pcbi.1009144},
 file = {Crowley, Kim et al. 2021 - PEDF, a pleiotropic WTC-LI biomarker:Attachments/Crowley, Kim et al. 2021 - PEDF, a pleiotropic WTC-LI biomarker.pdf:application/pdf}
}


@inproceedings{Culligan.2016,
 abstract = {Computer science courses have been shown to have a low rate of student retention. There are many possible reasons for this, and our research group have had considerable success in pinpointing the factors that influence outcome when learning to program. The earlier we are able to make these predictions, the earlier a teacher can intervene and provide help to an at-risk student, before they fail and/or drop out. PreSS (Predict Student Success) is a semi-automated machine learning system developed between 2002 and 2006 that can predict the performance of students on an introductory programming module with 80{\%} accuracy, after minimal programming exposure. Between 2013 and 2015, a fully automated web-based system was developed, known as PreSS{\#}, that replicates the original system but provides: a streamlined user interface; an easy acquisition process; automatic modeling; and reporting. Currently, the reporting component of PreSS{\#} outputs a value that indicates if the student is a {\textquotedbl}weak{\textquotedbl} or {\textquotedbl}strong{\textquotedbl} programmer, along with a measure of confidence in the prediction. This paper will discuss the development of VEAP: a Visualisation Engine and Analyser for PreSS{\#}. This software provides a comprehensive data visualisation and user interface, that will allow teachers to view data gathered and processed about institutions, classes and individual students, and provides access to further user-defined analysis, to allow a teacher to view how an intervention could influence a student's predicted outcome. {\copyright} 2016 ACM.},
 author = {Culligan, N. and Quille, K. and Bergin, S.},
 title = {VEAP: A visualisation engine and analyzer for PreSS{\#}},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014655502&doi=10.1145%2f2999541.2999553&partnerID=40&md5=a791641f6ac906fd7198b92e7c894117},
 keywords = {ML implementation},
 pages = {130--134},
 year = {2016},
 doi = {10.1145/2999541.2999553}
}


@inproceedings{Dai.2022,
 abstract = {Most AI projects start with a Python notebook running on a single laptop; however, one usually needs to go through a mountain of pains to scale it to handle larger dataset (for both experimentation and production deployment). These usually entail many manual and error-prone steps for the data scientists to fully take advantage of the available hardware resources (e.g., SIMD instructions, multi-processing, quantization, memory allocation optimization, data partitioning, distributed computing, etc.). To address this challenge, we have open sourced BigDL 2.0 at https://github.com/intel-analytics/BigDL/ under Apache 2.0 license (combining the original BigDL [19] and Analytics Zoo [18] projects); using BigDL 2.0, users can simply build conventional Python notebooks on their laptops (with possible AutoML support), which can then be transparently accelerated on a single node (with up-to 9.6x speedup in our experiments), and seamlessly scaled out to a large cluster (across several hundreds servers in real-world use cases). BigDL 2.0 has already been adopted by many real-world users (such as Mastercard, Burger King, Inspur, etc.) in production. {\copyright} 2022 IEEE.},
 author = {Dai, J. J. and Ding, D. and Shi, D. and Huang, S. and Wang, J. and Qiu, X. and Huang, K. and Song, G. and Wang, Y. and Gong, Q. and Song, J. and Yu, S. and Zheng, L. and Chen, Y. and Deng, J.},
 title = {BigDL 2.0: Seamless Scaling of AI Pipelines from Laptops to Distributed Cluster},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143485633&doi=10.1109%2fCVPR52688.2022.02076&partnerID=40&md5=8f9e677d24e927d541fc02a8728461ec},
 keywords = {Artefact Design},
 pages = {21407--21414},
 year = {2022},
 doi = {10.1109/CVPR52688.2022.02076}
}


@article{Das.2023,
 abstract = {Alzheimer's disease (AD) is a severe, growing, multifactorial disorder affecting millions of people worldwide characterized by cognitive decline and neurodegeneration. The accumulation of tau protein into paired helical filaments is one of the major pathological hallmarks of AD and has gained the interest of researchers as a potential drug target to treat AD. Lately, Artificial Intelligence (AI) has revolutionized the drug discovery process by speeding it up and reducing the overall cost. As a part of our continuous effort to identify potential tau aggregation inhibitors, and leveraging the power of AI, in this study, we used a fully automated AI-assisted ligand-based virtual screening tool, PyRMD to screen a library of 12 million compounds from the ZINC database to identify potential tau aggregation inhibitors. The preliminary hits from virtual screening were filtered for similar compounds and pan-assay interference compounds (the compounds containing reactive functional groups which can interfere with the assays) using RDKit. Further, the selected compounds were prioritized based on their molecular docking score with the binding pocket of tau where the binding pockets were identified using replica exchange molecular dynamics simulation. Thirty-three compounds showing good docking scores for all the tau clusters were selected and were further subjected to in silico pharmacokinetic prediction. Finally, top 10 compounds were selected for molecular dynamics simulation and MMPBSA binding free energy calculations resulting in the identification of UNK{\_}175, UNK{\_}1027, UNK{\_}1172, UNK{\_}1173, UNK{\_}1237, UNK{\_}1518, and UNK{\_}2181 as potential tau aggregation inhibitors. {\copyright} 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.},
 author = {Das, B. and Mathew, A. T. and Baidya, A.T.K. and Devi, B. and Salmon, R. R. and Kumar, R.},
 year = {2023},
 title = {Artificial intelligence assisted identification of potential tau aggregation inhibitors: ligand- and structure-based virtual screening, in silico ADME, and molecular dynamics study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151958501&doi=10.1007%2fs11030-023-10645-3&partnerID=40&md5=65c5a09898dffd04549e4acfc07e33b1},
 keywords = {ML implementation},
 journal = {Molecular Diversity},
 doi = {10.1007/s11030-023-10645-3},
 file = {Das, Mathew et al. 2023 - Artificial intelligence assisted identification:Attachments/Das, Mathew et al. 2023 - Artificial intelligence assisted identification.pdf:application/pdf}
}


@inproceedings{Datta.2023,
 abstract = {As the field of multimedia computing has grown rapidly, so has the need for larger datasets[5] and increased modeling capacity. Navigating this complex landscape often necessitates the use of sophisticated tools and cloud architectures, which all need to be addressed before the actual research commences. Recently, AutoML, an innovation previously exclusive to tabular data, has expanded to encompass multimedia data. This development has the potential to greatly streamline the research process, allowing researchers to shift their focus from model construction to the core content of their problems. In doing so, AutoML not only optimizes resource utilization but also boosts the reproducibility of results. The aim of this tutorial is to acquaint the multimedia community with AutoML technologies, underscoring their advantages and their practical applications in the field. {\copyright} 2023 Owner/Author.},
 author = {Datta, D. and Friedland, G.},
 title = {Efficient Multimedia Computing: Unleashing the Power of AutoML},
 keywords = {Case Study;Technical Review},
 pages = {9700--9701},
 year = {2023},
 doi = {10.1145/3581783.3613858},
 file = {Datta, Friedland 2023 - Efficient Multimedia Computing:Attachments/Datta, Friedland 2023 - Efficient Multimedia Computing.pdf:application/pdf}
}


@article{Davies.2022,
 abstract = {Background: Measurement of cardiac structure and function from images (e.g. volumes, mass and derived parameters such as left ventricular (LV) ejection fraction [LVEF]) guides care for millions. This is best assessed using cardiovascular magnetic resonance (CMR), but image analysis is currently performed by individual clinicians, which introduces error. We sought to develop a machine learning algorithm for volumetric analysis of CMR images with demonstrably better precision than human analysis. Methods: A fully automated machine learning algorithm was trained on 1923 scans (10 scanner models, 13 institutions, 9 clinical conditions, 60,000 contours) and used to segment the LV blood volume and myocardium. Performance was quantified by measuring precision on an independent multi-site validation dataset with multiple pathologies with n = 109 patients, scanned twice. This dataset was augmented with a further 1277 patients scanned as part of routine clinical care to allow qualitative assessment of generalization ability by identifying mis-segmentations. Machine learning algorithm (`machine') performance was compared to three clinicians (`human') and a commercial tool (cvi42, Circle Cardiovascular Imaging). Findings: Machine analysis was quicker (20~s per patient) than human (13~min). Overall machine mis-segmentation rate was 1 in 479 images for the combined dataset, occurring mostly in rare pathologies not encountered in training. Without correcting these mis-segmentations, machine analysis had superior precision to three clinicians (e.g. scan-rescan coefficients of variation of human vs machine: LVEF 6.0{\%} vs 4.2{\%}, LV mass 4.8{\%} vs. 3.6{\%}; both P {\textless} 0.05), translating to a 46{\%} reduction in required trial sample size using an LVEF endpoint. Conclusion: We present a fully automated algorithm for measuring LV structure and global systolic function that betters human performance for speed and precision. {\copyright} 2022, The Author(s).},
 author = {Davies, R. H. and Augusto, J. B. and Bhuva, A. and Xue, H. and Treibel, T. A. and Ye, Y. and Hughes, R. K. and Bai, W. and Lau, C. and Shiwani, H. and Fontana, M. and Kozor, R. and Herrey, A. and Lopes, L. R. and Maestrini, V. and Rosmini, S. and Petersen, S. E. and Kellman, P. and Rueckert, D. and Greenwood, J. P. and Captur, G. and Manisty, C. and Schelbert, E. and Moon, J. C.},
 year = {2022},
 title = {Precision measurement of cardiac structure and function in cardiovascular magnetic resonance using machine learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126220336&doi=10.1186%2fs12968-022-00846-4&partnerID=40&md5=b5698474d61bdfed112648064a3094dd},
 keywords = {ML implementation},
 volume = {24},
 number = {1},
 journal = {Journal of Cardiovascular Magnetic Resonance},
 doi = {10.1186/s12968-022-00846-4},
 file = {Davies, Augusto et al. 2022 - Precision measurement of cardiac structure:Attachments/Davies, Augusto et al. 2022 - Precision measurement of cardiac structure.pdf:application/pdf}
}


@article{DeinhardtEmmer.2023,
 abstract = {During cellular senescence, persistent growth arrest and changes in protein expression programs are accompanied by a senescence-associated secretory phenotype (SASP). In this study, we detected the upregulation of the SASP-related protein dipeptidyl peptidase 4 (DDP4) in human primary lung cells rendered senescent by exposure to ionizing radiation. DPP4 is an exopeptidase that plays a crucial role in the cleavage of various proteins, resulting in the loss of N-terminal dipeptides and proinflammatory effects. Interestingly, our data revealed an association between severe coronavirus disease 2019 (COVID-19) and DDP4, namely that DPP4 levels increased in the plasma of patients with COVID-19 and were correlated with age and disease progression. Although we could not determine the direct effect of DDP4 on viral replication, mechanistic studies in cell culture revealed a negative impact on the expression of the tight junction protein zonula occludens-1 (ZO-1), which contributes to epithelial barrier function. Mass spectrometry analysis indicated that DPP4 overexpressing cells exhibited a decrease in ZO-1 and increased expression of pro-inflammatory cytokines and chemokines. By investigating the effect of DPP4 on the barrier function of human primary cells, we detected an increase in ZO1 using DPP4 inhibitors. These results provide an important contribution to our understanding of DPP4 in the context of senescence, suggesting that DPP4 plays a major role as part of the SASP. Our results provide evidence that cellular senescence, a hallmark of aging, has an important impact on respiratory infections.},
 author = {Deinhardt-Emmer, S. and Deshpande, S. and Kitazawa, K. and {Herman, AB} and Bons, J. and Rose, J. P. and Kumar, P. A. and Anerillas, C. and Neri, F. and Ciotlos, S. and Perez, K. and K{\"o}se-Vogel, N. and H{\"a}der, A. and Abdelmohsen, K. and L{\"o}ffler, B. and Gorospe, M. and Melov, S. and Desprez, P. Y. and Furman, D. and Schilling, B. and Campisi, J.},
 year = {2023},
 title = {Role of the Senescence-Associated Factor Dipeptidyl Peptidase 4 in the Pathogenesis of SARS-CoV-2 Infection},
 keywords = {Experiment},
 issn = {2152-5250},
 journal = {AGING AND DISEASE},
 doi = {10.14336/AD.2023.0812},
 file = {Deinhardt-Emmer, Deshpande et al. 2023 - Role of the Senescence-Associated Factor:Attachments/Deinhardt-Emmer, Deshpande et al. 2023 - Role of the Senescence-Associated Factor.pdf:application/pdf}
}


@inproceedings{Deitke.2020,
 abstract = {Visual recognition ecosystems (e.g. ImageNet, Pascal, COCO) have undeniably played a prevailing role in the evolution of modern computer vision. We argue that interactive and embodied visual AI has reached a stage of development similar to visual recognition prior to the advent of these ecosystems. Recently, various synthetic environments have been introduced to facilitate research in embodied AI. Notwithstanding this progress, the crucial question of how well models trained in simulation generalize to reality has remained largely unanswered. The creation of a comparable ecosystem for simulation-to-real embodied AI presents many challenges: (1) the inherently interactive nature of the problem, (2) the need for tight alignments between real and simulated worlds, (3) the difficulty of replicating physical conditions for repeatable experiments, (4) and the associated cost. In this paper, we introduce ROBOTHOR to democratize research in interactive and embodied visual AI. ROBOTHOR offers a framework of simulated environments paired with physical counterparts to systematically explore and overcome the challenges of simulation-to-real transfer, and a platform where researchers across the globe can remotely test their embodied models in the physical world. As a first benchmark, our experiments show there exists a significant gap between the performance of models trained in simulation when they are tested in both simulations and their carefully constructed physical analogs. We hope that ROBOTHOR will spur the next stage of evolution in embodied computer vision. {\copyright} 2020 IEEE},
 author = {Deitke, M. and Han, W. and Herrasti, A. and Kembhavi, A. and Kolve, E. and Mottaghi, R. and Salvador, J. and Schwenk, D. and VanderBilt, E. and Wallingford, M. and Weihs, L. and Yatskar, M. and Farhadi, A.},
 title = {Robothor: An open simulation-to-real embodied AI platform},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090184268&doi=10.1109%2fCVPR42600.2020.00323&partnerID=40&md5=ac7d7b0f3ace2e56f5646213a8298a5b},
 keywords = {Artefact Design},
 pages = {3161--3171},
 year = {2020},
 doi = {10.1109/CVPR42600.2020.00323}
}


@article{Dhaya.2022,
 abstract = {Artificial Intelligence (AI) systems are computational simulations that are ``educated'' using knowledge and individual expert participation to replicate a decision that a professional would make provided the same data. A model tries to simulate a specific decision loop that several scientists would take if they had access to all kinds of knowledge. To convey a model, you make a model asset in AI Platform Prediction, make a variant of that model and, at that point, interface the model form to the model record put away in Cloud Storage. AI and DB information sharing are essential for cutting-edge processing for DBMS innovation. The inspirations promoting their incorporation advances incorporate the requirement for admittance to a lot of data that is shared information handling, effective administration of data as information, and astute preparation of information. Notwithstanding these inspirations, the plan for a smart information base interface (IDI) was likewise spurred by the craving to save the considerable speculation spoke to by most existing data sets. A few general ways to deal with the connectivity of AI and databases and different improvements in the area of clever information bases were already examined and announced in this paper. {\copyright} 2022 Taiwan Academic Network Management Committee. All rights reserved.},
 author = {Dhaya, R. and Kanthavel, R. and Venusamy, K.},
 year = {2022},
 title = {AI Based Learning Model Management Framework for Private Cloud Computing},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146768833&doi=10.53106%2f160792642022122307017&partnerID=40&md5=76ba1ca6eb0b9d99e550b5e23301089e},
 keywords = {Technical Review},
 pages = {1633--1642},
 volume = {23},
 number = {7},
 journal = {Journal of Internet Technology},
 doi = {10.53106/160792642022122307017}
}


@article{Dieci.2021,
 abstract = {In recent decades, the increasing interest in the field of immunotherapy has fostered an intense investigation of the breast cancer (BC) immune microenvironment. In this context, tumor-infiltrating lymphocytes (TILs) have emerged as a clinically relevant and highly reproducible biomarker capable of affecting BC prognosis and response to treatment. Indeed, the evaluation of TILs on primary tumors proved to be strongly prognostic in triple-negative (TN) BC patients treated with either adjuvant or neoadjuvant chemotherapy, as well as in early TNBC patients not receiving any systemic treatment, thus gaining level-1b evidence in this setting. In addition, a strong relationship between TILs and pathologic complete response after neoadjuvant chemotherapy has been reported in all BC subtypes and the prognostic role of higher TILs in early HER2-positive breast cancer patients has also been demonstrated. The interest in BC immune infiltrates has been further fueled by the introduction of the first immune checkpoint inhibitors in the treatment armamentarium of advanced TNBC in patients with PD-L1-positive status by FDA-approved assays. However, despite these advances, a biomarker capable of reliably and exhaustively predicting immunotherapy benefit in BC is still lacking, highlighting the imperative need to further deepen this issue. Finally, more comprehensive evaluation of immune infiltrates integrating both the quantity and quality of tumor-infiltrating immune cells and incorporation of TILs in composite scores encompassing other clinically or biologically relevant biomarkers, as well as the adoption of software-based and/or machine learning platforms for a more comprehensive characterization of BC immune infiltrates, are emerging as promising strategies potentially capable of optimizing patient selection and stratification in the research field. In the present review, we summarize available evidence and recent updates on immune infiltrates in BC, focusing on current clinical applications, potential clinical implications and major unresolved issues. {\copyright} 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
 author = {Dieci, M. V. and Miglietta, F. and Guarneri, V.},
 year = {2021},
 title = {Immune infiltrates in breast cancer: Recent updates and clinical implications},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100475107&doi=10.3390%2fcells10020223&partnerID=40&md5=c92436513500464e5ce47a41cf27ab04},
 keywords = {Literature Review},
 pages = {1--27},
 volume = {10},
 number = {2},
 journal = {Cells},
 doi = {10.3390/cells10020223},
 file = {Dieci, Miglietta et al. 2021 - Immune infiltrates in breast cancer:Attachments/Dieci, Miglietta et al. 2021 - Immune infiltrates in breast cancer.pdf:application/pdf}
}


@article{Ding.2024,
 abstract = {Objective: To establish an automatic diagnostic system based on machine learning for preliminarily analysis of urodynamic study applying in lower urinary tract dysfunction (LUTD). Methods: The eight most common conditions of LUTDs were included in the present study. A total of 527 eligible patients with complete data, from the year of 2015 to 2020, were enrolled in this study. In total, two global parameters (patients' age and sex) and 13 urodynamic parameters were considered to be the input for machine learning algorithms. Three machine learning approaches were applied and evaluated in this study, including Decision Tree (DT), Logistic Regression (LR), and Support Vector Machine (SVM). Results: By applying machine learning algorithms into the 8 common LUTDs, the DT models achieved the AUC of 0.63--0.98, the LR models achieved the AUC of 0.73--0.99, and the SVM models achieved the AUC of 0.64--1.00. For mutually exclusive diagnoses of underactive detrusor and acontractile detrusor, we developed a classification model that classifies the patients into either of these two diseases or double-negative class. For this classification method, the DT models achieved the AUC of 0.82--0.85 and the SVM models achieved the AUC of 0.86--0.90. Among all these models, the LR and the SVM models showed better performance. The best model of these diagnostic tasks achieved an average AUC of 0.90 (0.90 $\pm$ 0.08). Conclusions: An automatic diagnostic system was developed using three machine learning models in urodynamic studies. This automated machine learning process could lead to promising assistance and enhancements of diagnosis and provide more useful reference for LUTD treatment. {\copyright} 2023, The Author(s), under exclusive licence to Springer Nature B.V.},
 author = {Ding, Z. and Zhang, W. and Wang, H. and Ke, H. and Su, D. and Wang, Q. and Bian, K. and Su, F. and Xu, K.},
 year = {2024},
 title = {An automatic diagnostic system for the urodynamic study applying in lower urinary tract dysfunction},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172926879&doi=10.1007%2fs11255-023-03795-8&partnerID=40&md5=ca45169a895d9fab6d896ba2a2580b0c},
 keywords = {ML implementation},
 pages = {441--449},
 volume = {56},
 number = {2},
 journal = {International Urology and Nephrology},
 doi = {10.1007/s11255-023-03795-8},
 file = {Ding, Zhang et al. 2024 - An automatic diagnostic system:Attachments/Ding, Zhang et al. 2024 - An automatic diagnostic system.pdf:application/pdf}
}


@article{Dixit.2021,
 abstract = {Artificial Intelligence (AI) is the notion of machines mimicking complex cognitive functions usually associated with humans, such as reasoning, predicting, planning, and problem-solving. With constantly growing repositories of data, improving algorithmic sophistication and faster computing resources, AI is becoming increasingly integrated into everyday use. In healthcare, AI represents an opportunity to increase safety, improve quality, and reduce the burden on increasingly overstretched systems. As applications expand, the need for responsible oversight and governance becomes even more important. Artificial intelligence in the delivery of healthcare carries new opportunities and challenges, including the need for greater transparency, the impact AI tools may have on a larger number of patients and families, and potential biases that may be introduced by the way an AI platform was developed and built. This study provides practical guidance in the development and implementation of AI applications in healthcare, with a focus on risk identification, management, and mitigation. {\copyright} 2021 The Canadian College of Health Leaders. All rights reserved.},
 author = {Dixit, A. and Quaglietta, J. and Gaulton, C.},
 year = {2021},
 title = {Preparing for the future: How organizations can prepare boards, leaders, and risk managers for artificial intelligence},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115074768&doi=10.1177%2f08404704211037995&partnerID=40&md5=064c2646809f5b2131398d839c103fa9},
 keywords = {Case Study;Technical Review},
 pages = {346--352},
 volume = {34},
 number = {6},
 journal = {Healthcare Management Forum},
 doi = {10.1177/08404704211037995},
 file = {Dixit, Quaglietta et al. 2021 - Preparing for the future:Attachments/Dixit, Quaglietta et al. 2021 - Preparing for the future.pdf:application/pdf}
}


@article{Dournes.2022,
 abstract = {Background Chest computed tomography (CT) remains the imaging standard for demonstrating cystic fibrosis (CF) airway structural disease in vivo. However, visual scoring systems as an outcome measure are time consuming, require training and lack high reproducibility. Our objective was to validate a fully automated artificial intelligence (AI)-driven scoring system of CF lung disease severity. Methods Data were retrospectively collected in three CF reference centres, between 2008 and 2020, in 184 patients aged 4--54 years. An algorithm using three 2D convolutional neural networks was trained with 78 patients' CT scans (23530 CT slices) for the semantic labelling of bronchiectasis, peribronchial thickening, bronchial mucus, bronchiolar mucus and collapse/consolidation. 36 patients' CT scans (11435 CT slices) were used for testing versus ground-truth labels. The method's clinical validity was assessed in an independent group of 70 patients with or without lumacaftor/ivacaftor treatment (n=10 and n=60, respectively) with repeat examinations. Similarity and reproducibility were assessed using the Dice coefficient, correlations using the Spearman test, and paired comparisons using the Wilcoxon rank test. Results The overall pixelwise similarity of AI-driven versus ground-truth labels was good (Dice 0.71). All AI-driven volumetric quantifications had moderate to very good correlations to a visual imaging scoring (p{\textless}0.001) and fair to good correlations to forced expiratory volume in 1 s {\%} predicted at pulmonary function tests (p{\textless}0.001). Significant decreases in peribronchial thickening (p=0.005), bronchial mucus (p=0.005) and bronchiolar mucus (p=0.007) volumes were measured in patients with lumacaftor/ivacaftor. Conversely, bronchiectasis (p=0.002) and peribronchial thickening (p=0.008) volumes increased in patients without lumacaftor/ivacaftor. The reproducibility was almost perfect (Dice {\textgreater}0.99). Conclusion AI allows fully automated volumetric quantification of CF-related modifications over an entire lung. The novel scoring system could provide a robust disease outcome in the era of effective CF transmembrane conductance regulator modulator therapy. {\copyright} 2022 European Respiratory Society. All rights reserved.},
 author = {Dournes, G. and Hall, C. S. and Willmering, M. M. and Brody, A. S. and Macey, J. and Bui, S. and de Senneville, B. D. and Berger, P. and Laurent, F. and Benlala, I. and Woods, J. C.},
 year = {2022},
 title = {Artificial intelligence in computed tomography for quantifying lung changes in the era of CFTR modulators},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125682665&doi=10.1183%2f13993003.00844-2021&partnerID=40&md5=d0804449d783fbde66883a3f00c3a51f},
 keywords = {ML implementation},
 volume = {59},
 number = {3},
 journal = {European Respiratory Journal},
 doi = {10.1183/13993003.00844-2021},
 file = {Dournes, Hall et al. 2022 - Artificial intelligence in computed tomography:Attachments/Dournes, Hall et al. 2022 - Artificial intelligence in computed tomography.pdf:application/pdf}
}


@inproceedings{Drozdal.2020,
 abstract = {We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies - qualitative interviews, a controlled experiment, and a card-sorting task - to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool. {\copyright} ACM.},
 author = {Drozdal, J. and Weisz, J. and Wang, D. and Dass, G. and Yao, B. and Zhao, C. and Muller, M. and Ju, L. and Su, H.},
 title = {Trust in AutoML},
 keywords = {Empirical Study},
 pages = {297--307},
 year = {2020},
 doi = {10.1145/3377325.3377501},
 file = {Drozdal, Weisz et al 2020 - Trust in AutoML:Attachments/Drozdal, Weisz et al 2020 - Trust in AutoML.pdf:application/pdf}
}


@article{Du.2021,
 abstract = {Schizophrenia is a neurocognitive illness of synaptic and brain network-level dysconnectivity that often reaches a persistent chronic stage in many patients. Subtle language deficits are a core feature even in the early stages of schizophrenia. However, the primacy of language network dysconnectivity and language-related genetic variants in the observed phenotype in early stages of illness remains unclear. This study used two independent schizophrenia dataset consisting of 138 and 53 drug-naive first-episode schizophrenia (FES) patients, and 112 and 56 healthy controls, respectively. A brain-wide voxel-level functional connectivity analysis was conducted to investigate functional dysconnectivity and its relationship with illness duration. We also explored the association between critical language-related genetic (such as FOXP2) mutations and the altered functional connectivity in patients. We found elevated functional connectivity involving Broca's area, thalamus and temporal cortex that were replicated in two FES datasets. In particular, Broca's area - anterior cingulate cortex dysconnectivity was more pronounced for patients with shorter illness duration, while thalamic dysconnectivity was predominant in those with longer illness duration. Polygenic risk scores obtained from FOXP2-related genes were strongly associated with functional dysconnectivity identified in patients with shorter illness duration. Our results highlight the criticality of language network dysconnectivity, involving the Broca's area in early stages of schizophrenia, and the role of language-related genes in this aberration, providing both imaging and genetic evidence for the association between schizophrenia and the determinants of language.},
 author = {Du, J. N. and Palaniyappan, L. and Liu, Z. W. and Cheng, W. and Gong, W. K. and Zhu, M. M. and Wang, J. J. and Zhang, J. and Feng, J. F.},
 year = {2021},
 title = {The genetic determinants of language network dysconnectivity in drug-naive early stage schizophrenia},
 keywords = {Empirical Study},
 volume = {7},
 number = {1},
 issn = {2334-265X},
 journal = {NPJ SCHIZOPHRENIA},
 doi = {10.1038/s41537-021-00141-8},
 file = {Du, Palaniyappan et al. 2021 - The genetic determinants of language:Attachments/Du, Palaniyappan et al. 2021 - The genetic determinants of language.pdf:application/pdf}
}


@inproceedings{Dugyala.2023,
 abstract = {Artificial intelligence (AI) is recognized as a valuable tool in various healthcare uses for diagnosing and therapeutic decision-making. Due to the tremendous rise in accessible data and processing capacity, machine learning (ML) models have performed well or greater than doctors in numerous activities. The AI platform needs to be transparent, resilient, and interpretable to adhere to the principles of trusted AI. Present ML systems are alluded to as black boxes because of the absence of understanding of the mechanics related to the decision-making procedures. As a result, before ML can be implemented into ordinary healthcare processes, its transparency and interpretability must be understood. To address this issue, this study presents a method for understanding the transparency and interpretability of ML suggestion systems. We particularly adapt the suggested technique to a chronic condition that is frequent in seniors: heart disease. The suggested approach illustrates the fundamental cause for these suggestions and increases patient trust and interpretability of ML models by assessing the influence of various patient features on the suggestions.  {\copyright} 2023 IEEE.},
 author = {Dugyala, R. and Singh, S. K. and {Saleh Al Ansari}, M. and Gunasundari, C. and Aswini, K. and Sandhya, G.},
 title = {Understanding AI: Interpretability and Transparency in Machine Learning Models},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187313620&doi=10.1109%2fUPCON59197.2023.10434665&partnerID=40&md5=190c3c7b7e8e76cf4ac2426ad963b2fe},
 keywords = {Artefact Design},
 pages = {613--617},
 year = {2023},
 doi = {10.1109/UPCON59197.2023.10434665}
}


@article{Eardley.2022,
 abstract = {We present a case study that informs the creation of a 'companion guide' providing transparency to potential non-expert users of a ubiquitous machine learning (ML) platform during the initial onboarding. Ubiquitous platforms (e.g., smart home systems, including smart meters and conversational agents) are increasingly commonplace and increasingly apply complex ML methods. Understanding how non-ML experts comprehend these platforms is important in supporting participants in making an informed choice about if and how they adopt these platforms. To aid this decision-making process, we created a companion guide for a home health platform through an iterative user-centred-design process, seeking additional input from platform experts at all stages of the process to ensure the accuracy of explanations. This user-centred and expert informed design process highlights the need to present the platform's entire ecosystem at an appropriate level for those with differing backgrounds to understand, in order to support informed consent and decision making. {\copyright} 2022 ACM.},
 author = {Eardley, R. and Mackinnon, S. and Tonkin, E. L. and Soubutts, E. and Ayobi, A. and Linington, J. and Tourte, G.J.L. and Gross, Z. B. and Bailey, D. J. and Knights, R. and Gooberman-Hill, R. and Craddock, I. and O'Kane, A. A.},
 year = {2022},
 title = {A Case Study Investigating a User-Centred and Expert Informed 'Companion Guide' for a Complex Sensor-based Platform},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134206636&doi=10.1145%2f3534625&partnerID=40&md5=730122bf81e15f3f1bf3617661163f66},
 keywords = {Case Study},
 volume = {6},
 number = {2},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 doi = {10.1145/3534625}
}


@inproceedings{Eimer.2023,
 abstract = {In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhead. As a result of our findings, we recommend a set of best practices for the RL community, which should result in stronger empirical results with fewer computational costs, better reproducibility, and thus faster progress. In order to encourage the adoption of these practices, we provide plug-and-play implementations of the tuning algorithms used in this paper at https://github.com/facebookresearch/how-to-autorl. {\copyright} 2023 Proceedings of Machine Learning Research. All rights reserved.},
 author = {Eimer, T. and Lindauer, M. and Raileanu, R.},
 title = {Hyperparameters in Reinforcement Learning and How To Tune Them},
 keywords = {Artefact Design},
 pages = {9104--9149},
 year = {2023},
 file = {Eimer, Lindauer et al 2023 - Hyperparameters in Reinforcement Learning:Attachments/Eimer, Lindauer et al 2023 - Hyperparameters in Reinforcement Learning.pdf:application/pdf}
}


@article{Elmore.2020,
 abstract = {Background: Given the worldwide spread of the 2019 Novel Coronavirus (COVID-19), there is an urgent need to identify risk and protective factors and expose areas of insufficient understanding. Emerging tools, such as the Rapid Evidence Map (rEM), are being developed to systematically characterize large collections of scientific literature. We sought to generate an rEM of risk and protective factors to comprehensively inform areas that impact COVID-19 outcomes for different sub-populations in order to better protect the public. Methods: We developed a protocol that includes a study goal, study questions, a PECO statement, and a process for screening literature by combining semi-automated machine learning with the expertise of our review team. We applied this protocol to reports within the COVID-19 Open Research Dataset (CORD-19) that were published in early 2020. SWIFT-Active Screener was used to prioritize records according to pre-defined inclusion criteria. Relevant studies were categorized by risk and protective status; susceptibility category (Behavioral, Physiological, Demographic, and Environmental); and affected sub-populations. Using tagged studies, we created an rEM for COVID-19 susceptibility that reveals: (1) current lines of evidence; (2) knowledge gaps; and (3) areas that may benefit from systematic review. Results: We imported 4,330 titles and abstracts from CORD-19. After screening 3,521 of these to achieve 99{\%} estimated recall, 217 relevant studies were identified. Most included studies concerned the impact of underlying comorbidities (Physiological); age and gender (Demographic); and social factors (Environmental) on COVID-19 outcomes. Among the relevant studies, older males with comorbidities were commonly reported to have the poorest outcomes. We noted a paucity of COVID-19 studies among children and susceptible sub-groups, including pregnant women, racial minorities, refugees/migrants, and healthcare workers, with few studies examining protective factors. Conclusion: Using rEM analysis, we synthesized the recent body of evidence related to COVID-19 risk and protective factors. The results provide a comprehensive tool for rapidly elucidating COVID-19 susceptibility patterns and identifying resource-rich/resource-poor areas of research that may benefit from future investigation as the pandemic evolves. {\copyright} Copyright {\copyright} 2020 Elmore, Schmidt, Lam, Howard, Tandon, Norman, Phillips, Shah, Patel, Albert, Taxman and Shah.},
 author = {Elmore, R. and Schmidt, L. and Lam, J. and Howard, B. E. and Tandon, A. and Norman, C. and Phillips, J. and Shah, M. and Patel, S. and Albert, T. and Taxman, D. J. and Shah, R. R.},
 year = {2020},
 title = {Risk and Protective Factors in the COVID-19 Pandemic: A Rapid Evidence Map},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097395540&doi=10.3389%2ffpubh.2020.582205&partnerID=40&md5=c1e50e241f8ca65fe88ae7d0750c7a09},
 keywords = {ML implementation},
 volume = {8},
 journal = {Frontiers in Public Health},
 doi = {10.3389/fpubh.2020.582205},
 file = {Elmore, Schmidt et al. 2020 - Risk and Protective Factors:Attachments/Elmore, Schmidt et al. 2020 - Risk and Protective Factors.pdf:application/pdf}
}


@article{Elmore.2023,
 abstract = {Background: Small unoccupied aircraft systems (UAS) are replacing or supplementing occupied aircraft and ground-based surveys in animal monitoring due to improved sensors, efficiency, costs, and logistical benefits. Numerous UAS and sensors are available and have been used in various methods. However, justification for selection or methods used are not typically offered in published literature. Furthermore, existing reviews do not adequately cover past and current UAS applications for animal monitoring, nor their associated UAS/sensor characteristics and environmental considerations. We present a systematic map that collects and consolidates evidence pertaining to UAS monitoring of animals. Methods: We investigated the current state of knowledge on UAS applications in terrestrial animal monitoring by using an accurate, comprehensive, and repeatable systematic map approach. We searched relevant peer-reviewed and grey literature, as well as dissertations and theses, using online publication databases, Google Scholar, and by request through a professional network of collaborators and publicly available websites. We used a tiered approach to article exclusion with eligible studies being those that monitor (i.e., identify, count, estimate, etc.) terrestrial vertebrate animals. Extracted metadata concerning UAS, sensors, animals, methodology, and results were recorded in Microsoft Access. We queried and catalogued evidence in the final database to produce tables, figures, and geographic maps to accompany this full narrative review, answering our primary and secondary questions. Review findings: We found 5539 articles from our literature searches of which 216 were included with extracted metadata categories in our database and narrative review. Studies exhibited exponential growth over time but have levelled off between 2019 and 2021 and were primarily conducted in North America, Australia, and Antarctica. Each metadata category had major clusters and gaps, which are described in the narrative review. Conclusions: Our systematic map provides a useful synthesis of current applications of UAS-animal related studies and identifies major knowledge clusters (well-represented subtopics that are amenable to full synthesis by a systematic review) and gaps (unreported or underrepresented topics that warrant additional primary research) that guide future research directions and UAS applications. The literature for the use of UAS to conduct animal surveys has expanded intensely since its inception in 2006 but is still in its infancy. Since 2015, technological improvements and subsequent cost reductions facilitated widespread research, often to validate UAS technology to survey single species with application of descriptive statistics over limited spatial and temporal scales. Studies since the 2015 expansion have still generally focused on large birds or mammals in open landscapes of 4 countries, but regulations, such as maximum altitude and line-of-sight limitations, remain barriers to improved animal surveys with UAS. Critical knowledge gaps include the lack of (1) best practices for using UAS to conduct standardized surveys in general, (2) best practices to survey whole wildlife communities in delineated areas, and (3) data on factors affecting bias in counting animals from UAS images. Promising advances include the use of thermal sensors in forested environments or nocturnal surveys and the development of automated or semi-automated machine-learning algorithms to accurately detect, identify, and count animals from UAS images. {\copyright} 2023, The Author(s).},
 author = {Elmore, J. A. and Schultz, E. A. and Jones, L. R. and Evans, K. O. and Samiappan, S. and Pfeiffer, M. B. and Blackwell, B. F. and Iglay, R. B.},
 year = {2023},
 title = {Evidence on the efficacy of small unoccupied aircraft systems (UAS) as a survey tool for North American terrestrial, vertebrate animals: a systematic map},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148444744&doi=10.1186%2fs13750-022-00294-8&partnerID=40&md5=f5932ec42aa84b062547b9870a6aaa46},
 keywords = {Literature Review},
 volume = {12},
 number = {1},
 journal = {Environmental Evidence},
 doi = {10.1186/s13750-022-00294-8},
 file = {Elmore, Schultz et al. 2023 - Evidence on the efficacy:Attachments/Elmore, Schultz et al. 2023 - Evidence on the efficacy.pdf:application/pdf}
}


@article{ElSappagh.2023,
 abstract = {Medical applications of Artificial Intelligence (AI) have consistently shown remarkable performance in providing medical professionals and patients with support for complex tasks. Nevertheless, the use of these applications in sensitive clinical domains where high-stakes decisions are involved could be much more extensive if patients, medical professionals, and regulators were provided with mechanisms for trusting the results provided by AI systems. A key issue for achieving this is endowing AI systems with key dimensions of Trustworthy AI (TAI), such as fairness, transparency, robustness, or accountability, which are not usually considered within this context in a generalized and systematic manner. This paper reviews the recent advances in the TAI domain, including TAI standards and guidelines. We propose several requirements to be addressed in the design, development, and deployment of TAI systems and present a novel machine learning pipeline that contains TAI requirements as embedded components. Moreover, as an example of how current AI systems in medicine consider the TAI perspective, the study extensively reviews the recent literature (2017-2021) on AI systems in a prevalent and high social-impact disease: diagnosis and progression detection of Alzheimer's Disease (AD). The most relevant AI systems in the AD domain are compared and discussed (such as machine learning, deep learning, ensembles, time series, and multimodal multitask) from the perspective of how they address TAI in their design. Several open challenges are highlighted, which could be claimed as one of the main reasons to justify the rare application of AI systems in real clinical environments. The study provides a roadmap to measure the TAI status of an AI systems and highlights its limitations. In addition, it provides the main guidelines to overcome these limitations and build medically trusted AI-based applications in the medical domain.},
 author = {El-Sappagh, S. and Alonso-Moral, J. M. and Abuhmed, T. and Ali, F. and Bugar{\'i}n-Diz, A.},
 year = {2023},
 title = {Trustworthy artificial intelligence in Alzheimer's disease: state of the art, opportunities, and challenges},
 keywords = {Literature Review},
 pages = {11149--11296},
 volume = {56},
 number = {10},
 issn = {1573-7462},
 journal = {ARTIFICIAL INTELLIGENCE REVIEW},
 doi = {10.1007/s10462-023-10415-5},
 file = {El-Sappagh, Alonso-Moral et al. 2023 - Trustworthy artificial intelligence in Alzheimer's:Attachments/El-Sappagh, Alonso-Moral et al. 2023 - Trustworthy artificial intelligence in Alzheimer's.pdf:application/pdf}
}


@inproceedings{Elshawi.2022,
 abstract = {Machine learning algorithms have been widely employed in various applications and fields. Novel technologies in automated machine learning (AutoML) ease algorithm selection and hyperparameter optimization complexity. AutoML frame-works have achieved notable success in hyperparameter tuning and surpassed the performance of human experts. However, depending on such frameworks as black-box can leave machine learning practitioners without insights into the inner working of the AutoML process and hence influence their trust in the models produced. In addition, excluding humans from the loop creates several limitations. For example, most of the current AutoML frameworks ignore the user preferences on defining or controlling the search space, which consequently can impact the performance of the models produced and the acceptance of these models by the end-users. The research in the area of transparency and controllability of AutoML has attracted much interest lately, both in academia and industry. However, existing tools are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning, particularly clustering, remains a largely unexplored problem. Motivated by these shortcomings, we design and implement cSmartML-GlassBox, an interactive visualization tool that en-ables users to refine the search space of AutoML and analyze the results. cSmartML-GlassBox is equipped with a recommendation engine to recommend a time budget that is likely adequate for a new dataset to obtain well-performing pipeline. In addition, the tool supports multi-granularity visualization to enable machine learning practitioners to monitor the AutoML process, analyze the explored configurations and refine/control the search space. Furthermore, cSmartML-GlassBox is equipped with a logging mechanism such that repeated runs on the same dataset can be more effective by avoiding evaluating the same previously considered configurations. We demonstrate the effectiveness and usability of the cSmartML-GlassBox through a user evaluation study with 23 participants and an expert-based usability study based on four experts. We find that the proposed tool increases users' understanding and trust in the AutoML frameworks.  {\copyright} 2022 IEEE.},
 author = {Elshawi, R. and Sakr, S.},
 title = {cSmartML-Glassbox: Increasing Transparency and Controllability in Automated Clustering},
 keywords = {Artefact Design;Case Study},
 pages = {47--54},
 year = {2022},
 doi = {10.1109/ICDMW58026.2022.00015},
 file = {Elshawi, Sakr 2022 - cSmartML-Glassbox:Attachments/Elshawi, Sakr 2022 - cSmartML-Glassbox.pdf:application/pdf}
}


@article{Emmaneel.2019,
 abstract = {Common variable immunodeficiency (CVID) is one of the most frequently diagnosed primary antibody deficiencies (PADs), a group of disorders characterized by a decrease in one or more immunoglobulin (sub)classes and/or impaired antibody responses caused by inborn defects in B cells in the absence of other major immune defects. CVID patients suffer from recurrent infections and disease-related, non-infectious, complications such as autoimmune manifestations, lymphoproliferation, and malignancies. A timely diagnosis is essential for optimal follow-up and treatment. However, CVID is by definition a diagnosis of exclusion, thereby covering a heterogeneous patient population and making it difficult to establish a definite diagnosis. To aid the diagnosis of CVID patients, and distinguish them from other PADs, we developed an automated machine learning pipeline which performs automated diagnosis based on flow cytometric immunophenotyping. Using this pipeline, we analyzed the immunophenotypic profile in a pediatric and adult cohort of 28 patients with CVID, 23 patients with idiopathic primary hypogammaglobulinemia, 21 patients with IgG subclass deficiency, six patients with isolated IgA deficiency, one patient with isolated IgM deficiency, and 100 unrelated healthy controls. Flow cytometry analysis is traditionally done by manual identification of the cell populations of interest. Yet, this approach has severe limitations including subjectivity of the manual gating and bias toward known populations. To overcome these limitations, we here propose an automated computational flow cytometry pipeline that successfully distinguishes CVID phenotypes from other PADs and healthy controls. Compared to the traditional, manual analysis, our pipeline is fully automated, performing automated quality control and data pre-processing, automated population identification (gating) and deriving features from these populations to build a machine learning classifier to distinguish CVID from other PADs and healthy controls. This results in a more reproducible flow cytometry analysis, and improves the diagnosis compared to manual analysis: our pipelines achieve on average a balanced accuracy score of 0.93 ($\pm$0.07), whereas using the manually extracted populations, an averaged balanced accuracy score of 0.72 ($\pm$0.23) is achieved. {\copyright} 2019 Emmaneel, Bogaert, Van Gassen, Tavernier, Dullaers, Haerynck and Saeys.},
 author = {Emmaneel, A. and Bogaert, D. J. and {van Gassen}, S. and Tavernier, S. J. and Dullaers, M. and Haerynck, F. and Saeys, Y.},
 year = {2019},
 title = {A computational pipeline for the diagnosis of CVID patients},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072011894&doi=10.3389%2ffimmu.2019.02009&partnerID=40&md5=94aefc6dfab22c7e673f3add250023ba},
 keywords = {ML implementation},
 volume = {10},
 number = {AUG},
 journal = {Frontiers in Immunology},
 doi = {10.3389/fimmu.2019.02009},
 file = {Emmaneel, Bogaert et al. 2019 - A computational pipeline:Attachments/Emmaneel, Bogaert et al. 2019 - A computational pipeline.pdf:application/pdf}
}


@inproceedings{Esteves.2016,
 abstract = {Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: {\textquotedbl}What is the cleanest and lowest-impact solution, i.e., the minimal effort to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?{\textquotedbl}. We introduce a novel and low-impact methodology specifically designed for code built in that context, combining Semantic Web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: MEX Vocabulary. {\copyright} 2016 ACM.},
 author = {Esteves, D. and Mendes, P. N. and Moussallem, D. and Duarte, J. C. and Zaveri, A. and Lehmann, J. and Neto, C. B. and Costa, I. and Cavalcanti, M. C.},
 title = {MEX interfaces: Automating machine learning metadata generation},
 keywords = {Artefact Design},
 pages = {17--24},
 year = {2016},
 doi = {10.1145/2993318.2993320},
 file = {Esteves, Mendes et al 2016 - MEX interfaces:Attachments/Esteves, Mendes et al 2016 - MEX interfaces.pdf:application/pdf}
}


@article{Etheridge.2020,
 abstract = {Background and objective To develop a semi-automated, machine-learning based workflow to evaluate the ellipsoid zone (EZ) assessed by spectral domain optical coherence tomography (SD-OCT) in eyes with macular edema secondary to central retinal or hemi-retinal vein occlusion in SCORE2 treated with anti-vascular endothelial growth factor agents. Methods SD-OCT macular volume scans of a randomly selected subset of 75 SCORE2 study eyes were converted to the Digital Imaging and Communications in Medicine (DICOM) format, and the EZ layer was segmented using nonproprietary software. Segmented layer coordinates were exported and used to generate en face EZ thickness maps. Within the central subfield, the area of EZ defect was measured using manual and semi-automated approaches via a customized workflow in the open-source data analytics platform, Konstanz Information Miner (KNIME). Results A total of 184 volume scans from 74 study eyes were analyzed. The mean$\pm$SD area of EZ defect was similar between manual (0.19$\pm$0.22 mm2) and semi-automated measurements (0.19$\pm$0.21 mm2, p = 0.93; intra-class correlation coefficient = 0.90; average bias = 0.01, 95{\%} confidence interval of limits of agreement -0.18--0.20). Conclusions A customized workflow generated via an open-source data analytics platform that applied machine-learning methods demonstrated reliable measurements of EZ area defect from en face thickness maps. The result of our semi-automated approach were comparable to manual measurements. {\copyright} 2020 Etheridge et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
 author = {Etheridge, T. and Dobson, E.T.A. and Wiedenmann, M. and Papudesu, C. and Scott, I. U. and Ip, M. S. and Eliceiri, K. W. and Blodi, B. A. and Domalpally, A.},
 year = {2020},
 title = {A semi-automated machine-learning based workflow for ellipsoid zone analysis in eyes with macular edema: SCORE2 pilot study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084277091&doi=10.1371%2fjournal.pone.0232494&partnerID=40&md5=9a570f339a7d9873a5900fb63efc6ec1},
 keywords = {ML implementation},
 volume = {15},
 number = {4},
 journal = {PLoS ONE},
 doi = {10.1371/journal.pone.0232494},
 file = {Etheridge, Dobson et al. 2020 - A semi-automated machine-learning based workflow:Attachments/Etheridge, Dobson et al. 2020 - A semi-automated machine-learning based workflow.pdf:application/pdf}
}


@article{Fagman.2023,
 abstract = {Background: Plaque analysis with coronary computed tomography angiography (CCTA) is a promising tool to identify high risk of future coronary events. The analysis process is time-consuming, and requires highly trained readers. Deep learning models have proved to excel at similar tasks, however, training these models requires large sets of expert-annotated training data. The aims of this study were to generate a large, high-quality annotated CCTA dataset derived from Swedish CArdioPulmonary BioImage Study (SCAPIS), report the reproducibility of the annotation core lab and describe the plaque characteristics and their association with established risk factors.Methods and results: The coronary artery tree was manually segmented using semi-automatic software by four primary and one senior secondary reader. A randomly selected sample of 469 subjects, all with coronary plaques and stratified for cardiovascular risk using the Systematic Coronary Risk Evaluation (SCORE), were analyzed. The reproducibility study (n = 78) showed an agreement for plaque detection of 0.91 (0.84-0.97). The mean percentage difference for plaque volumes was-0.6{\%} the mean absolute percentage difference 19.4{\%} (CV 13.7{\%}, ICC 0.94). There was a positive correlation between SCORE and total plaque volume (rho = 0.30, p {\textless} 0.001) and total low attenuation plaque volume (rho = 0.29, p {\textless} 0.001).Conclusions: We have generated a CCTA dataset with high-quality plaque annotations showing good reproducibility and an expected correlation between plaque features and cardiovascular risk. The stratified data sampling has enriched high-risk plaques making the data well suited as training, validation and test data for a fully automatic analysis tool based on deep learning.},
 author = {Fagman, E. and Alv{\'e}n, J. and Westerbergh, J. and Kitslaar, P. and Kercsik, M. and Cederlund, K. and Duvernoy, O. and Engvall, J. and Gon{\c{c}}alves, I. and Markstad, H. and Ostenfeld, E. and Bergstr{\"o}m, G. and Hjelmgren, O.},
 year = {2023},
 title = {High-quality annotations for deep learning enabled plaque analysis in SCAPIS cardiac computed tomography angiography},
 keywords = {ML implementation},
 volume = {9},
 number = {5},
 issn = {2405-8440},
 journal = {HELIYON},
 doi = {10.1016/j.heliyon.2023.e16058},
 file = {Fagman, Alv{\'e}n et al. 2023 - High-quality annotations for deep learning:Attachments/Fagman, Alv{\'e}n et al. 2023 - High-quality annotations for deep learning.pdf:application/pdf}
}


@article{Farina.2022,
 abstract = {Background: Obesity is chronic health problem. Screening for the obesity phenotype is limited by the availability of practical methods. Methods: We determined the reproducibility and accuracy of an automated machine-learning method using smartphone camera-enabled capture and analysis of single, two-dimensional (2D) standing lateral digital images to estimate fat mass (FM) compared to dual X-ray absorptiometry (DXA) in females and males. We also report the first model to predict abdominal FM using 2D digital images. Results: Gender-specific 2D estimates of FM were significantly correlated (p {\textless} 0.001) with DXA FM values and not different (p {\textgreater} 0.05). Reproducibility of FM estimates was very high (R2 = 0.99) with high concordance (R2 = 0.99) and low absolute pure error (0.114 to 0.116 kg) and percent error (1.3 and 3{\%}). Bland--Altman plots revealed no proportional bias with limits of agreement of 4.9 to $-$4.3 kg and 3.9 to $-$4.9 kg for females and males, respectively. A novel 2D model to estimate abdominal (lumbar 2--5) FM produced high correlations (R2 = 0.99) and concordance (R2 = 0.99) compared to DXA abdominal FM values. Conclusions: A smartphone camera trained with machine learning and automated processing of 2D lateral standing digital images is an objective and valid method to estimate FM and, with proof of concept, to determine abdominal FM. It can facilitate practical identification of the obesity phenotype in adults. {\copyright} 2022 by the authors.},
 author = {Farina, G. L. and Orlandi, C. and Lukaski, H. and Nescolarde, L.},
 year = {2022},
 title = {Digital Single-Image Smartphone Assessment of Total Body Fat and Abdominal Fat Using Machine Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141640084&doi=10.3390%2fs22218365&partnerID=40&md5=3b567237dd6add2a72d6d2677c800d86},
 keywords = {ML implementation},
 volume = {22},
 number = {21},
 journal = {Sensors},
 doi = {10.3390/s22218365},
 file = {Farina, Orlandi et al. 2022 - Digital Single-Image Smartphone Assessment:Attachments/Farina, Orlandi et al. 2022 - Digital Single-Image Smartphone Assessment.pdf:application/pdf}
}


@article{Fernandes.2024,
 abstract = {Simple Summary Artificial intelligence tools, such as convolutional neural networks, are used in tracking and counting animals, species identification, and measurement of morphometric data in order to optimize operations and minimize the animals' stress and physical injuries when they are handled, which lead to an increase in disease and mortality in livestock. This study aims to evaluate and understand the effectiveness of counting different quantities of Serrasalmidae fingerlings by means of images using neural networks. These are promising tools to strengthen round fish farming, an important species for South American aquaculture, in order to increase production efficiency, profitability, and the transparency in the commercialization of fingerlings.Abstract Aquaculture produces more than 122 million tons of fish globally. Among the several economically important species are the Serrasalmidae, which are valued for their nutritional and sensory characteristics. To meet the growing demand, there is a need for automation and accuracy of processes, at a lower cost. Convolutional neural networks (CNNs) are a viable alternative for automation, reducing human intervention, work time, errors, and production costs. Therefore, the objective of this work is to evaluate the efficacy of convolutional neural networks (CNNs) in counting round fish fingerlings (Serrasalmidae) at different densities using 390 color photographs in an illuminated environment. The photographs were submitted to two convolutional neural networks for object detection: one model was adapted from a pre-trained CNN and the other was an online platform based on AutoML. The metrics used for performance evaluation were precision (P), recall (R), accuracy (A), and F1-Score. In conclusion, convolutional neural networks (CNNs) are effective tools for detecting and counting fish. The pre-trained CNN demonstrated outstanding performance in identifying fish fingerlings, achieving accuracy, precision, and recall rates of 99{\%} or higher, regardless of fish density. On the other hand, the AutoML exhibited reduced accuracy and recall rates as the number of fish increased.},
 author = {Fernandes, M. P. and Costa, A. C. and Fran{\c{c}}a, H. F.D. and Souza, A. S. and Viadanna, P. H.D. and Lima, L. D. and Horn, L. D. and Pierozan, M. B. and de Rezende, I. R. and de Medeiros, R. M.D. and Braganholo, B. M. and {Da Silva}, L. O.P. and Nacife, J. M. and Costa, K. A.D. and {Da Silva}, M. A.P. and de Oliveira, R. F.},
 year = {2024},
 title = {Convolutional Neural Networks in the Inspection of Serrasalmids (Characiformes) Fingerlings},
 keywords = {ML implementation},
 volume = {14},
 number = {4},
 issn = {2076-2615},
 journal = {ANIMALS},
 doi = {10.3390/ani14040606},
 file = {Fernandes, Costa et al. 2024 - Convolutional Neural Networks:Attachments/Fernandes, Costa et al. 2024 - Convolutional Neural Networks.pdf:application/pdf}
}


@article{Fernandez.2022,
 abstract = {Background: Breast cancer (BC) grading plays a critical role in patient management despite the considerable inter- and intra-observer variability, highlighting the need for decision support tools to improve reproducibility and prognostic accuracy for use in clinical practice. The objective was to evaluate the ability of a digital artificial intelligence (AI) assay (PDxBr) to enrich BC grading and improve risk categorization for predicting recurrence. Methods: In our population-based longitudinal clinical development and validation study, we enrolled 2075 patients from Mount Sinai Hospital with infiltrating ductal carcinoma of the breast. With 3:1 balanced training and validation cohorts, patients were retrospectively followed for a median of 6~years. The main outcome was to validate an automated BC phenotyping system combined with clinical features to produce a binomial risk score predicting BC recurrence at diagnosis. Results: The PDxBr training model (n = 1559 patients) had a C-index of 0.78 (95{\%} CI, 0.76--0.81) versus clinical 0.71 (95{\%} CI, 0.67--0.74) and image feature models 0.72 (95{\%} CI, 0.70--0.74). A risk score of 58 (scale 0--100) stratified patients as low or high risk, hazard ratio (HR) 5.5 (95{\%} CI 4.19--7.2, p {\textless} 0.001), with a sensitivity 0.71, specificity 0.77, NPV 0.95, and PPV 0.32 for predicting BC recurrence within 6~years. In the validation cohort (n = 516), the C-index was 0.75 (95{\%} CI, 0.72--0.79) versus clinical 0.71 (95{\%} CI 0.66--0.75) versus image feature models 0.67 (95{\%} CI, 0.63--071). The validation cohort had an HR of 4.4 (95{\%} CI 2.7--7.1, p {\textless} 0.001), sensitivity of 0.60, specificity 0.77, NPV 0.94, and PPV 0.24 for predicting BC recurrence within 6~years. PDxBr also improved Oncotype Recurrence Score (RS) performance: RS 31 cutoff, C-index of 0.36 (95{\%} CI 0.26--0.45), sensitivity 37{\%}, specificity 48{\%}, HR 0.48, p = 0.04 versus Oncotype RS plus AI-grade C-index 0.72 (95{\%} CI 0.67--0.79), sensitivity 78{\%}, specificity 49{\%}, HR 4.6, p {\textless} 0.001 versus Oncotype RS plus PDxBr, C-index 0.76 (95{\%} CI 0.70--0.82), sensitivity 67{\%}, specificity 80{\%}, HR 6.1, p {\textless} 0.001. Conclusions: PDxBr is a digital BC test combining automated AI-BC prognostic grade with clinical--pathologic features to predict the risk of early-stage BC recurrence. With future validation studies, we anticipate the PDxBr model will enrich current gene expression assays and enhance treatment decision-making. {\copyright} 2022, The Author(s).},
 author = {Fernandez, G. and Prastawa, M. and Madduri, A. S. and Scott, R. and Marami, B. and Shpalensky, N. and Cascetta, K. and Sawyer, M. and Chan, M. and Koll, G. and Shtabsky, A. and Feliz, A. and Hansen, T. and Veremis, B. and Cordon-Cardo, C. and Zeineh, J. and Donovan, M. J.},
 year = {2022},
 title = {Development and validation of an AI-enabled digital breast cancer assay to predict early-stage breast cancer recurrence within 6 years},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144261283&doi=10.1186%2fs13058-022-01592-2&partnerID=40&md5=3c5d4c3ebf2e82ab9412cc98136799ad},
 keywords = {ML implementation},
 volume = {24},
 number = {1},
 journal = {Breast Cancer Research},
 doi = {10.1186/s13058-022-01592-2},
 file = {Fernandez, Prastawa et al. 2022 - Development and validation:Attachments/Fernandez, Prastawa et al. 2022 - Development and validation.pdf:application/pdf}
}


@proceedings{Fezzazi.2021,
 abstract = {Blockchain and machine learning (ML) have emerged as two disruptive technologies that could transform various sectors. Blockchain is known as peer to peer decentralized, distributed ledger technology that enables to store and exchange anything of value, it is deterministic, permanent and immutable. On the other hand, ML is the ability of computers to learn without being programmed, and it is probabilistic. Hence, when blockchain and ML converge, they surely would benefit from each other. Blockchain can enhance security of ML platforms, and ML can provide automation and optimization to the blockchain solutions. In this work, we advocate the importance of enhancing blockchain with ML algorithms, as a proof of principle we address the issue of secure and intelligent e-voting.The use of blockchain technology has brought tremendous different application domains, e-voting is one of them. Most existing e-Voting systems require central authority during the process of authentication and verification of the voter. In this paper we propose a safe online voting approach based on blockchain and ML to provide a solution to this issue. We use blockchain to ensure integrity and transparency of the votes, and ML for automating the verification process of eligible voters based on AI-powered oracle platform for face authentication. The proposed solution offers automation, security, and mobility to the voting system.  {\copyright} 2021 IEEE.},
 year = {2021},
 title = {Towards a Blockchain based Intelligent and Secure Voting},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123501889&doi=10.1109%2fICDS53782.2021.9626751&partnerID=40&md5=ba916243f195b60b4379bf3209609ac0},
 keywords = {ML implementation},
 editor = {Fezzazi, A. E. and Adadi, A. and Berrada, M.},
 doi = {10.1109/ICDS53782.2021.9626751}
}


@article{Filitto.2022,
 abstract = {Background: Rectal cancer is a malignant neoplasm of the large intestine resulting from the uncontrolled proliferation of the rectal tract. Predicting the pathologic response of neoadjuvant chemoradiotherapy at an MRI primary staging scan in patients affected by locally advanced rectal cancer (LARC) could lead to significant improvement in the survival and quality of life of the patients. In this study, the possibility of automatizing this estimation from a primary staging MRI scan, using a fully automated artificial intelligence-based model for the segmentation and consequent characterization of the tumor areas using radiomic features was evaluated. The TRG score was used to evaluate the clinical outcome. Methods: Forty-three patients under treatment in the IRCCS Sant'Orsola-Malpighi Polyclinic were retrospectively selected for the study; a U-Net model was trained for the automated segmentation of the tumor areas; the radiomic features were collected and used to predict the tumor regression grade (TRG) score. Results: The segmentation of tumor areas outperformed the state-of-the-art results in terms of the Dice score coefficient or was comparable to them but with the advantage of considering mucinous cases. Analysis of the radiomic features extracted from the lesion areas allowed us to predict the TRG score, with the results agreeing with the state-of-the-art results. Conclusions: The results obtained regarding TRG prediction using the proposed fully automated pipeline prove its possible usage as a viable decision support system for radiologists in clinical practice. {\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
 author = {Filitto, G. and Coppola, F. and Curti, N. and Giampieri, E. and Dall'olio, D. and Merlotti, A. and Cattabriga, A. and Cocozza, M. A. and {Taninokuchi Tomassoni}, M. and Remondini, D. and Pierotti, L. and Strigari, L. and Cuicchi, D. and Guido, A. and Rihawi, K. and D'errico, A. and {Di Fabio}, F. and Poggioli, G. and Morganti, A. G. and Ricciardiello, L. and Golfieri, R. and Castellani, G.},
 year = {2022},
 title = {Automated Prediction of the Response to Neoadjuvant Chemoradiotherapy in Patients Affected by Rectal Cancer},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129133809&doi=10.3390%2fcancers14092231&partnerID=40&md5=848b748a9b78790f8262d85e73c26c21},
 keywords = {ML implementation},
 volume = {14},
 number = {9},
 journal = {Cancers},
 doi = {10.3390/cancers14092231},
 file = {Filitto, Coppola et al. 2022 - Automated Prediction of the Response:Attachments/Filitto, Coppola et al. 2022 - Automated Prediction of the Response.pdf:application/pdf}
}


@article{Fu.2022,
 abstract = {Study Design. An algorithm was developed with MATLAB platform to automatically quantify the volume of cervical disc herniation (CDH) based on the sagittal magnetic resonance images. This automated program was used for CDH data set, and then compared with manual measurement results confirming its reliability. Objective. The aim was to develop a new software for automated CDH volume measurement. Summary of Background Data. CDH compresses the spinal cord, regarding as the leading cause of cervical myelopathy. However, the CDH volume, of great value to clinical symptoms, can be only manually measured with no-excellent but acceptable interobserver reliability. This was due to the manual error of outlining CDH area and inclusion of structure posterior vertebra. No studies has proposed such an automated algorithm of CDH volume quantification which is standardised to quantify the accurate volume of CDH thus helping doctors easily evaluate CDH progressing. Methods. The algorithm of CDH volume measurement was proposed. This program was then tested for 490 CDHs data set, from 185 patients with two repeated magnetic resonance imaging detections. Three individual observers manually measured the volumes of these CDHs, to justify the accuracy of this software. CDH volume was either in the classic way or the revised way excluding the influence of structure posterior vertebra. Results. The automated software was successfully developed on MATLAB platform, with no difference found with manual measurements (average level) in CDH volume measurement. The change ratios in CDH volumes were profoundly consistent with manual observation, showing the error of 5.8{\%} in median. The revised method elevated the absolute value of ratio by amplifying the percentage change. Conclusion. Our developed automated volumetry system was an standardized and accurate way, with selective removal module of structure posterior vertebra, replaceable for manual volume measurement of CDH, which was useful for spinal surgeons diagnosing and treating CDH disease. {\copyright} 2022 Lippincott Williams and Wilkins. All rights reserved.},
 author = {Fu, S. and Zhang, C. and Yan, X. and Li, D. and Wang, Y. and Dong, C. and Cao, Z. and Ning, Y. and Shao, C. and Yang, T.},
 year = {2022},
 title = {A New Automated AI-Assisted System to Assess Cervical Disc Herniation},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135599720&doi=10.1097%2fBRS.0000000000004376&partnerID=40&md5=c58293e43342ed9b051baeed31aa3f81},
 keywords = {ML implementation},
 pages = {E536-E544},
 volume = {47},
 number = {16},
 journal = {Spine},
 doi = {10.1097/BRS.0000000000004376}
}


@article{Garg.2019,
 abstract = {Objective: The manual adjudication of disease classification is time-consuming, error-prone, and limits scaling to large datasets. In ischemic stroke (IS), subtype classification is critical for management and outcome prediction. This study sought to use natural language processing of electronic health records (EHR) combined with machine learning methods to automate IS subtyping. Methods: Among IS patients from an observational registry with TOAST subtyping adjudicated by board-certified vascular neurologists, we analyzed unstructured text-based EHR data including neurology progress notes and neuroradiology reports using natural language processing. We performed several feature selection methods to reduce the high dimensionality of the features and 5-fold cross validation to test generalizability of our methods and minimize overfitting. We used several machine learning methods and calculated the kappa values for agreement between each machine learning approach to manual adjudication. We then performed a blinded testing of the best algorithm against a held-out subset of 50 cases. Results: Compared to manual classification, the best machine-based classification achieved a kappa of .25 using radiology reports alone, .57 using progress notes alone, and .57 using combined data. Kappa values varied by subtype being highest for cardioembolic (.64) and lowest for cryptogenic cases (.47). In the held-out test subset, machine-based classification agreed with rater classification in 40 of 50 cases (kappa .72). Conclusions: Automated machine learning approaches using textual data from the EHR shows agreement with manual TOAST classification. The automated pipeline, if externally validated, could enable large-scale stroke epidemiology research. {\copyright} 2019},
 author = {Garg, R. and Oh, E. and Naidech, A. and Kording, K. and Prabhakaran, S.},
 year = {2019},
 title = {Automating Ischemic Stroke Subtype Classification Using Machine Learning and Natural Language Processing},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065591295&doi=10.1016%2fj.jstrokecerebrovasdis.2019.02.004&partnerID=40&md5=3f08f7065d3dfc2cd0a997bd53b5fc2b},
 keywords = {ML implementation},
 pages = {2045--2051},
 volume = {28},
 number = {7},
 journal = {Journal of Stroke and Cerebrovascular Diseases},
 doi = {10.1016/j.jstrokecerebrovasdis.2019.02.004}
}


@article{Garouani.2022,
 abstract = {The Machine Learning(ML) based solutions in manufacturing industrial contexts often require skilled resources. More practical non-expert software solutions are then desired to enhance the usability of ML algorithms. The algorithm selection and configuration is one of the most difficult tasks for users like manufacturing specialists. The identification of the most appropriate algorithm in an automatic manner is among the major research challenges to achieve optimal performance of ML tools. In this paper, we present an auto-explained Automated Machine Learning tool for Big Industrial Data(AMLBID) to better cope with the prominent challenges posed by the evolution of Big Industrial Data. It is a meta-learning based decision support system for the automated selection and tuning of implied hyperparameters for ML algorithms. Moreover, the framework is equipped with an explainer module that makes the outcomes transparent and interpretable for well-performing ML systems. {\copyright} 2021 The Authors},
 author = {Garouani, M. and Ahmad, A. and Bouneffa, M. and Hamlich, M.},
 year = {2022},
 title = {AMLBID: An auto-explained Automated Machine Learning tool for Big Industrial Data},
 keywords = {Artefact Design},
 volume = {17},
 journal = {SoftwareX},
 doi = {10.1016/j.softx.2021.100919},
 file = {Garouani, Ahmad et al 2022 - AMLBID An auto-explained Automated Machine:Attachments/Garouani, Ahmad et al 2022 - AMLBID An auto-explained Automated Machine.pdf:application/pdf}
}


@inproceedings{Garouani.2023,
 abstract = {Automated machine learning (AutoML) has transformed the process of selecting optimal machine learning (ML) models by autonomously searching for the most appropriate ones and fine-tuning associated hyperparameters. This eliminates the burdensome task of trial-and-error selection and parametrization of ML algorithms. Nonetheless, the lack of transparency and explainability poses a significant challenge when using AutoML, as it hampers user trust in the system's recommendations. Consequently, users often allocate more resources to the search process, resulting in reduced efficiency of the AutoML systems. To address this challenge, we propose an interactive and explainable AutoML framework that enables users to understand the reasoning behind the recommendations and diagnose any limitations of the suggested models using various explainable AI methods. Additionally, our framework provides the possibility of automated performance refinement. To operationalize the framework, we introduce AMLExplainer, an XAI system for interactive and interpretable AutoML that visualizes and performs all stages of the proposed pipeline(s) within the widely used Bootstrap Dash environment. {\copyright} The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
 author = {Garouani, M. and Bouneffa, M.},
 title = {Unlocking the Black Box: Towards Interactive Explainable Automated Machine Learning},
 keywords = {Artefact Design},
 pages = {458--469},
 year = {2023},
 doi = {10.1007/978-3-031-48232-8{\textunderscore }42},
 file = {Garouani, Bouneffa 2023 - Unlocking the Black Box:Attachments/Garouani, Bouneffa 2023 - Unlocking the Black Box.pdf:application/pdf}
}


@article{Garouani.2023b,
 abstract = {We report a new release of the self-explainable AutoML software AMLBID. The software package is a meta-learning based AutoML decision support system that assists machine learning(ML) experts and neophytes in building well performing ML pipelines and makes the outcomes transparent and interpretable. The release 2.0 of the software introduces an enhanced efficiency of algorithms recommendation. The performance improvement is mainly achieved by the integration of the AeKNN meta-model. This implementation makes datasets meta-features more informative and further improve the accuracy. The release 2.0 also introduces the standalone AMLBIDesc, a desktop version of the AMLBID software which makes the tool more accessible to non-expert users. {\&} COPY; 2023 The Author(s). Published by Elsevier B.V. All rights reserved.},
 author = {Garouani, M. and Bouneffa, M. and Ahmad, A. and Hamlich, M.},
 year = {2023},
 title = {Version [2.0]- [AMLBID: An auto-explained Automated Machine Learning tool for Big Industrial Data]},
 keywords = {Artefact Design},
 volume = {23},
 journal = {SoftwareX},
 file = {Garouani, Bouneffa et al 2023 - Version [20]- [AMLBID:Attachments/Garouani, Bouneffa et al 2023 - Version [20]- [AMLBID.pdf:application/pdf}
}


@article{Gerges.2020,
 abstract = {Abstract: Greulich and Pyle (GP) is one of the most common methods to determine bone age from hand radiographs. In recent years, new methods were developed to increase the efficiency in bone age analysis like the shorthand bone age (SBA) and automated artificial intelligence algorithms. Objective: The aim of this study is to evaluate the accuracy and reliability of these two methods and examine if the reduction in analysis time compromises their efficacy. Methods: Two hundred thirteen males and 213 females had their bone age determined by two separate raters using the SBA and GP methods. Three weeks later, the two raters repeated the analysis of the radiographs. The raters timed themselves using an online stopwatch. De-identified radiographs were securely uploaded to an automated algorithm developed by a group of radiologists in Toronto. The gold standard was determined to be the radiology report attached to each radiograph, written by experienced radiologists using GP. Results: Intraclass correlation between each method and the gold standard fell within the range of 0.8--0.9, highlighting significant agreement. Most of the comparisons showed a statistically significant difference between the new methods and the gold standard; however, it may not be clinically significant as it ranges between 0.25 and 0.5~years. A bone age is considered clinically abnormal if it falls outside 2 standard deviations of the chronological age; standard deviations are calculated and provided in GP atlas. Conclusion: The shorthand bone age method and the automated algorithm produced values that are in agreement with the gold standard while reducing analysis time. {\copyright} 2020, ISS.},
 author = {Gerges, M. and Eng, H. and Chhina, H. and Cooper, A.},
 year = {2020},
 title = {Modernization of bone age assessment: comparing the accuracy and reliability of an artificial intelligence algorithm and shorthand bone age to Greulich and Pyle},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084146437&doi=10.1007%2fs00256-020-03429-5&partnerID=40&md5=08c06d60bc3ee2fb3920e0013064def6},
 keywords = {Empirical Study;ML implementation},
 pages = {1449--1457},
 volume = {49},
 number = {9},
 journal = {Skeletal Radiology},
 doi = {10.1007/s00256-020-03429-5},
 file = {Gerges, Eng et al. 2020 - Modernization of bone age assessment:Attachments/Gerges, Eng et al. 2020 - Modernization of bone age assessment.pdf:application/pdf}
}


@proceedings{Ghamizi.2019,
 abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research. {\copyright} 2019 Association for Computing Machinery.},
 year = {2019},
 title = {Automated search for configurations of convolutional neural network architectures},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123043273&doi=10.1145%2f3336294.3336306&partnerID=40&md5=ba2b0928ec714886277f49dc1c71c28b},
 keywords = {Artefact Design},
 volume = {A},
 editor = {Ghamizi, S. and Cordy, M. and Papadakis, M. and {Le Traon}, Y.},
 doi = {10.1145/3336294.3336306},
 file = {Ghamizi, Cordy et al. (Hg.) 2019 - Automated search for configurations:Attachments/Ghamizi, Cordy et al. (Hg.) 2019 - Automated search for configurations.pdf:application/pdf}
}


@article{Gil.2019,
 abstract = {Patterns, micro-patterns, and nano-patterns have many applications: program comprehension, code transformations, documentation aids, improving code robustness, etc. This work revisits the notion of nano-patterns-originally an obiter dictum of the work on micro-patterns. Nano-patterns here are taken as more general than their previous definition in the literature: predicates on short code snippets that represent some common and elementary programming missions such as {\textquotedbl}for each m is an element of M do ...{\textquotedbl}, or, {\textquotedbl}use x (but if x is null, y is a substitute){\textquotedbl}, which represent small and recurring programming idioms.

With this generalization, we offer a taxonomized languageof nanos nano-patterns for Java. We also describe the process of pattern harvesting we used and the underlying rationale, including our proposed prevalence threshold criterion, which, by capitalizing on Hirsch's famous h-index, makes a robust yard-stick of the pattern's significance.

An empirical survey of 78 Open Source Java projects indicates that the nano-patterns of our proposed language described here have a substantial prevalence in the code. About a third of the commands (executable statements) and half of the methods are instances of nano-patterns in the proposed language. Also, the language's prevalence is typically higher than that of languages harvested in a project specific, automated machine learning process.

Nano-patterns are implementation/language level details for most high level software engineering purposes. One contribution made by the present paper is in identifying the clutter made by the snippets, appreciating its presence, and imposing order on it. The language, the nano-patterns in it, and the contributed automatic tool for tracing nano-patterns in code may help to deal systematically with this low level, yet significant, portion of code.},
 author = {Gil, Y. and Marcovitch, O. and Orr{\'u}, M.},
 year = {2019},
 title = {A Nano-Pattern Language for Java},
 keywords = {ML implementation},
 volume = {54},
 issn = {2665-9182},
 journal = {JOURNAL OF COMPUTER LANGUAGES},
 doi = {10.1016/j.cola.2019.100905}
}


@article{Giovanelli.2024,
 abstract = {This work is a companion reproducibility paper of the experiments and results reported in Giovanelli et al. (2022), where data pre-processing pipelines are evaluated in order to find pipeline prototypes that reduce the classification error of supervised learning algorithms. With the recent shift towards data-centric approaches, where instead of the model, the dataset is systematically changed for better model performance, data pre-processing is receiving a lot of attention. Yet, its impact over the final analysis is not widely recognized, primarily due to the lack of publicly available experiments that quantify it. To bridge this gap, this work introduces a set of reproducible experiments on the impact of data pre-processing by providing a detailed reproducibility protocol together with a software tool and a set of extensible datasets, which allow for all the experiments and results of our aforementioned work to be reproduced. We introduce a set of strongly reproducible experiments based on a collection of intermediate results, and a set of weakly reproducible experiments (Lastra-D{\i}az, 0000) that allows reproducing our end-to-end optimization process and evaluation of all the methods reported in our primary paper. The reproducibility protocol is created in Docker and tested in Windows and Linux. In brief, our primary work (i) develops a method for generating effective prototypes, as templates or logical sequences of pre-processing transformations, and (ii) instantiates the prototypes into pipelines, in the form of executable or physical sequences of actual operators that implement the respective transformations. For the first, a set of heuristic rules learned from extensive experiments are used, and for the second techniques from Automated Machine Learning (AutoML) are applied. {\copyright} 2023 Elsevier Ltd},
 author = {Giovanelli, J. and Bilalli, B. and Abell{\'o}, A. and Silva-Coira, F. and de Bernardo, G.},
 year = {2024},
 title = {Reproducible experiments for generating pre-processing pipelines for AutoETL},
 keywords = {Artefact Design;Experiment},
 volume = {120},
 journal = {Information Systems},
 file = {Giovanelli, Bilalli et al 2024 - Reproducible experiments for generating pre-processing:Attachments/Giovanelli, Bilalli et al 2024 - Reproducible experiments for generating pre-processing.pdf:application/pdf}
}


@article{Glatstein.2023,
 abstract = {Human infertility is a major global public health issue estimated to affect one out of six couples, while the number of assisted reproduction cycles grows impressively year over year. Efforts to alleviate infertility using advanced technology are gaining traction rapidly as infertility has an enormous impact on couples and the potential to destabilize entire societies if replacement birthrates are not achieved. Artificial intelligence (AI) technologies, leveraged by the highly advanced assisted reproductive technology (ART) industry, are a promising addition to the armamentarium of tools available to combat global infertility. This review provides a background for current methodologies in embryo selection, which is a manual, time-consuming, and poorly reproducible task. AI has the potential to improve this process (among many others) in both the clinician's office and the IVF laboratory. Embryo selection is evolving through digital methodologies into an automated procedure, with superior reliability and reproducibility, that is likely to result in higher pregnancy rates for patients. There is an emerging body of data demonstrating the utility of AI applications in multiple areas in the IVF laboratory. AI platforms have been developed to evaluate individual embryologist performance; to provide quality assurance for culture systems; to correlate embryologist's assessments and AI systems; to predict embryo ploidy, implantation, fetal heartbeat, and live birth outcome; and to replace the current ``analogue'' system of embryo selection with a digital paradigm. AI capability will distinguish high performing, high profit margin, low-cost, and highly successful IVF clinic business models. We think it will become the standard, ``new normal'' in IVF labs, as rapidly and thoroughly as vitrification, blastocyst culture, and intracytoplasmic sperm injection replaced their predecessor technologies. At the time of this review, the AI technology to automate embryo evaluation and selection has robustly matured, and therefore, it is the main focus of this review. {\copyright} 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
 author = {Glatstein, I. and Chavez-Badiola, A. and Curchoe, C. L.},
 year = {2023},
 title = {New frontiers in embryo selection},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145830718&doi=10.1007%2fs10815-022-02708-5&partnerID=40&md5=d9523b49d0cbb8a225429be9ad3e7cd6},
 keywords = {ML implementation},
 pages = {223--234},
 volume = {40},
 number = {2},
 journal = {Journal of Assisted Reproduction and Genetics},
 doi = {10.1007/s10815-022-02708-5},
 file = {Glatstein, Chavez-Badiola et al. 2023 - New frontiers in embryo selection:Attachments/Glatstein, Chavez-Badiola et al. 2023 - New frontiers in embryo selection.pdf:application/pdf}
}


@article{GonzalezCanche.2023,
 abstract = {Labeling or classifying textual data and qualitative evidence is an expensive and consequential challenge. The rigor and consistency behind the construction of these labels ultimately shape research findings and conclusions. A multifaceted methodological conundrum to address this challenge is the need for human reasoning for classification that leads to deeper and more nuanced understandings; however, this same manual human classification comes with the well-documented increase in classification inconsistencies and errors, particularly when dealing with vast amounts of documents and teams of coders. An alternative to human coding consists of machine learning-assisted techniques. These data science and visualization techniques offer tools for data classification that are cost-effective and consistent but are prone to losing participants' meanings or voices for two main reasons: (a) these classifications typically aggregate all texts configuring each input file (i.e., each interview transcript) into a single topic or code and (b) these words configuring texts are analyzed outside of their original contexts. To address this challenge and analytic conundrum, we present an analytic framework and software tool, that addresses the following question: How to classify vast amounts of qualitative evidence effectively and efficiently without losing context or the original voices of our research participants and while leveraging the nuances that human reasoning bring to the qualitative and mixed methods analytic tables? This framework mirrors the line-by-line coding employed in human/manual code identification but relying on machine learning to classify texts in minutes rather than months. The resulting outputs provide complete transparency of the classification process and aid to recreate the contextualized, original, and unaltered meanings embedded in the input documents, as provided by our participants. We offer access to the database (Gonz{\'a}lez Canch{\'e}, 2022e) and software required (Gonz{\'a}lez Canch{\'e}, 2022a, Mac https://cutt.ly/jc7n3OT, and Windows https://cutt.ly/wc7nNKF) to replicate the analyses. We hope this opportunity to become familiar with the analytic framework and software, may result in expanded access of data science tools to analyze qualitative evidence (see also Gonz{\'a}lez Canch{\'e} 2022b, 2022c, 2022d, for related no-code data science applications to classify and analyze qualitative and textual data dynamically). {\copyright} The Author(s) 2023.},
 author = {{Gonz{\'a}lez Canch{\'e}}, M. S.},
 year = {2023},
 title = {Latent Code Identification (LACOID): A Machine Learning-Based Integrative Framework [and Open-Source Software] to Classify Big Textual Data, Rebuild Contextualized/Unaltered Meanings, and Avoid Aggregation Bias},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146059526&doi=10.1177%2f16094069221144940&partnerID=40&md5=091702cc761a6ec11d67254ee3a87a37},
 keywords = {Artefact Design;ML implementation},
 volume = {22},
 journal = {International Journal of Qualitative Methods},
 doi = {10.1177/16094069221144940}
}


@article{Gopagoni.2020,
 abstract = {Machine learning techniques are designed to derive knowledge out of existing data. Increased computational power, use of natural language processing, image processing methods made easy creation of rich data. Good domain knowledge is required to build useful models. Uncertainty remains around choosing the right sample data, variables reduction and selection of statistical algorithm. A suitable statistical method coupled with explaining variables is critical for model building and analysis. There are multiple choices around each parameter. An automated system which could help the scientists to select an appropriate data set coupled with learning algorithm will be very useful. A freely available web-based platform, named automated machine learning tool (AMLT), is developed in this study. AMLT will automate the entire model building process. AMLT is equipped with all most commonly used variable selection methods, statistical methods both for supervised and unsupervised learning. AMLT can also do the clustering. AMLT uses statistical principles like R2 to rank the models and automatic test set validation. Tool is validated for connectivity and capability by reproducing two published works. {\copyright} Science and Information Organization.},
 author = {Gopagoni, D. and Lakshmi, P. V.},
 year = {2020},
 title = {Automated machine learning tool: The first stop for data science and statistical model building},
 keywords = {Artefact Design},
 pages = {410--418},
 number = {2},
 journal = {International Journal of Advanced Computer Science and Applications},
 file = {Gopagoni, Lakshmi 2020 - Automated machine learning tool:Attachments/Gopagoni, Lakshmi 2020 - Automated machine learning tool.pdf:application/pdf}
}


@article{Gosiewska.2021,
 abstract = {Machine learning has proved to generate useful predictive models that can and should support decision makers in many areas. The availability of tools for AutoML makes it possible to quickly create an effective but complex predictive model. However, the complexity of such models is often a major obstacle in applications, especially in terms of high-stake decisions. We are experiencing a growing number of examples where the use of black boxes leads to decisions that are harmful, unfair or simply wrong. In this paper, we show that very often we can simplify complex models without compromising their performance; however, with the benefit of much needed transparency. We propose a framework that uses elastic black boxes as supervisor models to create simpler, less opaque, yet still accurate and interpretable glass box models. The new models were created using newly engineered features extracted with the help of a supervisor model. We supply the analysis using a large-scale benchmark on several tabular data sets from the OpenML database. There are tree main results of this paper: 1) we show that extracting information from complex models may improve the performance of simpler models, 2) we question a common myth that complex predictive models outperform simpler predictive models, 3) we present a real-life application of the proposed method. {\copyright} 2021 The Authors},
 author = {Gosiewska, A. and Kozak, A. and Biecek, P.},
 year = {2021},
 title = {Simpler is better: Lifting interpretability-performance trade-off via automated feature engineering},
 keywords = {Artefact Design},
 volume = {150},
 journal = {Decision Support Systems},
 file = {Gosiewska, Kozak et al 2021 - Simpler is better:Attachments/Gosiewska, Kozak et al 2021 - Simpler is better.pdf:application/pdf}
}


@article{Goyal.2020,
 abstract = {Background: Although analysis of cardiac magnetic resonance (CMR) images provides accurate and reproducible measurements of left ventricular (LV) volumes, these measurements are usually not performed throughout the cardiac cycle because of lack of tools that would allow such analysis within a reasonable timeframe. A fully-automated machine-learning (ML) algorithm was recently developed to automatically generate LV volume-time curves. Our aim was to validate ejection and filling parameters calculated from these curves using conventional analysis as a reference. Methods: We studied 21 patients undergoing clinical CMR examinations. LV volume-time curves were obtained using the ML-based algorithm (Neosoft), and independently using slice-by-slice, frame-by-frame manual tracing of the endocardial boundaries. Ejection and filling parameters derived from these curves were compared between the two techniques. For each parameter, Bland-Altman bias and limits of agreement (LOA) were expressed in percent of the mean measured value. Results: Time-volume curves were generated using the automated ML analysis within 2.5 $\pm$ 0.5 min, considerably faster than the manual analysis (43 $\pm$ 14 min per patient, including {\~{}}10 slices with 25--32 frames per slice). Time-volume curves were similar between the two techniques in magnitude and shape. Size and function parameters extracted from these curves showed no significant inter-technique differences, reflected by high correlations, small biases ({\textless}10{\%}) and mostly reasonably narrow LOA. Conclusion: ML software for dynamic LV volume measurement allows fast and accurate, fully automated analysis of ejection and filling parameters, compared to manual tracing based analysis. The ability to quickly evaluate time-volume curves is important for a more comprehensive evaluation of the patient's cardiac function. {\copyright} 2019},
 author = {Goyal, N. and Mor-Avi, V. and Volpato, V. and Narang, A. and Wang, S. and Salerno, M. and Lang, R. M. and Patel, A. R.},
 year = {2020},
 title = {Machine learning based quantification of ejection and filling parameters by fully automated dynamic measurement of left ventricular volumes from cardiac magnetic resonance images},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076496040&doi=10.1016%2fj.mri.2019.12.004&partnerID=40&md5=a590da2567d0753660081b23ecac3d0f},
 keywords = {ML implementation},
 pages = {28--32},
 volume = {67},
 journal = {Magnetic Resonance Imaging},
 doi = {10.1016/j.mri.2019.12.004},
 file = {Goyal, Mor-Avi et al. 2020 - Machine learning based quantification:Attachments/Goyal, Mor-Avi et al. 2020 - Machine learning based quantification.pdf:application/pdf}
}


@inproceedings{Griep.2023,
 abstract = {Automated machine learning (AutoML) creates additional opportunities for less advanced users to build and test their own data mining models. Even though AutoML creates the models for the user, there is still technical knowledge and tools needed to evaluate those models, and due to the black-box nature of the machine learning models, problems can arise with regard to algorithmic biases and fairness. Such biases can escalate in future applications, necessitating a structured approach for fairness evaluation in AutoML. This involves defining fairness criteria, selecting appropriate metrics, assessing fairness across groups, and addressing biases. In the realm of educational data mining, where AutoML is prevalent, biases related to attributes like gender or race can lead to unethical outcomes. Since fairness metrics vary in definition and strength, and some may even contradict others, making fairness evaluation more complex. In this paper, ten fairness metrics were chosen, explored, and implemented on four AutoML tools, Vertex AI, AutoSklearn, AutoKeras, and PyCaret. We identified two open educational datasets and built both prediction and classification models on those AutoML frameworks. We report our work in evaluating different machine learning models created by AutoML and provide discussions about the challenges in evaluating fairness in those models and our effort to mitigate and resolve the problems of algorithmic bias in educational data mining. {\copyright} 2023 ACM.},
 author = {Griep, K. and Stone, L. and Edwards, S. and Hill, M. and Cui, J. and Liu, C. and Jia, Q. and Song, Y.},
 title = {Ensuring Ethical, Transparent, and Auditable Use of Education Data and Algorithms on AutoML},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187556696&doi=10.1145%2f3639479.3639492&partnerID=40&md5=7be3a2c05a22dd4b9984af3e39a7dd2d},
 keywords = {Technical Review},
 pages = {66--72},
 year = {2023},
 doi = {10.1145/3639479.3639492}
}


@article{Guevara.2023,
 abstract = {In this research, we describe the MazeGen framework (as a maze generator), which generates navigation scenarios using Grammatical Evolution for robots or drones to navigate. The maze generator uses evolutionary algorithms to create robotic navigation scenarios with different semantic levels along a scenario profile. Grammatical Evolution is a Machine Learning technique from the Evolutionary Computing branch that uses a BNF grammar to describe the language of the possible scenario universe and a numerical encoding of individual scenarios along that grammar. Through a mapping process, it converts new numerical individuals obtained by operations on the parents' encodings to a new solution by means of grammar. In this context, the grammar describes the scenario elements and some composition rules. We also analyze associated concepts of complexity, understanding complexity as the cost of production of the scenario and skill levels needed to move around the maze. Preliminary results and statistics evidence a low correlation between complexity and the number of obstacles placed, as configurations with more difficult obstacle dispositions were found in the early stages of the evolution process and also when analyzing mazes taking into account their semantic meaning, earlier versions of the experiment not only resulted as too simplistic for the Smart Manufacturing domain, but also lacked correlation with possible real-world scenarios, as was evidenced in our experiments, where the most semantic meaning results had the lowest fitness score. They also show the emerging technology status of this approach, as we still need to find out how to reliably find solvable scenarios and characterize those belonging to the same class of equivalence. Despite being an emerging technology, MazeGen allows users to simplify the process of building configurations for smart manufacturing environments, by making it faster, more efficient, and reproducible, and it also puts the non-expert programmer in the center of the development process, as little boilerplate code is needed. {\copyright} 2023 by the authors.},
 author = {Guevara, I. H. and Margaria, T.},
 year = {2023},
 title = {MazeGen: A Low-Code Framework for Bootstrapping Robotic Navigation Scenarios for Smart Manufacturing Contexts},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159171741&doi=10.3390%2felectronics12092058&partnerID=40&md5=ea56f8e81911b6b4a5bbf3f19869c0f3},
 keywords = {Artefact Design},
 volume = {12},
 number = {9},
 journal = {Electronics (Switzerland)},
 doi = {10.3390/electronics12092058},
 file = {Guevara, Margaria 2023 - MazeGen A Low-Code Framework:Attachments/Guevara, Margaria 2023 - MazeGen A Low-Code Framework.pdf:application/pdf}
}


@article{Gundersen.2022,
 abstract = {Science is experiencing an ongoing reproducibility crisis. In light of this crisis, our objective is to investigate whether machine learning platforms provide out-of-the-box reproducibility. Our method is twofold: First, we survey machine learning platforms for whether they provide features that simplify making experiments reproducible out-of-the-box. Second, we conduct the exact same experiment on four different machine learning platforms, and by this varying the processing unit and ancillary software only. The survey shows that no machine learning platform supports the feature set described by the proposed framework while the experiment reveals statstically significant difference in results when the exact same experiment is conducted on different machine learning platforms. The surveyed machine learning platforms do not on their own enable users to achieve the full reproducibility potential of their research. Also, the machine learning platforms with most users provide less functionality for achieving it. Furthermore, results differ when executing the same experiment on the different platforms. Wrong conclusions can be inferred at the at 95{\%} confidence level. Hence, we conclude that machine learning platforms do not provide reproducibility out-of-the-box and that results generated from one machine learning platform alone cannot be fully trusted. {\copyright} 2021 The Author(s)},
 author = {Gundersen, O. E. and Shamsaliei, S. and Isdahl, R. J.},
 year = {2022},
 title = {Do machine learning platforms provide out-of-the-box reproducibility?},
 keywords = {Technical Review},
 pages = {34--47},
 volume = {126},
 journal = {Future Generation Computer Systems},
 file = {Gundersen, Shamsaliei et al. 2022 - Do machine learning platforms provide:Attachments/Gundersen, Shamsaliei et al. 2022 - Do machine learning platforms provide.pdf:application/pdf}
}


@article{Guo.2022,
 abstract = {To lessen the spread of COVID-19 and other dangerous bacteria and viruses, contactless distribution of different items has gained widespread popularity. In order to complete delivery tasks at a catering facility, this paper explores the development of an autonomous mobile robot. The robot, in particular, plans its path and maintains smooth and flexible mobility using a Time Elastic Band (TEB) motion control method and an upgraded Dijkstra algorithm. On the open-source AI platform of iFLYTEK, a voice recognition module was trained to recognize voice signals of different tones and loudness, and an image recognition capability was attained using YOLOv4 and SIFT. The UCAR intelligent vehicle platform, made available by iFLYTEK, served as the foundation for the development of the mobile robot system. The robot took part in China's 16th National University Student Intelligent Car Race, an experimental demonstration test of the developed mobile robotics. The results of the experiments and task tests demonstrated that the proposed robot architecture was workable. In addition, we designed and put together a mobile robot utilizing components from the Taobao website. Compared to UCAR, this robot is less expensive and has the flexibility to be used in a variety of real-world settings.},
 author = {Guo, P. and Shi, H. C. and Wang, S. J. and Tang, L. S. and Wang, Z. P.},
 year = {2022},
 title = {An ROS Architecture for Autonomous Mobile Robots with UCAR Platforms in Smart Restaurants},
 keywords = {ML implementation},
 volume = {10},
 number = {10},
 issn = {2075-1702},
 journal = {MACHINES},
 doi = {10.3390/machines10100844},
 file = {Guo, Shi et al. 2022 - An ROS Architecture for Autonomous:Attachments/Guo, Shi et al. 2022 - An ROS Architecture for Autonomous.pdf:application/pdf}
}


@inproceedings{Haagen.2023,
 abstract = {Machine learning (ML) algorithms are increasingly used in high-stake domains like healthcare. While ML systems frequently outperform humans in specific tasks, ensuring safety and transparency is critical in these domains. Interpretability, therefore, plays a crucial role in understanding the decision-making process, auditing and correction of ML models and establishing trust. Furthermore, there is a growing demand for automated machine learning (AutoML) to facilitate model development without expert intervention. However, the combination of interpretability and AutoML has received limited attention thus far. In this study, we propose two objective model-agnostic measures of interpretability to quantify model compactness and explanation stability, embedded within an automated interpretable ML pipeline. We experiment with a set of interpretable models on medical classification tasks reporting the proposed measures along with the predictive performances. We further conduct a user study with domain experts to evaluate the correlation between these measures and the subjective concept of interpretability. Our findings demonstrate the effectiveness of the proposed measures, affirming their success and validating their utility in creating an interpretable automated pipeline. {\copyright} 2023 CEUR-WS. All rights reserved.},
 author = {Haagen, T. and Kaya, H. and Snijder, J. and Nierman, M.},
 title = {AutoXplain: Towards Automated Interpretable Model Selection},
 keywords = {Artefact Design;Case Study},
 pages = {18--23},
 year = {2023},
 file = {Haagen, Kaya et al 2023 - AutoXplain:Attachments/Haagen, Kaya et al 2023 - AutoXplain.pdf:application/pdf}
}


@article{Hall.2023,
 abstract = {Objective: Healthcare is increasingly digitized, yet remote and automated machine learning (ML) triage prediction systems for virtual urgent care use remain limited. The Canadian Triage and Acuity Scale (CTAS) is the gold standard triage tool for in-person care in Canada. The current work describes the development of a ML-based acuity score modelled after the CTAS system. Methods: The ML-based acuity score model was developed using 2,460,109 de-identified patient-level encounter records from three large healthcare organizations (Ontario, Canada). Data included presenting complaint, clinical modifiers, age, sex, and self-reported pain. 2,041,987 records were high acuity (CTAS 1--3) and 416,870 records were low acuity (CTAS 4--5). Five models were trained: decision tree, k-nearest neighbors, random forest, gradient boosting regressor, and neural net. The outcome variable of interest was the acuity score predicted by the ML system compared to the CTAS score assigned by the triage nurse. Results: Gradient boosting regressor demonstrated the greatest prediction accuracy. This final model was tuned toward up triaging to minimize patient risk if adopted into the clinical context. The algorithm predicted the same score in 47.4{\%} of cases, and the same or more acute score in 95.0{\%} of cases. Conclusions: The ML algorithm shows reasonable predictive accuracy and high predictive safety and was developed using the largest dataset of its kind to date. Future work will involve conducting a pilot study to validate and prospectively assess reliability of the ML algorithm to assign acuity scores remotely. {\copyright} 2023, BioMed Central Ltd., part of Springer Nature.},
 author = {Hall, J. N. and Galaev, R. and Gavrilov, M. and Mondoux, S.},
 year = {2023},
 title = {Development of a machine learning-based acuity score prediction model for virtual care settings},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173030342&doi=10.1186%2fs12911-023-02307-z&partnerID=40&md5=dc6711e083bbc57d8469a32f366f1d5a},
 keywords = {ML implementation},
 volume = {23},
 number = {1},
 journal = {BMC Medical Informatics and Decision Making},
 doi = {10.1186/s12911-023-02307-z},
 file = {Hall, Galaev et al. 2023 - Development of a machine learning-based:Attachments/Hall, Galaev et al. 2023 - Development of a machine learning-based.pdf:application/pdf}
}


@article{HattrickSimpers.2020,
 abstract = {In this short comment we present a reproducibility study for our recent manuscript {\textquotedbl}A simple constrained machine learning model for predicting high-pressure-hydrogen-compressor materials{\textquotedbl} by Hattrick-Simpers, et al., Mol. Syst. Des. Eng., 2018, 3, 509{\textquotedbl} using a suite of open source materials data science tools. The principal goal of this study is to provide the interested reader the ability to reproduce our previous machine learning model with minimal effort and then perform predictions upon the holdout set used in that manuscript. In transcribing our model from the Java-based Magpie/Weka framework to the Python-based Matminer/scikit-learn framework we noticed an unexpected discrepancy in the predictions between the two platforms. To compare the performance of nominally equivalent random forest regression models across these two platforms, we trained and evaluated 50 replicate models for each platform using random 90{\%} subsets of the full hydride training set for each replicate. The Magpie/Weka models showed somewhat higher predicted mean absolute error (5.6 +/- 0.4) than the Matminer/scikit-learn models (4.2 +/- 0.4) on the holdout set, although the validation statistics were within error of one another. It is beyond the scope of this comment to fully analyze the ultimate source of the variance in these predictions, but we speculate that some contribution results from differences in how Magpie treats duplicate compositions in the training set and/or differences in RF implementation between Weka and scikit-learn.

Design, System, Application Using machine learning techniques to predict chemistries of materials with novel properties has been of great interest to the materials community over the past few years. Unfortunately, many of these approaches will predict the figure of merit for any combination of materials with no insight as to the potential stability of the material or its engineering feasibility. While evaluation of material stability can be challenging, the application of techno-economic constraints to materials design is relatively straightforward and can be implemented early in the prediction process. In this manuscript, we used a free open source materials machine learning platform on a free open source experimental database to generate thousands of new alloy combinations with favorable enthalpies of formation for high pressure compressors. We then whittled down our list of potential alloys using a series of engineering constraints such as enthalpy values, cost, and simple stoichiometric rules. This enabled us to focus our stability check to the Fe-Mn-Ti-X system. Comparison of the Fe-Mn-Ti to CALPHAD, previous experimental studies, and multiple DFT studies resulted in contradictory predictions of stability indicating that it is a system with the potential to provide insights to materials scientists and engineers. This time we provided the code to run the machine learning model.},
 author = {Hattrick-Simpers, J. and DeCost, B.},
 year = {2020},
 title = {Comment on {\textquotedbl}A simple constrained machine learning model for predicting high-pressure-hydrogen-compressor materials{\textquotedbl} by Hattrick-Simpers, {\textless}i{\textgreater}et al.{\textless}/i{\textgreater}, {\textless}i{\textgreater}Molecular Systems Design {\&} Engineering{\textless}/i{\textgreater}, 2018, 3, 509},
 keywords = {Comment},
 pages = {589--591},
 volume = {5},
 number = {2},
 issn = {2058-9689},
 journal = {MOLECULAR SYSTEMS DESIGN {\&} ENGINEERING},
 doi = {10.1039/c9me00138g},
 file = {Hattrick-Simpers, DeCost 2020 - Comment on A simple constrained:Attachments/Hattrick-Simpers, DeCost 2020 - Comment on A simple constrained.pdf:application/pdf}
}


@article{Hendawi.2023,
 abstract = {Background: Machine learning approaches, including deep learning, have demonstrated remarkable effectiveness in the diagnosis and prediction of diabetes. However, these approaches often operate as opaque black boxes, leaving health care providers in the dark about the reasoning behind predictions. This opacity poses a barrier to the widespread adoption of machine learning in diabetes and health care, leading to confusion and eroding trust. Objective: This study aimed to address this critical issue by developing and evaluating an explainable artificial intelligence (AI) platform, XAI4Diabetes, designed to empower health care professionals with a clear understanding of AI-generated predictions and recommendations for diabetes care. XAI4Diabetes not only delivers diabetes risk predictions but also furnishes easily interpretable explanations for complex machine learning models and their outcomes. Methods: XAI4Diabetes features a versatile multimodule explanation framework that leverages machine learning, knowledge graphs, and ontologies. The platform comprises the following four essential modules: (1) knowledge base, (2) knowledge matching, (3) prediction, and (4) interpretation. By harnessing AI techniques, XAI4Diabetes forecasts diabetes risk and provides valuable insights into the prediction process and outcomes. A structured, survey-based user study assessed the app's usability and influence on participants' comprehension of machine learning predictions in real-world patient scenarios. Results: A prototype mobile app was meticulously developed and subjected to thorough usability studies and satisfaction surveys. The evaluation study findings underscore the substantial improvement in medical professionals' comprehension of key aspects, including the (1) diabetes prediction process, (2) data sets used for model training, (3) data features used, and (4) relative significance of different features in prediction outcomes. Most participants reported heightened understanding of and trust in AI predictions following their use of XAI4Diabetes. The satisfaction survey results further revealed a high level of overall user satisfaction with the tool. Conclusions: This study introduces XAI4Diabetes, a versatile multi-model explainable prediction platform tailored to diabetes care. By enabling transparent diabetes risk predictions and delivering interpretable insights, XAI4Diabetes empowers health care professionals to comprehend the AI-driven decision-making process, thereby fostering transparency and trust. These advancements hold the potential to mitigate biases and facilitate the broader integration of AI in diabetes care. {\copyright}Rasha Hendawi, Juan Li, Souradip Roy.},
 author = {Hendawi, R. and Li, J. and Roy, S.},
 year = {2023},
 title = {A Mobile App That Addresses Interpretability Challenges in Machine Learning--Based Diabetes Predictions: Survey-Based User Study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178111667&doi=10.2196%2f50328&partnerID=40&md5=a1f8d7929c693af278cfb2f1aceaf64a},
 keywords = {Technical Review},
 volume = {7},
 number = {1},
 journal = {JMIR Formative Research},
 doi = {10.2196/50328},
 file = {Hendawi, Li et al. 2023 - A Mobile App That Addresses:Attachments/Hendawi, Li et al. 2023 - A Mobile App That Addresses.pdf:application/pdf}
}


@inproceedings{HerreraSanchez.2024,
 abstract = {Medical imaging classification is an area that has taken relevance in recent years due to the capability to support the medical specialist at the time of diagnosis. However, there are different instruments to obtain images from the body, and each body organ is captured differently due to its chemical composition. In this way, there are some difficulties in working with different imaging modalities. Firstly, using different functions or methods to extract features from the images is necessary. Secondly, the classification performance depends on the relevant features extracted from the images, and thirdly, it is necessary to find the classifier that performs with the minimum error. Following the concept of Auto-Machine Learning (AutoML), where the feature engineering and the hyperparameter tuning of the classifier are done automatically, this work proposes an automated approach for feature extraction and image classification based on Genetic Programming. The approach modifies the functions and their parameters and the hyperparameters for the classifier. The results show that the approach can deal with different imaging modalities, demonstrating that feature extraction is necessary to increase the classification performance. For X-ray images, it achieves a classification accuracy of 0.99, and for computerized tomography, it achieves an accuracy of 0.96. On the other hand, the solutions given by the approach are easily reproducible and easy to interpret. {\copyright} 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
 author = {Herrera-S{\'a}nchez, D. and Acosta-Mesa, H.-G. and Mezura-Montes, E.},
 title = {Auto Machine Learning Based on Genetic Programming for Medical Image Classification},
 keywords = {Artefact Design},
 pages = {349--359},
 year = {2024},
 doi = {10.1007/978-3-031-51940-6{\textunderscore }26},
 file = {Herrera-S{\'a}nchez, Acosta-Mesa et al 2024 - Auto Machine Learning Based:Attachments/Herrera-S{\'a}nchez, Acosta-Mesa et al 2024 - Auto Machine Learning Based.pdf:application/pdf}
}


@article{Heydon.2021,
 abstract = {Background/aims Human grading of digital images from diabetic retinopathy (DR) screening programmes represents a significant challenge, due to the increasing prevalence of diabetes. We evaluate the performance of an automated artificial intelligence (AI) algorithm to triage retinal images from the English Diabetic Eye Screening Programme (DESP) into test-positive/technical failure versus test-negative, using human grading following a standard national protocol as the reference standard. Methods Retinal images from 30 405 consecutive screening episodes from three English DESPs were manually graded following a standard national protocol and by an automated process with machine learning enabled software, EyeArt v2.1. Screening performance (sensitivity, specificity) and diagnostic accuracy (95{\%} CIs) were determined using human grades as the reference standard. Results Sensitivity (95{\%} CIs) of EyeArt was 95.7{\%} (94.8{\%} to 96.5{\%}) for referable retinopathy (human graded ungradable, referable maculopathy, moderate-to-severe non-proliferative or proliferative). This comprises sensitivities of 98.3{\%} (97.3{\%} to 98.9{\%}) for mild-to-moderate non-proliferative retinopathy with referable maculopathy, 100{\%} (98.7{\%},100{\%}) for moderate-to-severe non-proliferative retinopathy and 100{\%} (97.9{\%},100{\%}) for proliferative disease. EyeArt agreed with the human grade of no retinopathy (specificity) in 68{\%} (67{\%} to 69{\%}), with a specificity of 54.0{\%} (53.4{\%} to 54.5{\%}) when combined with non-referable retinopathy. Conclusion The algorithm demonstrated safe levels of sensitivity for high-risk retinopathy in a real-world screening service, with specificity that could halve the workload for human graders. AI machine learning and deep learning algorithms such as this can provide clinically equivalent, rapid detection of retinopathy, particularly in settings where a trained workforce is unavailable or where large-scale and rapid results are needed. {\copyright} 2021 BMJ Publishing Group. All rights reserved.},
 author = {Heydon, P. and Egan, C. and Bolter, L. and Chambers, R. and Anderson, J. and Aldington, S. and Stratton, I. M. and Scanlon, P. H. and Webster, L. and Mann, S. and {Du Chemin}, A. and Owen, C. G. and Tufail, A. and Rudnicka, A. R.},
 year = {2021},
 title = {Prospective evaluation of an artificial intelligence-enabled algorithm for automated diabetic retinopathy screening of 30 000 patients},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089784050&doi=10.1136%2fbjophthalmol-2020-316594&partnerID=40&md5=5389bca6c9e906d40962611a547e4d32},
 keywords = {ML implementation},
 pages = {723--728},
 volume = {105},
 number = {5},
 journal = {British Journal of Ophthalmology},
 doi = {10.1136/bjophthalmol-2020-316594},
 file = {Heydon, Egan et al. 2021 - Prospective evaluation of an artificial:Attachments/Heydon, Egan et al. 2021 - Prospective evaluation of an artificial.pdf:application/pdf}
}


@inproceedings{Horn.2020,
 abstract = {This paper describes the autofeat Python library, which provides a scikit-learn style linear regression model with automated feature engineering and selection capabilities. Complex non-linear machine learning models such as neural networks are in practice often difficult to train and even harder to explain to non-statisticians, who require transparent analysis results as a basis for important business decisions. While linear models are efficient and intuitive, they generally provide lower prediction accuracies. Our library provides a multi-step feature engineering and selection process, where first a large pool of non-linear features is generated, from which then a small and robust set of meaningful features is selected, which improve the prediction accuracy of a linear model while retaining its interpretability. {\copyright} Springer Nature Switzerland AG 2020.},
 author = {Horn, F. and Pack, R. and Rieger, M.},
 title = {The autofeat python library for automated feature engineering and selection},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083711893&doi=10.1007%2f978-3-030-43823-4_10&partnerID=40&md5=5ab59506953b9e288aa905eaeccc80cb},
 keywords = {Artefact Design},
 pages = {111--120},
 year = {2020},
 doi = {10.1007/978-3-030-43823-4{\textunderscore }10}
}


@article{Ikemura.2021,
 abstract = {Background: During a pandemic, it is important for clinicians to stratify patients and decide who receives limited medical resources. Machine learning models have been proposed to accurately predict COVID-19 disease severity. Previous studies have typically tested only one machine learning algorithm and limited performance evaluation to area under the curve analysis. To obtain the best results possible, it may be important to test different machine learning algorithms to find the best prediction model. Objective: In this study, we aimed to use automated machine learning (autoML) to train various machine learning algorithms. We selected the model that best predicted patients' chances of surviving a SARS-CoV-2 infection. In addition, we identified which variables (ie, vital signs, biomarkers, comorbidities, etc) were the most influential in generating an accurate model. Methods: Data were retrospectively collected from all patients who tested positive for COVID-19 at our institution between March 1 and July 3, 2020. We collected 48 variables from each patient within 36 hours before or after the index time (ie, real-time polymerase chain reaction positivity). Patients were followed for 30 days or until death. Patients' data were used to build 20 machine learning models with various algorithms via autoML. The performance of machine learning models was measured by analyzing the area under the precision-recall curve (AUPCR). Subsequently, we established model interpretability via Shapley additive explanation and partial dependence plots to identify and rank variables that drove model predictions. Afterward, we conducted dimensionality reduction to extract the 10 most influential variables. AutoML models were retrained by only using these 10 variables, and the output models were evaluated against the model that used 48 variables. Results: Data from 4313 patients were used to develop the models. The best model that was generated by using autoML and 48 variables was the stacked ensemble model (AUPRC=0.807). The two best independent models were the gradient boost machine and extreme gradient boost models, which had an AUPRC of 0.803 and 0.793, respectively. The deep learning model (AUPRC=0.73) was substantially inferior to the other models. The 10 most influential variables for generating high-performing models were systolic and diastolic blood pressure, age, pulse oximetry level, blood urea nitrogen level, lactate dehydrogenase level, D-dimer level, troponin level, respiratory rate, and Charlson comorbidity score. After the autoML models were retrained with these 10 variables, the stacked ensemble model still had the best performance (AUPRC=0.791). Conclusions: We used autoML to develop high-performing models that predicted the survival of patients with COVID-19. In addition, we identified important variables that correlated with mortality. This is proof of concept that autoML is an efficient, effective, and informative method for generating machine learning--based clinical decision support tools. {\copyright} Kenji Ikemura, Eran Bellin, Yukako Yagi, Henny Billett, Mahmoud Saada, Katelyn Simone, Lindsay Stahl, James Szymanski, D Y Goldstein, Morayma Reyes Gil.},
 author = {Ikemura, K. and Bellin, E. and Yagi, Y. and Billett, H. and Saada, M. and Simone, K. and Stahl, L. and Szymanski, J. and Goldstein, D. Y. and Gil, M. R.},
 year = {2021},
 title = {Using automated machine learning to predict the mortality of patients with COVID-19: Prediction model development study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101866295&doi=10.2196%2f23458&partnerID=40&md5=f0181b7aac369f27c45cf264eb686f1f},
 keywords = {ML implementation},
 volume = {23},
 number = {2},
 journal = {Journal of Medical Internet Research},
 doi = {10.2196/23458}
}


@article{Inagaki.2023,
 abstract = {Purpose: Less-invasive early diagnosis of lung cancer is essential for improving patient survival rates. The purpose of this study is to demonstrate that serum comprehensive miRNA profile is high sensitive biomarker to early-stage lung cancer in direct comparison to the conventional blood biomarker using next-generation sequencing (NGS) technology combined with automated machine learning (AutoML). Methods: We first evaluated the reproducibility of our measurement system using Pearson's correlation coefficients between samples derived from a single pooled RNA sample. To generate comprehensive miRNA profile, we performed NGS analysis of miRNAs in 262 serum samples. Among the discovery set (57 patients with lung cancer and 57 healthy controls), 1123 miRNA-based diagnostic models for lung cancer detection were constructed and screened using AutoML technology. The diagnostic faculty of the best performance model was evaluated by inspecting the validation samples (74 patients with lung cancer and 74 healthy controls). Results: The Pearson's correlation coefficients between samples derived from the pooled RNA sample $\geq$ 0.98. In the validation analysis, the best model showed a high AUC score (0.98) and a high sensitivity for early stage lung cancer (85.7{\%}, n = 28). Furthermore, in comparison to carcinoembryonic antigen (CEA), a conventional blood biomarker for adenocarcinoma, the miRNA-based model showed higher sensitivity for early-stage lung adenocarcinoma (CEA, 27.8{\%}, n = 18; miRNA-based model, 77.8{\%}, n = 18). Conclusion: The miRNA-based diagnostic model showed a high sensitivity for lung cancer, including early-stage disease. Our study provides the experimental evidence that serum comprehensive miRNA profile can be a highly sensitive blood biomarker for early-stage lung cancer. {\copyright} 2023, The Author(s).},
 author = {Inagaki, M. and Uchiyama, M. and Yoshikawa-Kawabe, K. and Ito, M. and Murakami, H. and Gunji, M. and Minoshima, M. and Kohnoh, T. and Ito, R. and Kodama, Y. and Tanaka-Sakai, M. and Nakase, A. and Goto, N. and Tsushima, Y. and Mori, S. and Kozuka, M. and Otomo, R. and Hirai, M. and Fujino, M. and Yokoyama, T.},
 year = {2023},
 title = {Comprehensive circulating microRNA profile as a supersensitive biomarker for early-stage lung cancer screening},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153037366&doi=10.1007%2fs00432-023-04728-9&partnerID=40&md5=c07a29aafe0cd00ad336620cce48b4b1},
 keywords = {ML implementation},
 pages = {8297--8305},
 volume = {149},
 number = {11},
 journal = {Journal of Cancer Research and Clinical Oncology},
 doi = {10.1007/s00432-023-04728-9},
 file = {Inagaki, Uchiyama et al. 2023 - Comprehensive circulating microRNA profile:Attachments/Inagaki, Uchiyama et al. 2023 - Comprehensive circulating microRNA profile.pdf:application/pdf}
}


@incollection{Isdahl.2019,
 abstract = {Even machine learning experiments that are fully conducted on computers are not necessarily reproducible. An increasing number of open source and commercial, closed source machine learning platforms are being developed that help address this problem. However, there is no standard for assessing and comparing which features are required to fully support reproducibility. We propose a quantitative method that alleviates this problem. Based on the proposed method we assess and compare the current state of the art machine learning platforms for how well they support making empirical results reproducible. Our results show that BEAT and Floydhub have the best support for reproducibility with Codalab and Kaggle as close contenders. The most commonly used machine learning platforms provided by the big tech companies have poor support for reproducibility. {\copyright} 2019 IEEE.},
 author = {Isdahl, R. and Gundersen, O. E.},
 title = {Out-of-the-box reproducibility: A survey of machine learning platforms},
 keywords = {Technical Review},
 pages = {86--95},
 booktitle = {2019 15th International Conference on eScience (eScience)},
 doi = {10.1109/eScience.2019.00017},
 file = {Isdahl, Gundersen 2019 - Out-of-the-box reproducibility:Attachments/Isdahl, Gundersen 2019 - Out-of-the-box reproducibility.pdf:application/pdf}
}


@article{Jaremko.2021,
 abstract = {Objective: Preliminary assessment, via OMERACT filter, of manual and automated MRI hip effusion Volumetric Quantitative Measurement (VQM). Methods: For 358 hips (93 osteoarthritis subjects, bilateral, 2 time points), 2 radiologists performed manual VQM using custom Matlab software. A Mask R-CNN artificial-intelligence (AI) tool was trained to automatically compute joint fluid volumes. Results: Manual VQM had excellent inter-observer reliability (ICC 0.96). AI predicted hip fluid volumes with ICC 0.86 (status), 0.58 (change) vs. 2 human readers. Conclusion: Hip joint fluid volumes are reliably assessed by VQM. It is feasible to automate this approach using AI, with promising initial reliability. {\copyright} 2021 Elsevier Inc.},
 author = {Jaremko, J. L. and Felfeliyan, B. and Hareendranathan, A. and Thejeel, B. and Vanessa, Q.-L. and {\O}stergaard, M. and Conaghan, P. G. and Lambert, R.G.W. and Ronsky, J. L. and Maksymowych, W. P.},
 year = {2021},
 title = {Volumetric quantitative measurement of hip effusions by manual versus automated artificial intelligence techniques: An OMERACT preliminary validation study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103256490&doi=10.1016%2fj.semarthrit.2021.03.009&partnerID=40&md5=393364d8ef2dec7ccd599a39e80dfc52},
 keywords = {ML implementation},
 pages = {623--626},
 volume = {51},
 number = {3},
 journal = {Seminars in Arthritis and Rheumatism},
 doi = {10.1016/j.semarthrit.2021.03.009}
}


@proceedings{Jarrett.2021,
 abstract = {Time-series learning is the bread and butter of data-driven clinical decision support, and the recent explosion in ML research has demonstrated great potential in various healthcare settings. At the same time, medical time-series problems in the wild are challenging due to their highly composite nature: They entail design choices and interactions among components that preprocess data, impute missing values, select features, issue predictions, estimate uncertainty, and interpret models. Despite exponential growth in electronic patient data, there is a remarkable gap between the potential and realized utilization of ML for clinical research and decision support. In particular, orchestrating a real-world project lifecycle poses challenges in engineering (i.e. hard to build), evaluation (i.e. hard to assess), and efficiency (i.e. hard to optimize). Designed to address these issues simultaneously, Clairvoyance proposes a unified, end-to-end, autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical standard, and (iii) interface for optimization. Our ultimate goal lies in facilitating transparent and reproducible experimentation with complex inference workflows, providing integrated pathways for (1) personalized prediction, (2) treatment-effect estimation, and (3) information acquisition. Through illustrative examples on real-world data in outpatient, general wards, and intensive-care settings, we illustrate the applicability of the pipeline paradigm on core tasks in the healthcare journey. To the best of our knowledge, Clairvoyance is the first to demonstrate viability of a comprehensive and automatable pipeline for clinical time-series ML. {\copyright} 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.},
 year = {2021},
 title = {Clairvoyance: A pipeline toolkit for medical time series},
 keywords = {Artefact Design},
 editor = {Jarrett, D. and Yoon, J. and Bica, I. and Qian, Z. and Ercole, A. and {van der Schaar}, M.},
 file = {Jarrett, Yoon et al (Hg) 2021 - CLAIRVOYANCE:Attachments/Jarrett, Yoon et al (Hg) 2021 - CLAIRVOYANCE.pdf:application/pdf}
}


@article{Jhang.2022,
 abstract = {For long-term electrocardiogram (ECG) signal monitoring, a portable and small size acquisition device with Bluetooth low energy (BLE) communication is designed and integrated with a Nvidia Jetson Xavier NX for realizing the electrode motion artifact removal technique. The digitalized ECG codes are converted from a front-end circuit, which contains several amplifiers and filters in the acquisition system. Thereafter, a zero padding scheme is applied for each 10-bits data to separate them into two-bytes data for BLE transmission. Xavier Edge AI platform receives these transmitted data and removes the electrode motion (EM) noise using the proposed low memory shortcut connection-based denoised autoencoder (LMSC-DAE). The simulation results demonstrate that the proposed algorithm significantly improves the signal-to-noise ratio (SNR) by 5.41 dB under the condition of SNRin = 12 dB, compared with convolutional denoising autoencoder with long short-term memory (CNN-LSTM-DAE) method. For practical test, an Arduino DUE platform is employed to generate noise interference by controlling a commercial digital-to-analog convertor. By combining the proposed ECG acquisition device with a non-inverting weighted summer, it can be applied to verify the reproducibility of measurement for the proposed method. The measurement results clearly indicate that the proposed LMSC-DAE has a higher improvement of SNR and lower percentage root-mean-square difference than the state-of-the-art Fully Convolutional Denoising Autoencoder (FCN-DAE).  {\copyright} 2013 IEEE.},
 author = {Jhang, Y.-S. and Wang, S.-T. and Sheu, M.-H. and Wang, S.-H. and Lai, S.-C.},
 year = {2022},
 title = {Integration Design of Portable ECG Signal Acquisition With Deep-Learning Based Electrode Motion Artifact Removal on an Embedded System},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131741297&doi=10.1109%2fACCESS.2022.3178847&partnerID=40&md5=b16c5220d6fa0b686f09d65d87cf707d},
 keywords = {Artefact Design},
 pages = {57555--57564},
 volume = {10},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2022.3178847},
 file = {Jhang, Wang et al. 2022 - Integration Design of Portable ECG:Attachments/Jhang, Wang et al. 2022 - Integration Design of Portable ECG.pdf:application/pdf}
}


@article{Judson.2023,
 abstract = {Purpose: To determine the reliability of an artificial intelligence, deep learning (AI/DL)-based method of chest computer tomography (CT) scan analysis to distinguish pulmonary sarcoidosis from negative lung cancer screening chest CT scans (Lung~Imaging Reporting and Data System score 1, Lung-RADS score 1). Methods: Chest CT scans of pulmonary sarcoidosis were evaluated by a clinician experienced with sarcoidosis and a chest radiologist for clinical and radiologic evidence of sarcoidosis and exclusion of alternative or concomitant pulmonary diseases. The AI/DL based method used an ensemble network architecture combining Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). The method was applied to 126 pulmonary sarcoidosis and 96 Lung-RADS score 1 CT scans. The analytic approach of training and validation of the AI/DL method used a fivefold cross-validation technique, where 4/5th of the available data set was used to train a diagnostic model and tested on the remaining 1/5th of the data set, and repeated 4 more times with non-overlapping validation/test data. The probability values were used to generate Receiver Operating Characteristic (ROC) curves to assess the model's discriminatory power. Results: The sensitivity, specificity, positive and negative predictive value of the AI/DL method for the 5 folds of the training/validation sets and the entire set of CT scans were all over 94{\%} to distinguish pulmonary sarcoidosis from LUNG-RADS score 1 chest CT scans. The area under the curve for the~corresponding ROC curves were all over 97{\%}. Conclusion: This AL/DL model shows promise to distinguish sarcoidosis from alternative pulmonary conditions using minimal radiologic data. {\copyright} 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
 author = {Judson, M. A. and Qiu, J. and Dumas, C. L. and Yang, J. and Sarachan, B. and Mitra, J.},
 year = {2023},
 title = {An Artificial Intelligence Platform for the Radiologic Diagnosis of Pulmonary Sarcoidosis: An Initial Pilot Study of Chest Computed Tomography Analysis to Distinguish Pulmonary Sarcoidosis from a Negative Lung Cancer Screening Scan},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176588605&doi=10.1007%2fs00408-023-00655-1&partnerID=40&md5=18402cbae4788d4b127529d813f93e71},
 keywords = {ML implementation},
 pages = {611--616},
 volume = {201},
 number = {6},
 journal = {Lung},
 doi = {10.1007/s00408-023-00655-1},
 file = {Judson, Qiu et al. 2023 - An Artificial Intelligence Platform:Attachments/Judson, Qiu et al. 2023 - An Artificial Intelligence Platform.pdf:application/pdf}
}


@article{Kammuller.2023,
 abstract = {Right from the beginning, attendance has played an important role in the education systems, not only in student success but in the overall interest of the matter. Although all schools try to accentuate good attendance, still some schools find it hard to achieve the required level (96{\%} in UK) of average attendance. The most productive way of increasing the pupils$\prime$ attendance rate is to predict when it is going to go down, understand the reasons---why it happened---and act on the affecting factors so as to prevent it. Artificial intelligence (AI) is an automated machine learning solution for different types of problems. Several machine learning (ML) models like logistic regression, decision trees, etc. are easy to understand; however, complicated (Neural Network, BART etc.) ML models are not transparent but are black-boxes for humans. It is not always evident how machine intelligence arrived at a decision. However, not always, but in critical applications it is important that humans can understand the reasons for such decisions. In this paper, we present a methodology on the application example of pupil attendance for constructing explanations for AI classification algorithms. The methodology includes building a model of the application in the Isabelle Insider and Infrastructure framework (IIIf) and an algorithm (PCR) that helps us to obtain a detailed logical rule to specify the performance of the black-box algorithm, hence allowing us to explain it. The explanation is provided within the logical model of the IIIf, thus is suitable for human audiences. It has been shown that the RR-cycle of IIIf can be adapted to provide a method for iteratively extracting an explanation by interleaving attack tree analysis with precondition refinement, which finally yields a general rule that describes the decision taken by a black-box algorithm produced by Artificial intelligence. {\copyright} 2023 by the authors.},
 author = {Kamm{\"u}ller, F. and Satija, D.},
 year = {2023},
 title = {Explanation of Student Attendance AI Prediction with the Isabelle Infrastructure Framework $\dagger$},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168757106&doi=10.3390%2finfo14080453&partnerID=40&md5=883dcfb269a6a16820aaf7c4be4979d8},
 keywords = {ML implementation},
 volume = {14},
 number = {8},
 journal = {Information (Switzerland)},
 doi = {10.3390/info14080453},
 file = {Kamm{\"u}ller, Satija 2023 - Explanation of Student Attendance AI:Attachments/Kamm{\"u}ller, Satija 2023 - Explanation of Student Attendance AI.pdf:application/pdf}
}


@article{Karaglani.2022,
 abstract = {Background: The need for minimally invasive biomarkers for the early diagnosis of type 2 diabetes (T2DM) prior to the clinical onset and monitoring of \textgreek{b}-pancreatic cell loss is emerging. Here, we focused on studying circulating cell-free DNA (ccfDNA) as a liquid biopsy biomaterial for accurate diagnosis/monitoring of T2DM. Methods: ccfDNA levels were directly quantified in sera from 96 T2DM patients and 71 healthy individuals via fluorometry, and then fragment DNA size profiling was performed by capillary electrophoresis. Following this, ccfDNA methylation levels of five \textgreek{b}cell-related genes were measured via qPCR. Data were analyzed by automated machine learning to build classifying predictive models. Results: ccfDNA levels were found to be similar between groups but indicative of apoptosis in T2DM. INS (Insulin), IAPP (Islet Amyloid Polypeptide-Amylin), GCK (Glucokinase), and KCNJ11 (Potassium Inwardly Rectifying Channel Subfamily J member 11) levels differed significantly between groups. AutoML analysis delivered biosignatures including GCK, IAPP and KCNJ11 methylation, with the highest ever reported discriminating performance of T2DM from healthy individuals (AUC 0.927). Conclusions: Our data unravel the value of ccfDNA as a minimally invasive biomaterial carrying important clinical information for T2DM. Upon prospective clinical evaluation, the built biosignature can be disruptive for T2DM clinical management. {\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
 author = {Karaglani, M. and Panagopoulou, M. and Cheimonidi, C. and Tsamardinos, I. and Maltezos, E. and Papanas, N. and Papazoglou, D. and Mastorakos, G. and Chatzaki, E.},
 year = {2022},
 title = {Liquid Biopsy in Type 2 Diabetes Mellitus Management: Building Specific Biosignatures via Machine Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124756616&doi=10.3390%2fjcm11041045&partnerID=40&md5=b580b240e78debc4aa93a2b8e7d5d3d1},
 keywords = {ML implementation},
 volume = {11},
 number = {4},
 journal = {Journal of Clinical Medicine},
 doi = {10.3390/jcm11041045},
 file = {Karaglani, Panagopoulou et al. 2022 - Liquid Biopsy in Type 2:Attachments/Karaglani, Panagopoulou et al. 2022 - Liquid Biopsy in Type 2.pdf:application/pdf}
}


@article{Karhade.2021,
 abstract = {Purpose: The purpose of the study was to develop and evaluate an automated machine learning algorithm (AutoML) for children's classification according to early childhood caries (ECC) status. Methods: Clinical, demographic, behavioral, and parent-reported oral health status information for a sample of 6,404 three- to five-year-old children (mean age equals 54 months) participating in an epidemiologic study of early childhood oral health in North Carolina was used. ECC prevalence (decayed, missing, and filled primary teeth surfaces [dmfs] score greater than zero, using an International Caries Detection and Assessment System score greater than or equal to three caries lesion detection threshold) was 54 percent. Ten sets of ECC predictors were evaluated for ECC classification accuracy (i.e., area under the ROC curve [AUC], sensitivity [Se], and positive predictive value [PPV]) using an AutoML deployment on Google Cloud, followed by internal validation and external replication. Results: A parsimonious model including two terms (i.e., children's age and parent-reported child oral health status: excellent/very good/good/fair/poor) had the highest AUC (0.74), Se (0.67), and PPV (0.64) scores and similar performance using an external National Health and Nutrition Examination Survey (NHANES) dataset (AUC equals 0.80, Se equals 0.73, PPV equals 0.49). Contrarily, a comprehensive model with 12 variables covering demographics (e.g., race/ethnicity, parental education), oral health behaviors, fluoride exposure, and dental home had worse performance (AUC equals 0.66, Se equals 0.54, PPV equals 0.61). Conclusions: Parsimonious automated machine learning early childhood caries classifiers, including single-item self-reports, can be valuable for ECC screening. The classifier can accommodate biological information that can help improve its performance in the future.},
 author = {Karhade, D. S. and Roach, J. and Shrestha, P. and Simancas-Pallares, M. A. and Ginnis, J. and Burk, Z.J.S. and Ribeiro, A. A. and Cho, H. and Wu, D. and Divaris, K.},
 year = {2021},
 title = {An Automated Machine Learning Classifier for Early Childhood Caries},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111653558&partnerID=40&md5=bea12d09dd132b9bce8df413ae7a6773},
 keywords = {ML implementation},
 pages = {191--197},
 volume = {43},
 number = {3},
 journal = {Pediatric dentistry}
}


@article{Kazimierczak.2023,
 abstract = {The nasal septum is believed to play a crucial role in the development of the craniofacial skeleton. Nasal septum deviation (NSD) is a common condition, affecting 18--65{\%} of individuals. This study aimed to assess the prevalence of NSD and its potential association with abnormalities detected through cephalometric analysis using artificial intelligence (AI) algorithms. The study included CT scans of 120 consecutive, post-traumatic patients aged 18--30. Cephalometric analysis was performed using an AI web-based software, CephX. The automatic analysis comprised all the available cephalometric analyses. NSD was assessed using two methods: maximum deviation from an ideal non-deviated septum and septal deviation angle (SDA). The concordance of repeated manual measurements and automatic analyses was assessed. Of the 120 cases, 90 met the inclusion criteria. The AI-based cephalometric analysis provided comprehensive reports with over 100 measurements. Only the hinge axis angle (HAA) and SDA showed significant (p = 0.039) negative correlations. The rest of the cephalometric analyses showed no correlation with the NSD indicators. The analysis of the agreement between repeated manual measurements and automatic analyses showed good-to-excellent concordance, except in the case of two angular measurements: LI-N-B and Pr-N-A. The CephX AI platform showed high repeatability in automatic cephalometric analyses, demonstrating the reliability of the AI model for most cephalometric analyses. {\copyright} 2023 by the authors.},
 author = {Kazimierczak, N. and Kazimierczak, W. and Serafin, Z. and Nowicki, P. and Lemanowicz, A. and Nadolska, K. and Janiszewska-Olszowska, J.},
 year = {2023},
 title = {Correlation Analysis of Nasal Septum Deviation and Results of AI-Driven Automated 3D Cephalometric Analysis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175372676&doi=10.3390%2fjcm12206621&partnerID=40&md5=1d8877d94cfa9873fec9d0a301946263},
 keywords = {ML implementation},
 volume = {12},
 number = {20},
 journal = {Journal of Clinical Medicine},
 doi = {10.3390/jcm12206621},
 file = {Kazimierczak, Kazimierczak et al. 2023 - Correlation Analysis of Nasal Septum:Attachments/Kazimierczak, Kazimierczak et al. 2023 - Correlation Analysis of Nasal Septum.pdf:application/pdf}
}


@article{Kazimierczak.2024,
 abstract = {OBJECTIVES: To compare artificial intelligence (AI)-driven web-based platform and manual measurements for analysing facial asymmetry in craniofacial CT examinations. METHODS: The study included 95 craniofacial CT scans from patients aged 18-30 years. The degree of asymmetry was measured based on AI platform-predefined anatomical landmarks: sella (S), condylion (Co), anterior nasal spine (ANS), and menton (Me). The concordance between the results of automatic asymmetry reports and manual linear 3D measurements was calculated. The asymmetry rate (AR) indicator was determined for both automatic and manual measurements, and the concordance between them was calculated. The repeatability of manual measurements in 20 randomly selected subjects was assessed. The concordance of measurements of quantitative variables was assessed with interclass correlation coefficient (ICC) according to the Shrout and Fleiss classification. RESULTS: Erroneous AI tracings were found in 16.8{\%} of cases, reducing the analysed cases to 79. The agreement between automatic and manual asymmetry measurements was very low (ICC {\textless} 0.3). A lack of agreement between AI and manual AR analysis (ICC type 3 = 0) was found. The repeatability of manual measurements and AR calculations showed excellent correlation (ICC type 2 {\textgreater} 0.947). CONCLUSIONS: The results indicate that the rate of tracing errors and lack of agreement with manual AR analysis make it impossible to use the tested AI platform to assess the degree of facial asymmetry. {\copyright} The Author(s) 2023. Published by Oxford University Press on behalf of the British Institute of Radiology and the International Association of Dentomaxillofacial Radiology.},
 author = {Kazimierczak, N. and Kazimierczak, W. and Serafin, Z. and Nowicki, P. and Jankowski, T. and Jankowska, A. and Janiszewska-Olszowska, J.},
 year = {2024},
 title = {Skeletal facial asymmetry: reliability of manual and artificial intelligence-driven analysis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182286393&doi=10.1093%2fdmfr%2ftwad006&partnerID=40&md5=aa1e34437d863924f0906693424dd340},
 keywords = {ML implementation},
 pages = {52--59},
 volume = {53},
 number = {1},
 journal = {Dento maxillo facial radiology},
 doi = {10.1093/dmfr/twad006},
 file = {Kazimierczak, Kazimierczak et al. 2024 - Skeletal facial asymmetry:Attachments/Kazimierczak, Kazimierczak et al. 2024 - Skeletal facial asymmetry.pdf:application/pdf}
}


@article{Kc.2021,
 abstract = {Strategies for drug discovery and repositioning are urgently need with respect to COVID-19. Here we present REDIAL-2020, a suite of computational models for estimating small molecule activities in a range of SARS-CoV-2-related assays. Models were trained using publicly available, high-throughput screening data and by employing different descriptor types and various machine learning strategies. Here we describe the development and use of eleven models that span across the areas of viral entry, viral replication, live virus infectivity, in vitro infectivity and human cell toxicity. REDIAL-2020 is available as a web application through the DrugCentral web portal (http://drugcentral.org/Redial). The web application also provides similarity search results that display the most similar molecules to the query, as well as associated experimental data. REDIAL-2020 can serve as a rapid online tool for identifying active molecules for COVID-19 treatment. {\copyright} 2021, The Author(s), under exclusive licence to Springer Nature Limited.},
 author = {Kc, G. B. and Bocci, G. and Verma, S. and Hassan, M. M. and Holmes, J. and Yang, J. J. and Sirimulla, S. and Oprea, T. I.},
 year = {2021},
 title = {A machine learning platform to estimate anti-SARS-CoV-2 activities},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105206877&doi=10.1038%2fs42256-021-00335-w&partnerID=40&md5=4c0e2a15545c8f2b21b6b1ca93816bb5},
 keywords = {ML implementation},
 pages = {527--535},
 volume = {3},
 number = {6},
 journal = {Nature Machine Intelligence},
 doi = {10.1038/s42256-021-00335-w},
 file = {Kc, Bocci et al. 2021 - A machine learning platform:Attachments/Kc, Bocci et al. 2021 - A machine learning platform.pdf:application/pdf}
}


@article{Khalighifar.2022,
 abstract = {Bats play crucial ecological roles and provide valuable ecosystem services, yet many populations face serious threats from various ecological disturbances. The North American Bat Monitoring Program (NABat) aims to use its technology infrastructure to assess status and trends of bat populations, while developing innovative and community-driven conservation solutions. Here, we present NABat ML, an automated machine-learning algorithm that improves the scalability and scientific transparency of NABat acoustic monitoring. This model combines signal processing techniques and convolutional neural networks (CNNs) to detect and classify recorded bat echolocation calls. We developed our CNN model with internet-based computing resources (`cloud environment'), and trained it on {\textgreater}600,000 spectrogram images. We also incorporated species range maps to improve the robustness and accuracy of the model for future `unseen' data. We evaluated model performance using a comprehensive, independent, holdout dataset. NABat ML successfully distinguished 31 classes (30 species and a noise class) with overall weighted-average accuracy and precision rates of 92{\%}, and $\geq$90{\%} classification accuracy for 19 of the bat species. Using a single cloud-environment computing instance, the entire model training process took {\textless}16~h. Synthesis and applications. Our convolutional neural network (CNN)-based model, NABat ML, classifies 30 North American bat species using their recorded echolocation calls with an overall accuracy of 92{\%}. In addition to providing highly accurate species-level classification, NABat ML and its outputs are compatible with Bayesian and other statistical techniques for measuring uncertainty in classification. Our model is open-source and reproducible, enabling future implementations as software on end-user devices and cloud-based web applications. These qualities make NABat ML highly suitable for applications ranging from grassroots community science initiatives to big-data methods developed and implemented by researchers and professional practitioners. We believe the transparency and accessibility of NABat ML will encourage broad-scale participation in bat monitoring, and enable development of innovative solutions needed to conserve North American bat species. {\copyright} 2022 Her Majesty the Queen in Right of Canada and The Authors. Journal of Applied Ecology published by John Wiley {\&} Sons Ltd on behalf of British Ecological Society. Reproduced with the permission of the Minister of the Environment. This article has been contributed to by U.S. Government employees and their work is in the public domain in the USA.},
 author = {Khalighifar, A. and Gotthold, B. S. and Adams, E. and Barnett, J. and Beard, L. O. and Britzke, E. R. and Burger, P. A. and Chase, K. and Cordes, Z. and Cryan, P. M. and Ferrall, E. and Fill, C. T. and Gibson, S. E. and Haulton, G. S. and Irvine, K. M. and Katz, L. S. and Kendall, W. L. and Long, C. A. and {Mac Aodha}, O. and McBurney, T. and McCarthy, S. and McKown, M. W. and O'Keefe, J. and Patterson, L. D. and Pitcher, K. A. and Rustand, M. and Segers, J. L. and Seppanen, K. and Siemers, J. L. and Stratton, C. and Straw, B. R. and Weller, T. J. and Reichert, B. E.},
 year = {2022},
 title = {NABat ML: Utilizing deep learning to enable crowdsourced development of automated, scalable solutions for documenting North American bat populations},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138290403&doi=10.1111%2f1365-2664.14280&partnerID=40&md5=8e0c354b46bbae6029b95d1718d71d13},
 keywords = {ML implementation},
 pages = {2849--2862},
 volume = {59},
 number = {11},
 journal = {Journal of Applied Ecology},
 doi = {10.1111/1365-2664.14280}
}


@article{KhalighRazavi.2020,
 abstract = {Background: Cognitive impairment is common in patients with multiple sclerosis (MS). Accurate and repeatable measures of cognition have the potential to be used as markers of disease activity. Methods: We developed a 5-min computerized test to measure cognitive dysfunction in patients with MS. The proposed test - named the Integrated Cognitive Assessment (ICA) - is self-administered and language-independent. Ninety-one MS patients and 83 healthy controls (HC) took part in Substudy 1, in which each participant took the ICA test and the Brief International Cognitive Assessment for MS (BICAMS). We assessed ICA's test-retest reliability, its correlation with BICAMS, its sensitivity to discriminate patients with MS from the HC group, and its accuracy in detecting cognitive dysfunction. In Substudy 2, we recruited 48 MS patients, 38 of which had received an 8-week physical and cognitive rehabilitation programme and 10 MS patients who did not. We examined the association between the level of serum neurofilament light (NfL) in these patients and their ICA scores and Symbol Digit Modalities Test (SDMT) scores pre- and post-rehabilitation. Results: The ICA demonstrated excellent test-retest reliability (r = 0.94), with no learning bias, and showed a high level of convergent validity with BICAMS. The ICA was sensitive in discriminating the MS patients from the HC group, and demonstrated high accuracy (AUC = 95{\%}) in discriminating cognitively normal from cognitively impaired participants. Additionally, we found a strong association (r = - 0.79) between ICA score and the level of NfL in MS patients before and after rehabilitation. Conclusions: The ICA has the potential to be used as a digital marker of cognitive impairment and to monitor response to therapeutic interventions. In comparison to standard cognitive tools for MS, the ICA is shorter in duration, does not show a learning bias, and is independent of language. {\copyright} 2020 The Author(s).},
 author = {Khaligh-Razavi, S.-M. and Sadeghi, M. and Khanbagi, M. and Kalafatis, C. and Nabavi, S. M.},
 year = {2020},
 title = {A self-administered, artificial intelligence (AI) platform for cognitive assessment in multiple sclerosis (MS)},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084786498&doi=10.1186%2fs12883-020-01736-x&partnerID=40&md5=514d570812dcb2b6bb3a7fddfca9537e},
 keywords = {Artefact Design},
 volume = {20},
 number = {1},
 journal = {BMC Neurology},
 doi = {10.1186/s12883-020-01736-x},
 file = {Khaligh-Razavi, Sadeghi et al. 2020 - A self-administered:Attachments/Khaligh-Razavi, Sadeghi et al. 2020 - A self-administered.pdf:application/pdf}
}


@article{Khan.2021,
 abstract = {Background Balochistan is the largest province of Pakistan by area, and the least developed. It suffers from several political, tribal and border conflicts. The distances to health facilities for the catchment population are long with limited accessibility. Immunization is one of the most cost-effective interventions to prevent deaths from vaccine preventable diseases (VPDs), especially in children. While Pakistan has an overall routine immunization coverage of 66 per cent for fully immunized children (FIC), coverage in the province of Balochistan is much lower at 29 per cent. This study aimed to assess the feasibility of introducing mHealth intervention using an artificial intelligence (AI) platform based on SMS (short-message service) and Interactive Voice Response (IVRs) to remind and persuade parents to get their children vaccinated. Methods We employed a mixed study design using both quantitative and qualitative approaches. Baseline data were collected from 1,600 eligible mothers/parents within the catchment areas of 75 basic health units (BHUs) in Quetta (provincial capital of Balochistan province), and the automated platform was instituted with SMS and IVRs from EPI (Expanded Programme on Immunization) Quetta. Daily reminders and IVRs were sent to the cell numbers in the database/records. Responses were noted on the AI platform. After a period of about two months, an end line survey of 1,203 participants was undertaken with a loss of 397 (25{\%}) participants due to seasonal migration to warmer cities in Sindh province. For the qualitative part, three key informant interviews (KII) were conducted; two KIIs with Lady Health Supervisors and one KII with the WHO staff responsible for routine immunization. There were also three focus group discussions (FGDs). Results We found that the intervention was feasible as well as acceptable both at the community and programme management levels. The baseline indicators of immunization coverage improved significantly on end line survey (95{\%} confidence interval, CI = 0.208-0.269; P{\textless}0.001). Conclusions Our study demonstrates the potential for mHealth and AI to improve childhood immunization and addresses equity in the least developed areas of this country. The replication of the strategy in subnational immunization programmes could decrease morbidity and mortality due to VPDs. {\copyright} 2021, International Society of Global Health. All rights reserved.},
 author = {Khan, E. A. and Panezai, M. I. and Shahid, B. and Shahabuddin, A. and Akber, S.},
 year = {2021},
 title = {Increasing the demand for vaccination through mhealth in quetta city, balochistan in pakistan},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149784210&doi=10.29392%2f001c.28999&partnerID=40&md5=9f11011fa3a809b8a4efadb3781dcede},
 keywords = {ML implementation},
 volume = {5},
 journal = {Journal of Global Health Reports},
 doi = {10.29392/001c.28999},
 file = {Khan, Panezai et al. 2021 - Increasing the demand for vaccination:Attachments/Khan, Panezai et al. 2021 - Increasing the demand for vaccination.pdf:application/pdf}
}


@article{Khan.2024,
 abstract = {Accurate identification of flow regimes is paramount in several industries, especially in chemical and hydrocarbon sectors. This paper describes a comprehensive data-driven workflow for flow regime identification. The workflow encompasses: i) the collection of dynamic pressure signals using an experimentally verified numerical two-phase flow model for three different flow regimes: stratified, slug and annular flow, ii) feature extraction from pressure signals using Discrete Wavelet Transformation (DWT), iii) Evaluation and testing of 12 different Dimensionality Reduction (DR) techniques, iv) the application of an AutoML framework for automated Machine Learning classifier selection among K-Nearest Neighbors, Artificial Neural Networks, Support Vector Machines, Gradient Boosting, Random Forest, and Logistic Regression, with hyper-parameter tuning. Kernel Fisher Discriminant Analysis (KFDA) is the best DR technique, exhibiting superior goodness of clustering, while KNN proved to be the top classifier with an accuracy of 92.5 {\%} and excellent repeatability. The combination of DWT, KFDA and KNN was used to produce a virtual flow regime map. The proposed workflow represents a significant step forward in automating flow regime identification and enhancing the interpretability of ML classifiers, allowing its application to opaque pipes fitted with pressure sensors for achieving flow assurance and automatic monitoring of two-phase flow in various process industries. {\copyright} 2024 Elsevier Ltd},
 author = {Khan, U. and Pao, W. and Pilario, K. E. and Sallih, N.},
 year = {2024},
 title = {Flow regime classification using various dimensionality reduction methods and AutoML},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187186348&doi=10.1016%2fj.enganabound.2024.03.006&partnerID=40&md5=509d02099f4eb0e04107c3db4bf78697},
 keywords = {ML implementation},
 pages = {161--174},
 volume = {163},
 journal = {Engineering Analysis with Boundary Elements},
 doi = {10.1016/j.enganabound.2024.03.006}
}


@article{Kibriya.2023,
 abstract = {One of the most severe types of cancer caused by the uncontrollable proliferation of brain cells inside the skull is brain tumors. Hence, a fast and accurate tumor detection method is critical for the patient's health. Many automated artificial intelligence (AI) methods have recently been developed to diagnose tumors. These approaches, however, result in poor performance; hence, there is a need for an efficient technique to perform precise diagnoses. This paper suggests a novel approach for brain tumor detection via an ensemble of deep and hand-crafted feature vectors (FV). The novel FV is an ensemble of hand-crafted features based on the GLCM (gray level co-occurrence matrix) and in-depth features based on VGG16. The novel FV contains robust features compared to independent vectors, which improve the suggested method's discriminating capabilities. The proposed FV is then classified using SVM or support vector machines and the k-nearest neighbor classifier (KNN). The framework achieved the highest accuracy of 99{\%} on the ensemble FV. The results indicate the reliability and efficacy of the proposed methodology; hence, radiologists can use it to detect brain tumors through MRI (magnetic resonance imaging). The results show the robustness of the proposed method and can be deployed in the real environment to detect brain tumors from MRI images accurately. In addition, the performance of our model was validated via cross-tabulated data. {\copyright} 2023 by the authors.},
 author = {Kibriya, H. and Amin, R. and Kim, J. and Nawaz, M. and Gantassi, R.},
 year = {2023},
 title = {A Novel Approach for Brain Tumor Classification Using an Ensemble of Deep and Hand-Crafted Features},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160414940&doi=10.3390%2fs23104693&partnerID=40&md5=6669404954cfc690ebfbe672c528b296},
 keywords = {ML implementation},
 volume = {23},
 number = {10},
 journal = {Sensors},
 doi = {10.3390/s23104693},
 file = {Kibriya, Amin et al. 2023 - A Novel Approach for Brain:Attachments/Kibriya, Amin et al. 2023 - A Novel Approach for Brain.pdf:application/pdf}
}


@article{Kim.2022,
 abstract = {Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to adapt the given neural architecture to improve performance while maintaining computational costs. In the architecture adaptation task, we can utilize the known high-performance architectures, and the architecture adaptation results of NAT showed performance improvements on various architectures in their experiments. However, we verified that NAT lacks reproducibility through multiple trials of experiments. Moreover, it requires an additional architecture adaptation process before network weight training. In this paper, we propose proxyless neural architecture adaptation that is reproducible and efficient. The proposed method doesn't need a proxy task for architecture adaptation. It directly improves the architecture during the conventional training process, and we can directly use the trained neural network. Moreover, the proposed method can be applied to both supervised learning and self-supervised learning. The proposed method shows stable performance improvements on various architectures and various datasets. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to various models and datasets.  {\copyright} 2013 IEEE.},
 author = {Kim, D.-G. and Lee, H.-C.},
 year = {2022},
 title = {Proxyless Neural Architecture Adaptation at Once},
 keywords = {Artefact Design},
 pages = {99745--99753},
 volume = {10},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2022.3206765},
 file = {Kim, Lee 2022 - Proxyless Neural Architecture Adaptation:Attachments/Kim, Lee 2022 - Proxyless Neural Architecture Adaptation.pdf:application/pdf}
}


@article{Korot.2021,
 abstract = {Deep learning may transform health care, but model development has largely been dependent on availability of advanced technical expertise. Herein we present the development of a deep learning model by clinicians without coding, which predicts reported sex from retinal fundus photographs. A model was trained on 84,743 retinal fundus photos from the UK Biobank dataset. External validation was performed on 252 fundus photos from a tertiary ophthalmic referral center. For internal validation, the area under the receiver operating characteristic curve (AUROC) of the code free deep learning (CFDL) model was 0.93. Sensitivity, specificity, positive predictive value (PPV) and accuracy (ACC) were 88.8{\%}, 83.6{\%}, 87.3{\%} and 86.5{\%}, and for external validation were 83.9{\%}, 72.2{\%}, 78.2{\%} and 78.6{\%} respectively. Clinicians are currently unaware of distinct retinal feature variations between males and females, highlighting the importance of model explainability for this task. The model performed significantly worse when foveal pathology was present in the external validation dataset, ACC: 69.4{\%}, compared to 85.4{\%} in healthy eyes, suggesting the fovea is a salient region for model performance OR (95{\%} CI): 0.36 (0.19, 0.70) p = 0.0022. Automated machine learning (AutoML) may enable clinician-driven automated discovery of novel insights and disease biomarkers. {\copyright} 2021, The Author(s).},
 author = {Korot, E. and Pontikos, N. and Liu, X. and Wagner, S. K. and Faes, L. and Huemer, J. and Balaskas, K. and Denniston, A. K. and Khawaja, A. and Keane, P. A.},
 year = {2021},
 title = {Predicting sex from retinal fundus photographs using automated deep learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105796846&doi=10.1038%2fs41598-021-89743-x&partnerID=40&md5=4ae55c310de89ee108ae1fd84e443c71},
 keywords = {ML implementation},
 volume = {11},
 number = {1},
 journal = {Scientific Reports},
 doi = {10.1038/s41598-021-89743-x},
 file = {Korot, Pontikos et al. 2021 - Predicting sex from retinal fundus:Attachments/Korot, Pontikos et al. 2021 - Predicting sex from retinal fundus.pdf:application/pdf}
}


@article{Krishna.2023,
 abstract = {Background: Aortic stenosis (AS) is a common form of valvular heart disease, present in over 12{\%} of the population age 75 years and above. Transthoracic echocardiography (TTE) is the first line of imaging in the adjudication of AS severity but is time-consuming and requires expert sonographic and interpretation capabilities to yield accurate results. Artificial intelligence (AI) technology has emerged as a useful tool to address these limitations but has not yet been applied in a fully hands-off manner to evaluate AS. Here, we correlate artificial neural network measurements of key hemodynamic AS parameters to experienced human reader assessment. Methods: Two-dimensional and Doppler echocardiographic images from patients with normal aortic valves and all degrees of AS were analyzed by an artificial neural network (Us2.ai) with no human input to measure key variables in AS assessment. Trained echocardiographers blinded to AI data performed manual measurements of these variables, and correlation analyses were performed. Results: Our cohort included 256 patients with an average age of 67.6 $\pm$ 9.5 years. Across all AS severities, AI closely matched human measurement of aortic valve peak velocity (r = 0.97, P {\textless}.001), mean pressure gradient (r = 0.94, P {\textless}.001), aortic valve area by continuity equation (r = 0.88, P {\textless}.001), stroke volume index (r = 0.79, P {\textless}.001), left ventricular outflow tract velocity-time integral (r = 0.89, P {\textless}.001), aortic valve velocity-time integral (r = 0.96, P {\textless}.001), and left ventricular outflow tract diameter (r = 0.76, P {\textless}.001). Conclusions: Artificial neural networks have the capacity to closely mimic human measurement of all relevant parameters in the adjudication of AS severity. Application of this AI technology may minimize interscan variability, improve interpretation and diagnosis of AS, and allow for precise and reproducible identification and management of patients with AS. {\copyright} 2023},
 author = {Krishna, H. and Desai, K. and Slostad, B. and Bhayani, S. and Arnold, J. H. and Ouwerkerk, W. and Hummel, Y. and Lam, C.S.P. and Ezekowitz, J. and Frost, M. and Jiang, Z. and Equilbec, C. and Twing, A. and Pellikka, P. A. and Frazin, L. and Kansal, M.},
 year = {2023},
 title = {Fully Automated Artificial Intelligence Assessment of Aortic Stenosis by Echocardiography},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163729445&doi=10.1016%2fj.echo.2023.03.008&partnerID=40&md5=66536228f451e302507752ff4d633033},
 keywords = {ML implementation},
 pages = {769--777},
 volume = {36},
 number = {7},
 journal = {Journal of the American Society of Echocardiography},
 doi = {10.1016/j.echo.2023.03.008}
}


@article{Kumar.2022,
 abstract = {Phenomics requires quantification of large volumes of image data, necessitating high throughput image processing approaches. Existing image processing pipelines for Drosophila wings, a powerful genetic model for studying the underlying genetics for a broad range of cellular and developmental processes, are limited in speed, precision, and functional versatility. To expand on the utility of the wing as a phenotypic screening system, we developed MAPPER, an automated machine learning-based pipeline that quantifies high-dimensional phenotypic signatures, with each dimension quantifying a unique morphological feature of the Drosophila wing. MAPPER magnifies the power of Drosophila phenomics by rapidly quantifying subtle phenotypic differences in sample populations. We benchmarked MAPPER's accuracy and precision in replicating manual measurements to demonstrate its widespread utility. The morphological features extracted using MAPPER reveal variable sexual dimorphism across Drosophila species and unique underlying sex-specific differences in morphogen signaling in male and female wings. Moreover, the length of the proximal-distal axis across the species and sexes shows a conserved scaling relationship with respect to the wing size. In sum, MAPPER is an open-source tool for rapid, high-dimensional analysis of large imaging datasets. These high-content phenomic capabilities enable rigorous and systematic identification of genotype-to-phenotype relationships in a broad range of screening and drug testing applications and amplify the potential power of multimodal genomic approaches. Copyright {\copyright} 2022 Kumar, Huizar, Farf{\'a}n-Pira, Brodskiy, Soundarrajan, Nahmad and Zartman.},
 author = {Kumar, N. and Huizar, F. J. and Farf{\'a}n-Pira, K. J. and Brodskiy, P. A. and Soundarrajan, D. K. and Nahmad, M. and Zartman, J. J.},
 year = {2022},
 title = {MAPPER: An Open-Source, High-Dimensional Image Analysis Pipeline Unmasks Differential Regulation of Drosophila Wing Features},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128962164&doi=10.3389%2ffgene.2022.869719&partnerID=40&md5=eb91de9c83e1443564fa922f2a4b2327},
 keywords = {ML implementation},
 volume = {13},
 journal = {Frontiers in Genetics},
 doi = {10.3389/fgene.2022.869719},
 file = {Kumar, Huizar et al. 2022 - MAPPER An Open-Source:Attachments/Kumar, Huizar et al. 2022 - MAPPER An Open-Source.pdf:application/pdf}
}


@inproceedings{Kumari.2022,
 abstract = {Digital Twin (DT) has promising impact on the life cycle management of assets in manufacturing industry. The concept of DT has become possible with digitalisation and Artificial Intelligence (AI). Data driven Machine Learning (ML) capabilities, can enhance the performance of the DT. To replicate a dynamic system, the DT should continuously receive and process incoming data in real-time. However, every time that the system receives new incoming datasets, the challenges of ML such as data preparation, feature selection, model selection and performance evaluation, slow down the development process of DT. This paper proposes a MetaAnalyser platform that automates these steps for incoming datasets in real-time. The MetaAnalyser platform through automating data preparation, feature selection, model selection and performance evaluation, is expected to increase the level of agility in the development process of DT and the efficiency of the DT during its lifecycle. The MetaAnalyser platform is demonstrated in this paper by ranking the features that affect the arrival delays in trains and ranking regression models based on their performance on the dataset. {\copyright} 2022 Elsevier B.V.. All rights reserved.},
 author = {Kumari, J. and Karim, R. and Karim, K. and Arenbro, M.},
 title = {MetaAnalyser - A Concept and Toolkit for Enablement of Digital Twin},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132201129&doi=10.1016%2fj.ifacol.2022.04.193&partnerID=40&md5=3b10ac2e9a2bfaf0b106853baa12f0fa},
 keywords = {Artefact Design},
 pages = {199--204},
 year = {2022},
 doi = {10.1016/j.ifacol.2022.04.193}
}


@article{Kutana.2022,
 abstract = {Background: Evaluation of robotic surgical skill has become increasingly important as robotic approaches to common surgeries become more widely utilized. However, evaluation of these currently lacks standardization. In this paper, we aimed to review the literature on robotic surgical skill evaluation. Methods: A review of literature on robotic surgical skill evaluation was performed and representative literature presented over the past ten years. Results: The study of reliability and validity in robotic surgical evaluation shows two main assessment categories: manual and automatic. Manual assessments have been shown to be valid but typically are time consuming and costly. Automatic evaluation and simulation are similarly valid and simpler to implement. Initial reports on evaluation of skill using artificial intelligence platforms show validity. Few data on evaluation methods of surgical skill connect directly to patient outcomes. Conclusion: As evaluation in surgery begins to incorporate robotic skills, a simultaneous shift from manual to automatic evaluation may occur given the ease of implementation of these technologies. Robotic platforms offer the unique benefit of providing more objective data streams including kinematic data which allows for precise instrument tracking in the operative field. Such data streams will likely incrementally be implemented in performance evaluations. Similarly, with advances in artificial intelligence, machine evaluation of human technical skill will likely form the next wave of surgical evaluation. {\copyright} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
 author = {Kutana, S. and Bitner, D. P. and Addison, P. and Chung, P. J. and Talamini, M. A. and Filicori, F.},
 year = {2022},
 title = {Objective assessment of robotic surgical skills: review of literature and future directions},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125368288&doi=10.1007%2fs00464-022-09134-9&partnerID=40&md5=9bcbaf5e08f2ad4b7bcd1cba4a7413d9},
 keywords = {Literature Review},
 pages = {3698--3707},
 volume = {36},
 number = {6},
 journal = {Surgical Endoscopy},
 doi = {10.1007/s00464-022-09134-9},
 file = {Kutana, Bitner et al. 2022 - Objective assessment of robotic surgical:Attachments/Kutana, Bitner et al. 2022 - Objective assessment of robotic surgical.pdf:application/pdf}
}


@proceedings{Kwasniewska.2022,
 abstract = {The rate of progress in the field of Artificial Intelligence (AI) and Machine Learning (ML) has significantly increased over the past ten years and continues to accelerate. Since then, AI has made the leap from research case studies to real production ready applications. The significance of this growth cannot be undermined as it catalyzed the very nature of computing. Conventional platforms struggle to achieve greater performance and efficiency, what causes a surging demand for innovative AI accelerators, specialized platforms and purpose-built computes. At the same time, it is required to provide solutions for assessment of ML platform performance in a reproducible and unbiased manner to be able to provide a fair comparison of different products. This is especially valid for Human System Interaction (HSI) systems that require specific data handling for low latency responses in emergency situations or to improve user experience, as well as for preserving data privacy and security by processing it locally. Taking it into account, this work presents a comprehensive guideline on preferred benchmarking criteria for evaluation of ML platforms that include both lower level analysis of ML models and system-level evaluation of the entire pipeline. In addition, we propose a Systematic Taxonomy of Embedded Platforms (STEP) that can be used by the community and customers for better selection of specific ML hardware consistent with their needs for better design of ML-based HSI solutions.  {\copyright} 2022 IEEE.},
 year = {2022},
 title = {Preferred Benchmarking Criteria for Systematic Taxonomy of Embedded Platforms (STEP) in Human System Interaction Systems},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137924904&doi=10.1109%2fHSI55341.2022.9869470&partnerID=40&md5=201942078d3a68a450acaa900e8835b8},
 keywords = {Artefact Design},
 volume = {2022-July},
 editor = {Kwasniewska, A. and Raghava, S. and Davila, C. and Sevenier, M. and Gamba, D. and Ruminski, J.},
 doi = {10.1109/HSI55341.2022.9869470}
}


@article{Lam.2019,
 abstract = {Background: ``Evidence Mapping'' is an emerging tool that is increasingly being used to systematically identify, review, organize, quantify, and summarize the literature. It can be used as an effective method for identifying well-studied topic areas relevant to a broad research question along with any important literature gaps. However, because the procedure can be significantly resource-intensive, approaches that can increase the speed and reproducibility of evidence mapping are in great demand. Methods: We propose an alternative process called ``rapid Evidence Mapping'' (rEM) to map the scientific evidence in a time-efficient manner, while still utilizing rigorous, transparent and explicit methodological approaches. To illustrate its application, we have conducted a proof-of-concept case study on the topic of low-calorie sweeteners (LCS) with respect to human dietary exposures and health outcomes. During this process, we developed and made publicly available our study protocol, established a PECO (Participants, Exposure, Comparator, and Outcomes) statement, searched the literature, screened titles and abstracts to identify potentially relevant studies, and applied semi-automated machine learning approaches to tag and categorize the included articles. We created various visualizations including bubble plots and frequency tables to map the evidence and research gaps according to comparison type, population baseline health status, outcome group, and study sample size. We compared our results with a traditional evidence mapping of the same topic published in 2016 (Wang et al., 2016). Results: We conducted an rEM of LCS, for which we identified 8122 records from a PubMed search (January 1, 1946--May 1, 2014) and then utilized machine learning (SWIFT-Active Screener) to prioritize relevant records. After screening 2267 (28{\%}) of the total set of titles and abstracts to achieve 95{\%} estimated recall, we ultimately included 297 relevant studies. Overall, our findings corroborated those of Wang et al. (2016) and identified that most studies were acute or short-term in healthy individuals, and studied the outcomes of appetite, energy sensing and body weight. We also identified a lack of studies assessing appetite and dietary intake related outcomes in people with diabetes. The rEM approach required approximately 100 person-hours conducted over 7 calendar months. Conclusion: Rapid Evidence Mapping is an expeditious approach based on rigorous methodology that can be used to quickly summarize the available body of evidence relevant to a research question, identify gaps in the literature to inform future research, and contextualize the design of a systematic review within the broader scientific literature, significantly reducing human effort while yielding results comparable to those from traditional methods. The potential time savings of this approach in comparison to the traditional evidence mapping process make it a potentially powerful tool for rapidly translating knowledge to inform science-based decision-making. {\copyright} 2018 Elsevier Ltd},
 author = {Lam, J. and Howard, B. E. and Thayer, K. and Shah, R. R.},
 year = {2019},
 title = {Low-calorie sweeteners and health outcomes: A demonstration of rapid evidence mapping (rEM)},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059175079&doi=10.1016%2fj.envint.2018.11.070&partnerID=40&md5=0167501203f5bf721af0aed6d394144d},
 keywords = {ML implementation},
 pages = {451--458},
 volume = {123},
 journal = {Environment International},
 doi = {10.1016/j.envint.2018.11.070}
}


@inproceedings{Landes.2023,
 abstract = {Given the criticality and difficulty of reproducing machine learning experiments, there have been significant efforts in reducing the variance of these results. The ability to consistently reproduce results effectively strengthens the underlying hypothesis of the work and should be regarded as important as the novel aspect of the research itself. The contribution of this work is an open source framework that has the following characteristics: a) facilitates reproducing consistent results, b) allows hot-swapping features and embeddings without further processing and re-vectorizing the dataset, c) provides a means of easily creating, training and evaluating natural language processing deep learning models with little to no code changes, and d) is freely available to the community. {\copyright} 2023 Association for Computational Linguistics.},
 author = {Landes, P. and {Di Eugenio}, B. and Caragea, C.},
 title = {DeepZensols: A Deep Learning Natural Language Processing Framework for Experimentation and Reproducibility},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184996327&partnerID=40&md5=b14bad02fdea2849047e8171f173f07f},
 keywords = {Artefact Design},
 pages = {141--146},
 year = {2023}
}


@article{Laufer.2023,
 abstract = {Large-area processing remains a key challenge for perovskite solar cells (PSCs). Advanced understanding and improved reproducibility of scalable fabrication processes are required to unlock the technology's economic potential. In this regard, machine learning (ML) methods have emerged as a promising tool to accelerate research and unlock the control needed to produce large-area solution-processed perovskite thin films. However, a suitable dataset allowing the analysis of a scalable fabrication process is currently missing. Herein, a unique labeled in situ photoluminescence (PL) dataset for blade-coated PSCs is introduced and explored with unsupervised k-means clustering, demonstrating the feasibility to derive meaningful insights from such data. Correlations between the obtained clusters and the measured performance of PSC reveal that the in situ PL signal encodes information about the perovskite thin-film quality. Detrimental mechanisms during thin-film formation are detected by identifying spatial differences in PL patterns and, consequently, of device performance. In addition, k-nearest neighbors is applied to predict the performance of PSCs, motivating further investigations into ML-based in-line process monitoring of scalable PSC fabrication to detect, understand, and ultimately minimize process variations across iterations.},
 author = {Laufer, F. and Ziegler, S. and Schackmar, F. and Viteri, E. A.M. and G{\"o}tz, M. and Debus, C. and Isensee, F. and Paetzold, U. W.},
 year = {2023},
 title = {Process Insights into Perovskite Thin-Film Photovoltaics from Machine Learning with In Situ Luminescence Data},
 keywords = {Artefact Design},
 volume = {7},
 number = {7},
 issn = {2367-198X},
 journal = {SOLAR RRL},
 doi = {10.1002/solr.202201114}
}


@article{Lautenschlager.2020,
 abstract = {To assess the exposure of citizens to pollutants like NOx or particulate matter in urban areas, land use regression (LUR) models are a well established method. LUR models leverage information about environmental and anthropogenic factors such as cars, heating, or industry to predict air pollution in areas where no measurements have been made. However, existing approaches are often not globally applicable and require tedious hyper-parameter tuning to enable high quality predictions. In this work, we tackle these issues by introducing OpenLUR, an off-the-shelf approach for modeling air pollution that (i) works on a set of novel features solely extracted from the globally and openly available data source OpenStreetMap and (ii) is based on state-of-the-art machine learning featuring automated hyper-parameter tuning in order to minimize manual effort. We show that our proposed features are able to outperform their counterparts from local and closed sources, and illustrate how automated hyper parameter tuning can yield competitve results while alleviating the need for expert knowledge in machine learning and manual effort. Importantly, we further demonstrate the potential of the global availability of our features by applying cross-learning across different cities in order to reduce the need for a large amount of training samples. Overall, OpenLUR represents an off-the-shelf approach that facilitates easily reproducible experiments and the development of globally applicable models. {\copyright} 2020 Elsevier Ltd},
 author = {Lautenschlager, F. and Becker, M. and Kobs, K. and Steininger, M. and Davidson, P. and Krause, A. and Hotho, A.},
 year = {2020},
 title = {OpenLUR: Off-the-shelf air pollution modeling with open features and machine learning},
 keywords = {Artefact Design},
 volume = {233},
 journal = {Atmospheric Environment},
 file = {Lautenschlager, Becker et al 2020 - OpenLUR Off-the-shelf air pollution modeling:Attachments/Lautenschlager, Becker et al 2020 - OpenLUR Off-the-shelf air pollution modeling.pdf:application/pdf}
}


@inproceedings{Lazuka.2023,
 abstract = {As machine learning (ML) models have grown in complexity, so too have the expenses they incur when deployed in the cloud. In order to reduce the costs associated with ML serving, it is necessary to optimize the choice of cloud infrastructure used. Additionally, the chosen infrastructure must be able to deliver on the latency constraints that are typically defined for cloud services. This problem is made more challenging since today's organizations often need to work with more than one cloud provider, and each provider offers its own unique set of interfaces and infrastructure choices. In this work we present xCloudServing - a novel system for consistent and automated deployment of ML inference services across multi-ple cloud providers and regions. We describe the architecture and implementation of xCloudServing, as well as the different optimization algorithms implemented internally. These include established methods from the literature, as well as Niebo - our novel algorithm for minimizing cost whilst satisfying the tail latency constraint. We present simulation results for 5 different ML models over 3 cloud providers and multiple tail latency constraints that indicate that on average, Niebo outperforms state-of-the-art algorithms by 37{\%}. Additionally, we evaluate xCloudServing with live runs and demonstrate that it is robust to nondeterministic effects and exhibits reproducible behavior. {\copyright} 2023 IEEE.},
 author = {Lazuka, M. and Anghel, A. and Ram, P. and Pozidis, H. and Parnell, T.},
 title = {xCloudServing: Automated ML Serving Across Clouds},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174286624&doi=10.1109%2fCLOUD60044.2023.00011&partnerID=40&md5=c64055580759fa7d2d5c81500171c52e},
 keywords = {Artefact Design},
 pages = {1--12},
 year = {2023},
 doi = {10.1109/CLOUD60044.2023.00011}
}


@article{Leal.2023,
 abstract = {Crowdsourced data streams are popular and extremely valuable in several domains, namely in tourism. Tourism crowdsourcing platforms rely on past tourist and business inputs to provide tailored recommendations to current users in real time. The continuous, open, dynamic and non-curated nature of the crowd-originated data demands specific stream mining techniques to support online profiling, recommendation, change detection and adaptation, explanation and evaluation. The sought techniques must, not only, continuously improve and adapt profiles and models; but must also be transparent, overcome biases, prioritize preferences, master huge data volumes and all in real time. This article surveys the state-of-art of adaptive and explainable stream recommendation, extends the taxonomy of explainable recommendations from the offline to the stream-based scenario, and identifies future research opportunities. {\copyright} 2023 John Wiley {\&} Sons Ltd.},
 author = {Leal, F. and Veloso, B. and Malheiro, B. and Burguillo, J. C.},
 year = {2023},
 title = {Towards adaptive and transparent tourism recommendations: A survey},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165387433&doi=10.1111%2fexsy.13400&partnerID=40&md5=9dee8cb62b8ec87c68b267e39a6cf65a},
 keywords = {Literature Review},
 journal = {Expert Systems},
 doi = {10.1111/exsy.13400}
}


@article{Lee.2021,
 abstract = {Objective: To evaluate the accuracy and clinical efficacy of a hybrid Greulich-Pyle (GP) and modified Tanner-Whitehouse (TW) artificial intelligence (AI) model for bone age assessment. Materials and Methods: A deep learning-based model was trained on an open dataset of multiple ethnicities. A total of 102 hand radiographs (51 male and 51 female; mean age $\pm$ standard deviation = 10.95 $\pm$ 2.37 years) from a single institution were selected for external validation. Three human experts performed bone age assessments based on the GP atlas to develop a reference standard. Two study radiologists performed bone age assessments with and without AI model assistance in two separate sessions, for which the reading time was recorded. The performance of the AI software was assessed by comparing the mean absolute difference between the AI-calculated bone age and the reference standard. The reading time was compared between reading with and without AI using a paired t test. Furthermore, the reliability between the two study radiologists' bone age assessments was assessed using intraclass correlation coefficients (ICCs), and the results were compared between reading with and without AI. Results: The bone ages assessed by the experts and the AI model were not significantly different (11.39 $\pm$ 2.74 years and 11.35 $\pm$ 2.76 years, respectively, p = 0.31). The mean absolute difference was 0.39 years (95{\%} confidence interval, 0.33-- 0.45 years) between the automated AI assessment and the reference standard. The mean reading time of the two study radiologists was reduced from 54.29 to 35.37 seconds with AI model assistance (p {\textless} 0.001). The ICC of the two study radiologists slightly increased with AI model assistance (from 0.945 to 0.990). Conclusion: The proposed AI model was accurate for assessing bone age. Furthermore, this model appeared to enhance the clinical efficacy by reducing the reading time and improving the inter-observer reliability. {\copyright} 2021 The Korean Society of Radiology.},
 author = {Lee, K.-C. and Lee, K.-H. and Kang, C. H. and Ahn, K.-S. and Chung, L. Y. and Lee, J.-J. and Hong, S. J. and Kim, B. H. and Shim, E.},
 year = {2021},
 title = {Clinical validation of a deep learning-based hybrid (Greulich-pyle and modified tanner-whitehouse) method for bone age assessment},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120327496&doi=10.3348%2fKJR.2020.1468&partnerID=40&md5=cb577c2df0e50b875c1576899b3a7b35},
 keywords = {Empirical Study},
 pages = {2017--2025},
 volume = {22},
 number = {12},
 journal = {Korean Journal of Radiology},
 doi = {10.3348/KJR.2020.1468},
 file = {Lee, Lee et al. 2021 - Clinical validation of a deep:Attachments/Lee, Lee et al. 2021 - Clinical validation of a deep.pdf:application/pdf}
}


@article{Lee.2021b,
 abstract = {We have developed a new fully automated Artificial Intelligence (AI)-based method for deriving optimal models of complex absorption systems. The AI structure is built around VPFIT, a well-developed and extensively tested nonlinear least-squares code. The new method forms a sophisticated parallelized system, eliminating human decision-making and hence bias. Here, we describe the workings of such a system and apply it to synthetic spectra, in doing so establishing recommended methodologies for future analyses of Very Large Telescope (VLT) and Extremely Large Telescope (ELT) data. One important result is that modelling line broadening for high-redshift absorption components should include both thermal and turbulent components. Failing to do so means it is easy to derive the wrong model and hence incorrect parameter estimates. One topical application of our method concerns searches for spatial or temporal variations in fundamental constants. This subject is one of the key science drivers for the European Southern Observatory's ESPRESSO spectrograph on the VLT and for the HIRES spectrograph on the ELT. The quality of new data demands completely objective and reproducible methods. The Monte Carlo aspects of the new method described here reveal that model non-uniqueness can be significant, indicating that it is unrealistic to expect to derive an unambiguous estimate of the fine structure constant \textgreek{a} from one or a very small number of measurements. No matter how optimal the modelling method, it is a fundamental requirement to use a large sample of measurements to meaningfully constrain temporal or spatial \textgreek{a} variation.  {\copyright} 2021 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.},
 author = {Lee, C.-C. and Webb, J. K. and Carswell, R. F. and Milakovi{\'c}, D.},
 year = {2021},
 title = {Artificial intelligence and quasar absorption system modelling; Application to fundamental constants at high redshift},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107749034&doi=10.1093%2fmnras%2fstab977&partnerID=40&md5=c5fcf410dc2e671fb1f5905d258f04d4},
 keywords = {Artefact Design},
 pages = {1787--1800},
 volume = {504},
 number = {2},
 journal = {Monthly Notices of the Royal Astronomical Society},
 doi = {10.1093/mnras/stab977},
 file = {Lee, Webb et al. 2021 - Artificial intelligence and quasar absorption:Attachments/Lee, Webb et al. 2021 - Artificial intelligence and quasar absorption.pdf:application/pdf}
}


@article{Leigh.2020,
 abstract = {High-resolution Ca2+ imaging to study cellular Ca2+ behaviors has led to the creation of large datasets with a profound need for standardized and accurate analysis. To analyze these datasets, spatio-temporal maps (STMaps) that allow for 2D visualization of Ca2+ signals as a function of time and space are often used. Methods of STMap analysis rely on a highly arduous process of user defined segmentation and event-based data retrieval. These methods are often time consuming, lack accuracy, and are extremely variable between users. We designed a novel automated machine-learning based plugin for the analysis of Ca2+ STMaps (STMapAuto). The plugin includes optimized tools for Ca2+ signal preprocessing, automated segmentation, and automated extraction of key Ca2+ event information such as duration, spatial spread, frequency, propagation angle, and intensity in a variety of cell types including the Interstitial cells of Cajal (ICC). The plugin is fully implemented in Fiji and able to accurately detect and expeditiously quantify Ca2+ transient parameters from ICC. The plugin's speed of analysis of large-datasets was 197-fold faster than the commonly used single pixel-line method of analysis. The automated machine-learning based plugin described dramatically reduces opportunities for user error and provides a consistent method to allow high-throughput analysis of STMap datasets. {\copyright} 2020 The Author(s)},
 author = {Leigh, W. A. and {Del Valle}, G. and Kamran, S. A. and Drumm, B. T. and Tavakkoli, A. and Sanders, K. M. and Baker, S. A.},
 year = {2020},
 title = {A high throughput machine-learning driven analysis of Ca2+ spatio-temporal maps},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089214331&doi=10.1016%2fj.ceca.2020.102260&partnerID=40&md5=83a3f3e590e8998aadb7265bcd0306be},
 keywords = {Artefact Design},
 volume = {91},
 journal = {Cell Calcium},
 doi = {10.1016/j.ceca.2020.102260},
 file = {Leigh, Del Valle et al. 2020 - A high throughput machine-learning driven:Attachments/Leigh, Del Valle et al. 2020 - A high throughput machine-learning driven.pdf:application/pdf}
}


@article{Lennon.2022,
 abstract = {Context: Qualitative research - crucial for understanding human behavior - remains underutilized, in part due to the time and cost of annotating qualitative data (coding). Artificial intelligence (AI) has been suggested as a means to reduce those burdens. Older AI techniques (Latent Semantic Indexing / Latent Dirichlet Allocation (LSI/LDA)) have fallen short, in part because qualitative data is rife with idiom, non-standard expressions, and jargon. Objective: To develop an AI platform using updated techniques to augment qualitative data coding. Study Design and Analysis: We previously completed traditional qualitative analysis of a large dataset, with 11 qualitative categories and 72 subcategories (categories), and a final Cohen's kappa $\geq$ 0.65 as a measure of human inter-coder reliability (ICR) after coding. We built our Automated Qualitative Assistant (AQUA) using a semi-classical approach, replacing LSI/LDA with a graph-theoretic topic extraction and clustering method. AQUA was given the previously-identified qualitative categories and tasked with coding free-text data into those categories. Item coding was scored using cosine-similarity. Population Studied: Pennsylvanian adults. Instrument: Free-text responses to five open ended questions related to the COVID-19 pandemic (e.g. {\textquotedbl}What worries you most about the COVID-19 pandemic?{\textquotedbl}). Outcome Measures: AQUA's coding was compared to human coding using Cohen's kappa. This was done on all categories in aggregate, and also on category clusters to identify category groups amenable to AQUA support. AQUA's time to complete coding was compared to the time taken by the human coding team. Dataset: Five unlimited free-text survey answers from 538 responders. Results: AQUA's kappa for all categories was low (kappa{\~{}}0.45), reflecting the challenge of automated analysis of diverse language. However, for several 3-category combinations (with less linguistic diversity), AQUA performed comparably to human coders, with an ICR kappa range of 0.62 to 0.72 based on test-train split. AQUA's analysis (including human interpretation) took approximately 5 hours, compared to approximately 30 person hours for traditional coding. Conclusions: AQUA enables qualitative researchers to identify categories amenable to automated coding, and to rapidly conduct that coding on the entirety of very large datasets. This saves time and money, and avoids limitations inherent in limiting qualitative analysis to limited samples of a given dataset. {\copyright} 2021 Annals of Family Medicine, Inc.},
 author = {Lennon, R. and Calo, W. and Miller, E. and Zgierska, A. and {van Scoy}, L. and Fraleigh, R.},
 year = {2022},
 title = {Using artificial intelligence to support rapid, mixed-methods analysis: Developing an automated qualitative assistant (AQUA)},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150667699&doi=10.1370%2fafm.20.s1.2893&partnerID=40&md5=d8d8d44d9db2763fed6e935e1671ba3e},
 keywords = {Artefact Design},
 number = {20},
 journal = {Annals of family medicine},
 doi = {10.1370/afm.20.s1.2893},
 file = {Lennon, Calo et al. 2022 - Using artificial intelligence to support:Attachments/Lennon, Calo et al. 2022 - Using artificial intelligence to support.pdf:application/pdf}
}


@article{Liang.2021,
 abstract = {The in-depth understanding of cellular fate decision of human preimplantation embryos has prompted investigations on how changes in lineage allocation, which is far from trivial and remains a time-consuming task by experimental methods. It is desirable to develop a novel effective bioinformatics strategy to consider transitions of coordinated embryo lineage allocation and stage-specific patterns. There are rapidly growing applications of machine learning models to interpret complex datasets for identifying candidate development-related factors and lineage-determining molecular events. Here we developed the first machine learning platform, HelPredictor, that integrates three feature selection methods, namely, principal components analysis, F-score algorithm and squared coefficient of variation, and four classical machine learning classifiers that different combinations of methods and classifiers have independent outputs by increment feature selection method. With application to single-cell sequencing data of human embryo, HelPredictor not only achieved 94.9{\%} and 90.9{\%} respectively with cross-validation and independent test, but also fast classified different embryonic lineages and their development trajectories using less HelPredictor-predicted factors. The above-mentioned candidate lineage-specific genes were discussed in detail and were clustered for exploring transitions of embryonic heterogeneity. Our tool can fast and efficiently reveal potential lineage-specific and stage-specific biomarkers and provide insights into how advanced computational tools contribute to development research. The source code is available at https://github.com/liameihao/HelPredictor. {\copyright} 2021 The Author(s). Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.},
 author = {Liang, P. and Zheng, L. and Long, C. and Yang, W. and Yang, L. and Zuo, Y.},
 year = {2021},
 title = {HelPredictor models single-cell transcriptome to predict human embryo lineage allocation},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121949199&doi=10.1093%2fbib%2fbbab196&partnerID=40&md5=74cfc534aae550699174b2be9ac4fa3d},
 keywords = {Artefact Design},
 volume = {22},
 number = {6},
 journal = {Briefings in Bioinformatics},
 doi = {10.1093/bib/bbab196},
 file = {Liang, Zheng et al. 2021 - HelPredictor models single-cell transcriptome:Attachments/Liang, Zheng et al. 2021 - HelPredictor models single-cell transcriptome.pdf:application/pdf}
}


@proceedings{Lim.2022,
 abstract = {The process of routing energy conduits (pipelines, cables and umbilicals) in offshore locations represents a critical phase in the concept planning, engineering and construction of these assets. The downstream impact of poorly designed routes is epitomized by a) increased offshore construction durations b) requirements for additional engineered mitigations from geophysical/geotechnical constraints and c) unforeseen requirements for intervention during operations. The cause of these unoptimized routes can be due to low-level engineering tasks which confines to repetitive, inefficient, and unnecessarily iterative processes between draughters, engineers and asset owners. The increasing accessibility and advancement of digital technologies enables highly optimised solutions even through difficult offshore regions. To address the above, this paper presents the scoping, development and application of a multi-functional algorithm created using modern software code frameworks. The algorithm serves as building blocks into an artificial intelligence platform. This routing algorithm simulates, expands and adapts to engineering and consulting expertise from a worldwide network of energy experts. This recreation of expertise firstly identifies commonly encountered routing constraints such as geophysical features, seabed gradients, existing offshore facilities etc. Ideal geometric parameters are then determined to minimise route costs. These processes are then increased, thus enhancing expertise through scale. The algorithm structure will be presented in summarised minimal pseudocode. The pseudocode will present the application programming interface (API) between the constraints based and end parameter calculation approach. The API includes digital innovations such as a) processing of offshore geotechnical survey data, b) recreating offshore locales and routes in a data environment, c) implementation of geospatial intersection detection, d) 3-dimensional route length optimisation and e) automated route selection criteria. This will demonstrate the order of magnitude replication of subject matter expertise into a digital realm, thus eliminating time-consuming, repetition and human error. Finally, the application of the algorithm will be demonstrated by various case studies of offshore locales with challenging conditions such as highly disturbed seabeds and large quantities of existing man-made assets. The front-end cloud platform of the algorithm will be exhibited, showing a streamlined approach and improved routing engineering. Through this, engineers in the future offshore energy developments can answer the question {\textquotedbl}What is the best route?{\textquotedbl}. Copyright {\copyright} 2022, Offshore Technology Conference.},
 year = {2022},
 title = {Pseudocode and Demonstration of a Multi-Use Artificial Intelligence Algorithm to Perform Challenging and Highly Optimised Pipeline/Cable Routing Cases},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150678151&doi=10.4043%2f31360-MS&partnerID=40&md5=8d0851b00136842943e045120a3c725a},
 keywords = {Artefact Design},
 editor = {Lim, N. and Lim, L. and Komatineni, H.},
 doi = {10.4043/31360-MS}
}


@article{Lin.2018,
 abstract = {With the global trend of aging populations, the prevalence of blindness is expected to increase. However, due to deficiencies and imbalances, medical resources are far from satisfactory for ophthalmic disease diagnosis and management, particularly in primary medical facilities of developing countries. Eyeball is a visualized organ with its superficial position and transparent refractory media. Therefore, many ophthalmic diseases can be screened and diagnosed from images and photographs, such as cataract, cornea diseases and retinopathy, etc. As the development of data mining technology and accumulation of large amount of ophthalmic clinical data, the conditions are ripe for establishment of artificial intelligence (AI) diagnostic platform. Currently, attention should be focused on integrating the abundant ocular imaging resources, practically applying the data mining technology and gradually developing the universal AI platform for the management of ophthalmic disease. Copyright {\copyright} 2018 by the Chinese Medical Association.},
 author = {Lin, H. and Wu, X.},
 year = {2018},
 title = {Accelerating the construction of artificial intelligence diagnostic platform based on ophthalmic imaging database},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054525637&doi=10.3760%2fcma.j.issn.2095-0160.2018.08.001&partnerID=40&md5=140842f046013de9053a74d2d8576e41},
 keywords = {Literature Review},
 pages = {577--580},
 volume = {36},
 number = {8},
 journal = {Chinese Journal of Experimental Ophthalmology},
 doi = {10.3760/cma.j.issn.2095-0160.2018.08.001}
}


@proceedings{Lin.2022,
 abstract = {This paper presents some experiments to validate the design of an Automated Computer Vision (AutoCV) library for applications in scientific image understanding. AutoCV attempts to define a search space of algorithms used in common image analysis workflows and then uses a fitness function to automatically select individual algorithmic workflows for a given problem. The final goal is a semi-automated system that can assist researchers in finding specific computer vision algorithms that work for their specific research questions. As an example of this method the researchers have built the SEE-Insight tool which uses genetic algorithms to search for image analysis workflows. This tool has been used to implement an image segmentation workflow (SEE-Segment) and is being updated and modified to work with other image analysis workflows such as anchor point detection and counting. This work is motivated by analogous work being done in Automated Machine Learning (AutoML). As a way to validate the approach, this paper uses the SEE-Insight tool to recreate an AutoML solution (called SEE-Classify) and compares results to an existing AutoML solution (TPOT). As expected the existing AutoML tool worked better than the prototype SEE-Classify tool. However, the goal of this work was to learn from these well-established tools and possibly identify one of them that could be modified as a mature replacement for the core SEE-Insight search algorithm. Although this drop-in replacement was not found, reproducing the AutoML experiments in the SEE-Insight framework provided quite a few insights into best practices for moving forward with this research. {\copyright} 2022 ACM.},
 year = {2022},
 title = {Validating new Automated Computer Vision workflows to traditional Automated Machine Learning},
 keywords = {Artefact Design},
 editor = {Lin, D. and Prabagaran, P. and Colbry, D.},
 doi = {10.1145/3491418.3535174},
 file = {Lin, Prabagaran et al (Hg) 2022 - Validating new Automated Computer Vision:Attachments/Lin, Prabagaran et al (Hg) 2022 - Validating new Automated Computer Vision.pdf:application/pdf}
}


@article{Lin.2022b,
 abstract = {Interactive art has been significant advanced by the Internet of Things (IoT) and cyber physical interaction technologies, which enables the participants to engage with the art devices. Several tools and platforms have been proposed to create the art devices. However, the interactive artworks are typically developed with these art devices in ad hoc approaches, and the artists need to spend significant programming efforts to integrate the art devices. This paper proposes CATtalk, a platform to create and maintain interactive artworks. The novel idea is to treat all art devices in an interactive artwork as IoT devices that can be transparently reused by reconfiguration in CATtalk. Therefore, the artworks developed independently by individual artists can be quickly integrated to create new interactive applications. Through CATtalk's no-code and low-code mechanisms, the artists can manipulate CATtalk with little or no programing efforts. CATtalk offers a built-in mechanism so that any person in the audience can play with an interactive artwork with his/her smartphone. We also conduct analytic analysis, simulation and measurements to ensure that the interactive art performance in cross-country remote stages are not affected by the communications delays. In our measurements, the average local and remote communication delays are about 0.01 and 0.05 seconds, respectively. If the art performance is designed such that the average delay between two actions of a local (remote) performer is longer than 0.1 seconds, then the probability of out-of-sequence actions is less than 0.01{\%}. That is, the local dancer should perform slower than the remote dancer. Such delay analysis for remote interactive art performance has not been conducted in the literature.},
 author = {Lin, Y. B. and Luo, H. L. and Liao, C. C.},
 year = {2022},
 title = {CATtalk: An IoT-Based Interactive Art Development Platform},
 keywords = {Artefact Design},
 pages = {127754--127769},
 volume = {10},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2022.3227093},
 file = {Lin, Luo et al. 2022 - CATtalk An IoT-Based Interactive Art:Attachments/Lin, Luo et al. 2022 - CATtalk An IoT-Based Interactive Art.pdf:application/pdf}
}


@article{Liu.2021,
 abstract = {Model selection is one of the fundamental problems in kernel-based algorithms, which is commonly done by minimizing an estimation of generalization error. The notion of stability and cross-validation (CV) error of learning machines consists of two widely used tools for analyzing the generalization performance. However, there are some disadvantages to both tools when applied for model selection: 1) the stability of learning machines is not practical due to the difficulty of the estimation of its specific value and 2) the CV-based estimate of generalization error usually has a relatively high variance, so it is prone to overfitting. To overcome these two limitations, we present a novel notion of kernel stability (KS) for deriving the generalization error bounds and variance bounds of CV and provide an effective approach to the application of KS for practical model selection. Unlike the existing notions of stability of the learning machine, KS is defined on the kernel matrix; hence, it can avoid the difficulty of the estimation of its value. We manifest the relationship between the KS and the popular uniform stability of the learning algorithm, and further propose several KS-based generalization error bounds and variance bounds of CV. By minimizing the proposed bounds, we present two novel KS-based criteria that can ensure good performance. Finally, we empirically analyze the performance of the proposed criteria on many benchmark data, which demonstrates that our KS-based criteria are sound and effective.},
 author = {Liu, Y. and Liao, S. Z. and Zhang, H. and Ren, W. Q. and Wang, W. P.},
 year = {2021},
 title = {Kernel Stability for Model Selection in Kernel-Based Algorithms},
 keywords = {Artefact Design},
 pages = {5647--5658},
 volume = {51},
 number = {12},
 issn = {2168-2275},
 journal = {IEEE TRANSACTIONS ON CYBERNETICS},
 doi = {10.1109/TCYB.2019.2923824}
}


@article{Liu.2024,
 abstract = {This teaching tip describes using Microsoft Power BI Desktop in a class to analyze unstructured data from an exit survey of prior students from a Master of Science in Management Information Systems program. Results from a short survey administered to these students showed that the students, using the no-code Power BI, were able to accomplish their text analytics tasks in a shorter period, and with less overall effort, as compared to using traditional code-rich text analytics methods. The process is described in detail as to how the students, new to unstructured data analysis, can uncover these results. This process can be replicated by other instructors teaching text analytics. {\copyright} (2023), (ISCAP- Information Systems and Computing Academic Professionals). All Rights Reserved.},
 author = {Liu, C. and Downing, C.},
 year = {2024},
 title = {Using Text Analytics AI Insights in Microsoft Power BI Desktop to Score Sentiments, Extract Key Phrases, and Discover Unstructured Data Patterns},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186865622&doi=10.62273%2fPKER1800&partnerID=40&md5=45608a57e9c0afb50ba0f9c63ebdf977},
 keywords = {Case Study},
 pages = {48--55},
 volume = {35},
 number = {1},
 journal = {Journal of Information Systems Education},
 doi = {10.62273/PKER1800},
 file = {Liu, Downing 2024 - Using Text Analytics AI Insights:Attachments/Liu, Downing 2024 - Using Text Analytics AI Insights.pdf:application/pdf}
}


@article{Lopes.2023,
 abstract = {Networks found with neural architecture search (NAS) achieve the state-of-the-art performance in a variety of tasks, out-performing human-designed networks. However, most NAS methods heavily rely on human-defined assumptions that constrain the search: architecture's outer skeletons, number of layers, parameter heuristics, and search spaces. In addition, common search spaces consist of repeatable modules (cells) instead of fully exploring the architecture's search space by designing entire architectures (macro-search). Imposing such constraints requires deep human expertise and restricts the search to predefined settings. In this article, we propose less constrained macro-neural architecture search (LCMNAS), a method that pushes NAS to less constrained search spaces by performing macro-search without relying on predefined heuristics or bounded search spaces. LCMNAS introduces three components for the NAS pipeline: 1) a method that leverages information about well-known architectures to autonomously generate complex search spaces based on weighted directed graphs (WDGs) with hidden properties; 2) an evolutionary search strategy that generates complete architectures from scratch; and 3) a mixed-performance estimation approach that combines information about architectures at the initialization stage and lower fidelity estimates to infer their trainability and capacity to model complex functions. We present experiments in 14 different datasets showing that LCMNAS is capable of generating both cell and macro-based architectures with minimal GPU computation and state-of-the-art results. Moreover, we conduct extensive studies on the importance of different NAS components in both cell and macro-based settings. The code for reproducibility is publicly available at https://github.com/VascoLopes/LCMNAS. IEEE},
 author = {Lopes, V. and Alexandre, L. A.},
 year = {2023},
 title = {Toward Less Constrained Macro-Neural Architecture Search},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181567642&doi=10.1109%2fTNNLS.2023.3326648&partnerID=40&md5=9d63ebc54425eeb5c2b87559d2ecc6c3},
 keywords = {Artefact Design},
 pages = {1--15},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 doi = {10.1109/TNNLS.2023.3326648}
}


@article{Luo.2019,
 abstract = {IMPORTANCE Deviation from normal adolescent brain development precedes manifestations of many major psychiatric symptoms. Such altered developmental trajectories in adolescents may be linked to genetic risk for psychopathology.

OBJECTIVE To identify genetic variants associated with adolescent brain structure and explore psychopathologic relevance of such associations.

DESIGN, SETTING, AND PARTICIPANTS Voxelwise genome-wide association study in a cohort of healthy adolescents aged 14 years and validation of the findings using 4 independent samples across the life span with allele-specific expression analysis of top hits. Group comparison of the identified gene-brain association among patients with schizophrenia, unaffected siblings, and healthy control individuals. This was a population-based, multicenter study combined with a clinical sample that included participants from the IMAGEN cohort, Saguenay Youth Study, Three-City Study, and Lieber Institute for Brain Development sample cohorts and UK biobank who were assessed for both brain imaging and genetic sequencing. Clinical samples included patients with schizophrenia and unaffected siblings of patients from the Lieber Institute for Brain Development study. Data were analyzed between October 2015 and April 2018.

MAIN OUTCOMES AND MEASURES Gray matter volume was assessed by neuroimaging and genetic variants were genotyped by Illumina BeadChip.

RESULTS The discovery sample included 1721 adolescents (873 girls [50.7{\%}]), with a mean (SD) age of 14.44 (0.41) years. The replication samples consisted of 8690 healthy adults (4497 women [51.8{\%}]) from 4 independent studies across the life span. A nonsynonymous genetic variant (minor T allele of rs13107325 in SLC39A8, a gene implicated in schizophrenia) was associated with greater gray matter volume of the putamen (variance explained of 4.21{\%} in the left hemisphere; 8.66; 95{\%} CI, 6.59-10.81; P = 5.35 x 10(-18); and 4.44{\%} in the right hemisphere; t = 8.90; 95{\%} CI, 6.75-11.19; P = 6.80 x 10(-19)) and also with a lower gene expression of SLC39A8 specifically in the putamen (t127 = -3.87; P = 1.70 x 10(-4)). The identified association was validated in samples across the life span but was significantly weakened in both patients with schizophrenia (z = -3.05; P = .002; n = 157) and unaffected siblings (z = -2.08; P = .04; n = 149).

CONCLUSIONS AND RELEVANCE Our results show that a missense mutation in gene SLC39A8 is associated with larger gray matter volume in the putamen and that this association is significantly weakened in schizophrenia. These results may suggest a role for aberrant ion transport in the etiology of psychosis and provide a target for preemptive developmental interventions aimed at restoring the functional effect of this mutation.},
 author = {Luo, Q. and Chen, Q. and Wang, W. J. and Desrivi{\`e}res, S. and Quinlan, E. B. and Jia, T. Y. and Macare, C. and Robert, G. H. and Cui, J. and Guedj, M. and Palaniyappan, L. and Kherif, F. and Banaschewski, T. and Bokde, A. L.W. and B{\"u}chel, C. and Flor, H. and Frouin, V. and Garavan, H. and Gowland, P. and Heinz, A. and Ittermann, B. and Martinot, J. L. and Artiges, E. and Paill{\`e}re-Martinot, M. L. and Nees, F. and Orfanos, D. P. and Poustka, L. and Fr{\"o}hner, J. H. and Smolka, M. N. and Walter, H. and Whelan, R. and Callicott, J. H. and Mattay, V. S. and Pausova, Z. and Dartigues, J. F. and Tzourio, C. and Crivello, F. and Berman, K. F. and Li, F. and Paus, T. and Weinberger and Murray, R. M. and Schumann, G. and Feng, J. F. and Barker, G. and Bromberg, U. and Millenet, S. and Lemaitre, H. and {IMAGEN Consortium}},
 year = {2019},
 title = {Association of a Schizophrenia-Risk Nonsynonymous Variant With Putamen Volume in Adolescents A Voxelwise and Genome-Wide Association Study},
 keywords = {Empirical Study},
 pages = {435--445},
 volume = {76},
 number = {4},
 issn = {2168-6238},
 journal = {JAMA PSYCHIATRY},
 doi = {10.1001/jamapsychiatry.2018.4126}
}


@inproceedings{Macdonald.2018,
 abstract = {Experimentation using IR systems has traditionally been a procedural and laborious process. Queries must be run on an index, with any parameters of the retrieval models suitably tuned. With the advent of learning-to-rank, such experimental processes (including the appropriate folding of queries to achieve cross-fold validation) have resulted in complicated experimental designs and hence scripting. At the same time, machine learning platforms such as Scikit Learn and Apache Spark have pioneered the notion of an experimental pipeline , which naturally allows a supervised classification experiment to be expressed a series of stages, which can be learned or transformed. In this demonstration, we detail Terrier-Spark, a recent adaptation to the Terrier Information Retrieval platform which permits it to be used within the experimental pipelines of Spark. We argue that this (1) provides an agile experimental platform for information retrieval, comparable to that enjoyed by other branches of data science; (2) aids research reproducibility in information retrieval by facilitating easily-distributable notebooks containing conducted experiments; and (3) facilitates the teaching of information retrieval experiments in educational environments. {\copyright} 2018 Author.},
 author = {Macdonald, C.},
 title = {Combining terrier with apache spark to create agile experimental information retrieval pipelines},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051500638&doi=10.1145%2f3209978.3210174&partnerID=40&md5=677bf14abb8dbdbd1e578896882564f7},
 keywords = {Artefact Design},
 pages = {1309--1312},
 year = {2018},
 doi = {10.1145/3209978.3210174},
 file = {Macdonald 2018 - Combining terrier with apache spark:Attachments/Macdonald 2018 - Combining terrier with apache spark.pdf:application/pdf}
}


@article{Mahto.2022,
 abstract = {Background: Artificial Intelligence has created a huge impact in different areas of dentistry. Automated cephalometric analysis is one of the major applications of artificial intelligence in the field of orthodontics. Various automated cephalometric software have been developed which utilizes artificial intelligence and claim to be reliable. The purpose of this study was to compare the linear and angular cephalometric measurements obtained from web-based fully automated Artificial Intelligence (AI) driven platform ``WebCeph''{\texttrademark} with that from manual tracing and evaluate the validity and reliability of automated cephalometric measurements obtained from ``WebCeph''{\texttrademark}. Methods: Thirty pre-treatment lateral cephalograms of patients were randomly selected. For manual tracing, digital images of same cephalograms were printed using compatible X-ray printer. After calibration, a total of 18 landmarks was plotted and 12 measurements (8 angular and 4 linear) were obtained using standard protocols. The digital images of each cephalogram were uploaded to ``WebCeph''{\texttrademark} server. After image calibration, the automated cephalometric measurements obtained through AI digitization were downloaded for each image. Intraclass correlation coefficient (ICC) was used to determine agreement between the measurements obtained from two methods. ICC value {\textless} 0.75 was considered as poor to moderate agreement while an ICC value between 0.75 and 0.90 was considered as good agreement. Agreement was rated as excellent when ICC value {\textgreater} 0.90 was obtained. Results: All the measurements had ICC value above 0.75. A higher ICC value {\textgreater} 0.9 was obtained for seven parameters i.e. ANB, FMA, IMPA/L1 to MP (°), LL to E-line, L1 to NB (mm), L1 to NB (°), S-N to Go-Gn whereas five parameters i.e. UL to E-line, U1 to NA (mm), SNA, SNB, U1 to NA (°) showed ICC value between 0.75 and 0.90. Conclusion: A good agreement was found between the cephalometric measurements obtained from ``WebCeph''{\texttrademark} and manual tracing. {\copyright} 2022, The Author(s).},
 author = {Mahto, R. K. and Kafle, D. and Giri, A. and Luintel, S. and Karki, A.},
 year = {2022},
 title = {Evaluation of fully automated cephalometric measurements obtained from web-based artificial intelligence driven platform},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128561503&doi=10.1186%2fs12903-022-02170-w&partnerID=40&md5=47428b602a59bd6c7dcea31035275c50},
 keywords = {ML implementation},
 volume = {22},
 number = {1},
 journal = {BMC Oral Health},
 doi = {10.1186/s12903-022-02170-w},
 file = {Mahto, Kafle et al. 2022 - Evaluation of fully automated cephalometric:Attachments/Mahto, Kafle et al. 2022 - Evaluation of fully automated cephalometric.pdf:application/pdf}
}


@article{Makarious.2022,
 abstract = {Personalized medicine promises individualized disease prediction and treatment. The convergence of machine learning (ML) and available multimodal data is key moving forward. We build upon previous work to deliver multimodal predictions of Parkinson's disease (PD) risk and systematically develop a model using GenoML, an automated ML package, to make improved multi-omic predictions of PD, validated in an external cohort. We investigated top features, constructed hypothesis-free disease-relevant networks, and investigated drug--gene interactions. We performed automated ML on multimodal data from the Parkinson's progression marker initiative (PPMI). After selecting the best performing algorithm, all PPMI data was used to tune the selected model. The model was validated in the Parkinson's Disease Biomarker Program (PDBP) dataset. Our initial model showed an area under the curve (AUC) of 89.72{\%} for the diagnosis of PD. The tuned model was then tested for validation on external data (PDBP, AUC 85.03{\%}). Optimizing thresholds for classification increased the diagnosis prediction accuracy and other metrics. Finally, networks were built to identify gene communities specific to PD. Combining data modalities outperforms the single biomarker paradigm. UPSIT and PRS contributed most to the predictive power of the model, but the accuracy of these are supplemented by many smaller effect transcripts and risk SNPs. Our model is best suited to identifying large groups of individuals to monitor within a health registry or biobank to prioritize for further testing. This approach allows complex predictive models to be reproducible and accessible to the community, with the package, code, and results publicly available. {\copyright} 2022, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.},
 author = {Makarious, M. B. and Leonard, H. L. and Vitale, D. and Iwaki, H. and Sargent, L. and Dadu, A. and Violich, I. and Hutchins, E. and Saffo, D. and Bandres-Ciga, S. and Kim, J. J. and Song, Y. and Maleknia, M. and Bookman, M. and Nojopranoto, W. and Campbell, R. H. and Hashemi, S. H. and Botia, J. A. and Carter, J. F. and Craig, D. W. and {van Keuren-Jensen}, K. and Morris, H. R. and Hardy, J. A. and Blauwendraat, C. and Singleton, A. B. and Faghri, F. and Nalls, M. A.},
 year = {2022},
 title = {Multi-modality machine learning predicting Parkinson's disease},
 keywords = {ML implementation},
 volume = {8},
 number = {1},
 journal = {npj Parkinson's Disease},
 doi = {10.1038/s41531-022-00288-w},
 file = {Makarious, Leonard et al. 2022 - Multi-modality machine learning predicting Parkinson's:Attachments/Makarious, Leonard et al. 2022 - Multi-modality machine learning predicting Parkinson's.pdf:application/pdf}
}


@article{Makino.2023,
 abstract = {The Stochastic Schemata Exploiter (SSE), one of the Evolutionary Algorithms, is designed to find the optimal solution of a function. SSE extracts common schemata from individual sets with high fitness and generates individuals from the common schemata. For hyper-parameter optimization, the initialization method, the schema extraction method, and the new individual generation method, which are characteristic processes in SSE, are extended. In this paper, an SSE-based multi-objective optimization for AutoML is proposed. AutoML gives good results in terms of model accuracy. However, if only model accuracy is considered, the model may be too complex. Such complex models cannot always be allowed because of the long computation time. The proposed method maximizes the stacking model accuracy and minimizes the model complexity simultaneously. When compared with existing methods, SSE has interesting features such as fewer control parameters and faster convergence properties. The visualization method makes the optimization process transparent and helps users understand the process.},
 author = {Makino, H. and Kita, E.},
 year = {2023},
 title = {Application of a Stochastic Schemata Exploiter for Multi-Objective Hyper-parameter Optimization of Machine Learning},
 keywords = {Artefact Design},
 pages = {179--213},
 volume = {17},
 number = {2},
 issn = {1867-3236},
 journal = {REVIEW OF SOCIONETWORK STRATEGIES},
 file = {Makino, Kita 2023 - Application of a Stochastic Schemata:Attachments/Makino, Kita 2023 - Application of a Stochastic Schemata.pdf:application/pdf}
}


@article{Maroun.2024,
 abstract = {Background: In patients with bicuspid aortic valve (BAV), 4D flow MRI can quantify regions exposed to abnormal aortic hemodynamics, including high wall shear stress (WSS), a known stimulus for arterial wall dysfunction. However, the long-term multiscan reproducibility of 4D flow MRI-derived hemodynamic parameters is unknown. Purpose: To investigate the long-term stability of 4D flow MRI-derived peak velocity, WSS, and WSS-derived heatmaps in patients with BAV undergoing multiyear surveillance imaging. Study Type: Retrospective. Population: 20 BAV patients (mean age 48.4 $\pm$ 13.9 years; 14 males) with five 4D flow MRI scans, with intervals of at least 6 months between scans, and 125 controls (mean age: 50.7 $\pm$ 15.8 years; 67 males). Field Strength/Sequence: 1.5 and 3.0T, prospectively ECG and respiratory navigator-gated aortic 4D flow MRI. Assessment: Automated AI-based 4D flow analysis pipelines were used for data preprocessing, aorta 3D segmentation, and quantification of ascending aorta (AAo) peak velocity, peak systolic WSS, and heatmap-derived relative area of elevated WSS compared to WSS ranges in age and sex-matched normative control populations. Growth rate was derived from the maximum AAo diameters measured on the first and fifth MRI scans. Statistical Tests: One-way repeated measures analysis of variance. P {\textless} 0.05 indicated significance. Results: One hundred 4D flow MRI exams (five per patient) were analyzed. The mean total follow-up duration was 5.5 $\pm$ 1.1 years, and the average growth rate was 0.3 $\pm$ 0.2 mm/year. Peak velocity, peak systolic WSS, and relative area of elevated WSS did not change significantly over the follow-up period (P = 0.64, P = 0.69, and P = 0.35, respectively). The patterns and areas of elevated WSS demonstrated good reproducibility on semiquantitative assessment. Conclusion: 4D flow MRI-derived peak velocity, WSS, and WSS-derived heatmaps showed good multiyear and multiscan stability in BAV patients with low aortic growth rates. These findings underscore the reliability of these metrics in monitoring BAV patients for potential risk of dilation. Level of Evidence: 3. Technical Efficacy: Stage 1. {\copyright} 2024 International Society for Magnetic Resonance in Medicine.},
 author = {Maroun, A. and Scott, M. B. and Catania, R. and Berhane, H. and Jarvis, K. and Allen, B. D. and Barker, A. J. and Markl, M.},
 year = {2024},
 title = {Multiyear Interval Changes in Aortic Wall Shear Stress in Patients with Bicuspid Aortic Valve Assessed by 4D Flow MRI},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186907712&doi=10.1002%2fjmri.29305&partnerID=40&md5=712458dbffd2180234223c28fcef363a},
 keywords = {ML implementation},
 journal = {Journal of Magnetic Resonance Imaging},
 doi = {10.1002/jmri.29305}
}


@proceedings{Martin.2022,
 abstract = {Brain organoids are three-dimensional tissues gener-ated in vitro from pluripotent stem cells and replicating the early development of Human brain. To implement, test and compare methods to follow their growth on microscopic images, a large dataset not always available is required with a trusted ground truth when developing automated Machine Learning solutions. Recently, optimized Generative Adversarial Networks prove to generate only a similar object content but not a background specific to the real acquisition modality. In this work, a small database of brain organoid bright field images, characterized by a shot noise background, is extended using the already validated AAEGAN architecture, and specific noise or a mixture noise injected in the generator. We hypothesize this noise injection could help to generate an homogeneous and similar bright-field background. To validate or invalidate our generated images we use metric calculation, and a dimensional reduction on features on original and generated images. Our result suggest that noise injection can modulate the generated image backgrounds in order to produce a more similar content as produced in the microscopic reality. A validation of these images by biological experts could augment the original dataset and allow their analysis by Deep-based solutions.  {\copyright} 2022 IEEE.},
 year = {2022},
 title = {AAEGAN Optimization by Purposeful Noise Injection for the Generation of Bright-Field Brain Organoid Images},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133127982&doi=10.1109%2fIPTA54936.2022.9784149&partnerID=40&md5=2e5d6456db5c992cf690956e559732de},
 keywords = {ML implementation},
 editor = {Martin, C. B. and Chane, C. S. and Clouchoux, C. and Histace, A.},
 doi = {10.1109/IPTA54936.2022.9784149}
}


@proceedings{Mehta.2022,
 abstract = {The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. Recently, several new NAS benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these NAS benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do not generalize to other benchmarks. To help remedy this problem, we introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research. Our code is available at https://github.com/automl/naslib. {\copyright} 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.},
 year = {2022},
 title = {Nas-bench-suite: Nas evaluation is (now) surprisingly easy},
 keywords = {Artefact Design},
 editor = {Mehta, Y. and White, C. and Zela, A. and Krishnakumar, A. and Zabergja, G. and Moradian, S. and Safari, M. and Yu, K. and Hutter, F.},
 file = {Mehta, White et al (Hg) 2022 - NAS-BENCH-SUITE:Attachments/Mehta, White et al (Hg) 2022 - NAS-BENCH-SUITE.pdf:application/pdf}
}


@article{Midena.2023,
 abstract = {Artificial intelligence (AI) and deep learning (DL)-based systems have gained wide interest in macular disorders, including diabetic macular edema (DME). This paper aims to validate an AI algorithm for identifying and quantifying different major optical coherence tomography (OCT) biomarkers in DME eyes by comparing the algorithm to human expert manual examination. Intraretinal (IRF) and subretinal fluid (SRF) detection and volumes, external limiting-membrane (ELM) and ellipsoid zone (EZ) integrity, and hyperreflective retina foci (HRF) quantification were analyzed. Three-hundred three DME eyes were included. The mean central subfield thickness was 386.5 $\pm$ 130.2 µm. IRF was present in all eyes and confirmed by AI software. The agreement (kappa value) (95{\%} confidence interval) for SRF presence and ELM and EZ interruption were 0.831 (0.738--0.924), 0.934 (0.886--0.982), and 0.936 (0.894--0.977), respectively. The accuracy of the automatic quantification of IRF, SRF, ELM, and EZ ranged between 94.7{\%} and 95.7{\%}, while accuracy of quality parameters ranged between 99.0{\%} (OCT layer segmentation) and 100.0{\%} (fovea centering). The Intraclass Correlation Coefficient between clinical and automated HRF count was excellent (0.97). This AI algorithm provides a reliable and reproducible assessment of the most relevant OCT biomarkers in DME. It may allow clinicians to routinely identify and quantify these parameters, offering an objective way of diagnosing and following DME eyes. {\copyright} 2023 by the authors.},
 author = {Midena, E. and Toto, L. and Frizziero, L. and Covello, G. and Torresin, T. and Midena, G. and Danieli, L. and Pilotto, E. and Figus, M. and Mariotti, C. and Lupidi, M.},
 year = {2023},
 title = {Validation of an Automated Artificial Intelligence Algorithm for the Quantification of Major OCT Parameters in Diabetic Macular Edema},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151330268&doi=10.3390%2fjcm12062134&partnerID=40&md5=c98208acd7163d8490aad42197940239},
 keywords = {ML implementation},
 volume = {12},
 number = {6},
 journal = {Journal of Clinical Medicine},
 doi = {10.3390/jcm12062134},
 file = {Midena, Toto et al. 2023 - Validation of an Automated Artificial:Attachments/Midena, Toto et al. 2023 - Validation of an Automated Artificial.pdf:application/pdf}
}


@article{Miller.2020,
 abstract = {In late 2019, ASHRAE hosted the Great Energy Predictor III (GEPIII) machine learning competition on the Kaggle platform. This launch marked the third energy prediction competition from ASHRAE and the first since the mid-1990s. In this updated version, the competitors were provided with over 20 million points of training data from 2,380 energy meters collected for 1,448 buildings from 16 sources. This competition's overall objective was to find the most accurate modeling solutions for the prediction of over 41 million private and public test data points. The competition had 4,370 participants, split across 3,614 teams from 94 countries who submitted 39,403 predictions. In addition to the top five winning workflows, the competitors publicly shared 415 reproducible online machine learning workflow examples (notebooks), including over 40 additional, full solutions. This paper gives a high-level overview of the competition preparation and dataset, competitors and their discussions, machine learning workflows and models generated, winners and their submissions, discussion of lessons learned, and competition outputs and next steps. The most popular and accurate machine learning workflows used large ensembles of mostly gradient boosting tree models, such as LightGBM. Similar to the first predictor competition, preprocessing of the data sets emerged as a key differentiator.},
 author = {Miller, C. and Arjunan, P. and Kathirgamanathan, A. and Fu, C. and Roth, J. and Park, J. Y. and Balbach, C. and Gowri, K. and Nagy, Z. and Fontanini, A. D. and Haberl, J.},
 year = {2020},
 title = {The ASHRAE Great Energy Predictor III competition: Overview and results},
 keywords = {ML implementation},
 pages = {1427--1447},
 volume = {26},
 number = {10},
 issn = {2374-474X},
 journal = {SCIENCE AND TECHNOLOGY FOR THE BUILT ENVIRONMENT},
 doi = {10.1080/23744731.2020.1795514}
}


@proceedings{Miltiadou.2023,
 abstract = {Over the past couple of years, implementations of Artificial Intelligence (AI) have significantly risen in numerous platforms, tools and applications around the world, impacting a broad range of industries such as manufacturing towards Smart Factories and Industry 4.0, in general. Nevertheless, despite industrial AI being the driving force for smart factories, there is strong reluctance in its adoption by manufacturers due to the lack of transparency of the black-box AI models and trust behind the decisions taken, as well as the awareness of where and how it should be incorporated in their processes and products. This paper introduces the Explainable AI platform of XMANAI which takes advantage of the latest AI advancements and technological breakthroughs in Explainable AI (XAI) in order to build {\textquotedbl}glass box{\textquotedbl}AI models that are explainable to a {\textquotedbl}human-in-the-loop{\textquotedbl}without the decrease of AI performance. The core of the platform consists of a catalogue of hybrid and graph AI models which are built, fine-tuned and validated either as baseline AI models that will be reusable to address any manufacturing problem or trained AI models that have been fine-tuned for solving concrete manufacturing problems in a trustful manner through value-based explanations that are easily and effectively interpreted by humans.  {\copyright} 2023 IEEE.},
 year = {2023},
 title = {A novel Explainable Artificial Intelligence and secure Artificial Intelligence asset sharing platform for the manufacturing industry},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181051306&doi=10.1109%2fICE%2fITMC58018.2023.10332346&partnerID=40&md5=12161db1510781d564e1040ff91718a6},
 keywords = {Artefact Design},
 editor = {Miltiadou, D. and Perakis, K. and Sesana, M. and Calabresi, M. and Lampathaki, F. and Biliri, E.},
 doi = {10.1109/ICE/ITMC58018.2023.10332346}
}


@article{Mircica.2022,
 abstract = {In this article, I cumulate previous research findings indicating that immersive technologies, automated machine learning, data visualization tools, and location analytics can assist in retaining repeat shoppers, influencing consumer patterns and driving user engagement in virtual retail stores. I contribute to the literature on the decentralized metaverse by showing that data sharing technologies and visual analytics can optimize operations and livestream video shopping experiences in retail and business locations as regards digital ownership in the blockchain-based virtual economy. Throughout March 2022, I performed a quantitative literature review of the Web of Science, Scopus, and ProQuest databases, with search terms including ``metaverse'' + ``immersive digital content,'' ``engaging digital content,'' ``data visualization tools,'' and ``location analytics.'' As I inspected research published between 2021 and 2022, only 76 articles satisfied the eligibility criteria. By eliminating controversial findings, outcomes unsubstantiated by replication, too imprecise material, or having similar titles, I decided upon 14, generally empirical, sources. Data visualization tools: Dimensions (bibliometric mapping) and VOSviewer (layout algorithms). Reporting quality assessment tool: PRISMA. Methodological quality assessment tools include: AMSTAR, Distiller SR, MMAT, and ROBIS. {\copyright} 2022, Addleton Academic Publishers. All rights reserved.},
 author = {Mircică, N.},
 year = {2022},
 title = {Immersive and Engaging Digital Content, Data Visualization Tools, and Location Analytics in a Decentralized Metaverse},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135621164&doi=10.22381%2flpi2120226&partnerID=40&md5=3070c827d8ff36247314b08c3e258aee},
 keywords = {Literature Review},
 pages = {89--104},
 volume = {21},
 journal = {Linguistic and Philosophical Investigations},
 doi = {10.22381/lpi2120226}
}


@article{Mirheidari.2024,
 abstract = {Research into clinical applications of speech-based emotion recognition (SER) technologies has been steadily increasing over the past few years. One such potential application is the automatic recognition of expressed emotion (EE) components within family environments. The identification of EE is highly important as they have been linked with a range of adverse life events. Manual coding of these events requires time-consuming specialist training, amplifying the need for automated approaches. Herein we describe an automated machine learning approach for determining the degree of warmth, a key component of EE, from acoustic and text natural language features. Our dataset of 52 recorded interviews is taken from recordings, collected over 20 years ago, from a nationally representative birth cohort of British twin children, and was manually coded for EE by two researchers (inter-rater reliability 0.84--0.90). We demonstrate that the degree of warmth can be predicted with an F1score of 64.7{\%} despite working with audio recordings of highly variable quality. Our highly promising results suggest that machine learning may be able to assist in the coding of EE in the near future. {\copyright} 2024 Public Library of Science. All rights reserved.},
 author = {Mirheidari, B. and Bittar, A. and Cummins, N. and Downs, J. and Fisher, H. L. and Christensen, H.},
 year = {2024},
 title = {Automatic detection of expressed emotion from Five-Minute Speech Samples: Challenges and opportunities},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188320747&doi=10.1371%2fjournal.pone.0300518&partnerID=40&md5=b2a7cbacd9e44797674445a238adc041},
 keywords = {ML implementation},
 volume = {19},
 number = {3 March},
 journal = {PLoS ONE},
 doi = {10.1371/journal.pone.0300518},
 file = {Mirheidari, Bittar et al. 2024 - Automatic detection of expressed emotion:Attachments/Mirheidari, Bittar et al. 2024 - Automatic detection of expressed emotion.pdf:application/pdf}
}


@proceedings{Muller.2022,
 abstract = {Digital technologies, particularly the internet, led to unprecedented opportunities to freely inform oneself, debate, and share thoughts. However, the reduced level of control through traditional gatekeepers such as journalists also led to a surge in problematic (e.g., fake news), straight-up abusive, and hateful content (e.g., hate speech). Being under ethical and often legal pressures, many operators of platforms respond to the onslaught of abusive user-generated content by introducing automated, machine learning-enabled moderation tools. Even though meant to protect online audiences, such systems have massive implications regarding free speech, algorithmic fairness, and algorithmic transparency. We set forth to present a large-scale survey experiment that aims at illuminating how the degree of transparency influences the commenter's acceptance of the machine-made decision, dependent on its outcome. With the presented study design, we seek to determine the necessary amount of transparency needed for automated comment moderation to be accepted by commenters. {\copyright} 2022 17th International Conference on Wirtschaftsinformatik, WI 2022. All rights reserved.},
 year = {2022},
 title = {Exploring Audience's Attitudes Towards Machine Learning-based Automation in Comment Moderation},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172030691&partnerID=40&md5=9c42ebbce7b33fa9fca6ad63159e6a16},
 keywords = {Experiment},
 editor = {M{\"u}ller, K. and Koelmann, H. and Niemann, M. and Plattfaut, R. and Becker, J.}
}


@proceedings{Murphy.2020,
 abstract = {In this work, we present an ongoing, three year project funded by the Advanced Research Project Agency (ARPA-E) to develop an infrastructure to accelerate the development and deployment of artificial intelligence solutions for the electric grid. The project addresses the critical issues that we have identified as hampering the deployment of AI on electric grid measurements. These issues have stymied the potential for insight from high resolution grid measurements and generally hindered artificial intelligence innovation in the utility industry. The project consists of three main components. The first is the deployment of a diverse set of grid sensors to capture a variety of grid behaviour, both from the field and in simulation. The second is the deployment of a highly performant, scalable, cloud-based data management and AI platform designed for time series data to enable the easy storage, processing and analysis of grid sensor data. The third is the cultivation of an open research community of experts around the platform and data through useful educational material, code and data sharing, and data science competitions. Overall, the project will accelerate the development of analytics, machine learning, and AI by addressing existing gaps in data, tools, and people, with the aim of improving the electric grid. {\copyright} 2020 IEEE.},
 year = {2020},
 title = {Accelerating Artificial Intelligence on the Grid},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089153759&doi=10.1109%2fPSC50246.2020.9131317&partnerID=40&md5=5157b9f1d3c4c35a0b6644bbc8e3adff},
 keywords = {ML implementation},
 editor = {Murphy, S. and Jones, K. and Laughner, T. and Bariya, M. and von Meier, A.},
 doi = {10.1109/PSC50246.2020.9131317}
}


@inproceedings{Mushtaq.2018,
 abstract = {Design of a low-complexity optimal decoder or quaternion orthogonal designs (QODs) has been an open research problem. In his paper, we identify the main issue wih the conventional maximum-likelihood (ML) decoder based on quaternion-norm that obstructs the existence of decoupled decoding at the receiver. An optimal decoder which has both characteristics, i.e., linear and decoupled, is found using transmit precoding to realize an effective channel at the receiver that simplifies the criterion and ultimately leads to a low-complexity decoder for all QODs. The proposed solution is then successfully appled to a wide range of quaternion designs where the conventional ML decoder fails to provide decoupled decoding. Simulaton resuls of these designs validate the optimaly of the proposed generalized decoder. Overall, the bit error rate (BER) curves exhibit the trend of diversity-multiplexing tradeoff, i.e., the coded matrices of low code rate outperform the ones of higher code rate in terms of diversity. {\copyright} 2018 VDE VERLAG GMBH.},
 author = {Mushtaq, E. and Ali, S. and Hassan, S. A.},
 title = {Efficient quaternion-based fast-decodable space time codes},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050036300&partnerID=40&md5=b02336d0d10cb3be1e5fbb0b2b6cea54},
 keywords = {Artefact Design},
 pages = {231--236},
 year = {2018}
}


@article{Narang.2019,
 abstract = {Aims: Studies have demonstrated the ability of a new automated algorithm for volumetric analysis of 3D echocardiographic (3DE) datasets to provide accurate and reproducible measurements of left ventricular and left atrial (LV, LA) volumes at end-systole and end-diastole. Recently, this methodology was expanded using a machine learning (ML) approach to automatically measure chamber volumes throughout the cardiac cycle, resulting in LV and LA volume-time curves. We aimed to validate ejection and filling parameters obtained from these curves by comparing them to independent well-validated reference techniques. Methods and results: We studied 20 patients referred for cardiac magnetic resonance (CMR) examinations, who underwent 3DE imaging the same day. Volume-time curves were obtained for both LV and LA chambers using the ML algorithm (Philips HeartModel), and independently conventional 3DE volumetric analysis (TomTec), and CMR images (slice-by-slice, frame-by-frame manual tracing). Automatically derived LV and LA volumes and ejection/filling parameters were compared against both reference techniques. Minor manual correction of the automatically detected LV and LA borders was needed in 4/20 and 5/20 cases, respectively. Time required to generate volume-time curves was 35 $\pm$ 17 s using ML algorithm, 3.6 $\pm$ 0.9 min using conventional 3DE analysis, and 96 $\pm$ 14 min using CMR. Volume-time curves obtained by all three techniques were similar in shape and magnitude. In both comparisons, ejection/filling parameters showed no significant inter-technique differences. Bland-Altman analysis confirmed small biases, despite wide limits of agreement. Conclusion: The automated ML algorithm can quickly measure dynamic LV and LA volumes and accurately analyse ejection/filling parameters. Incorporation of this algorithm into the clinical workflow may increase the utilization of 3DE imaging. {\copyright} The Author(s) 2018. Published on behalf of the European Society of Cardiology. All rights reserved.},
 author = {Narang, A. and Mor-Avi, V. and Prado, A. and Volpato, V. and Prater, D. and Tamborini, G. and Fusini, L. and Pepi, M. and Goyal, N. and Addetia, K. and Gon{\c{c}}alves, A. and Patel, A. R. and Lang, R. M.},
 year = {2019},
 title = {Machine learning based automated dynamic quantification of left heart chamber volumes},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063969798&doi=10.1093%2fehjci%2fjey137&partnerID=40&md5=2b2092274c3a940dff7d87b6eace4189},
 keywords = {ML implementation},
 pages = {541--549},
 volume = {20},
 number = {5},
 journal = {European Heart Journal Cardiovascular Imaging},
 doi = {10.1093/ehjci/jey137},
 file = {Narang, Mor-Avi et al. 2019 - Machine learning based automated dynamic:Attachments/Narang, Mor-Avi et al. 2019 - Machine learning based automated dynamic.pdf:application/pdf}
}


@incollection{Narayan.2020,
 abstract = {We present Ultron-AutoML, an open-source, distributed framework for efficient hyper-parameter optimization (HPO) of ML models. Considering that hyper-parameter optimization is compute intensive and time-consuming, the framework has been designed for reliability - the ability to successfully complete an HPO Job in a multi-tenant, failure prone environment, as well as efficiency - completing the job with minimum compute cost and wall-clock time. From a user's perspective, the framework emphasizes ease of use and customizability. The user can declaratively specify and execute an HPO Job, while ancillary tasks - containerizing and running the user's scripts, model checkpointing, monitoring progress, parallelization - are handled by the framework. At the same time, the user has complete flexibility in composing the code-base for specifying the ML model training algorithm as well as, optionally, any custom HPO algorithm. The framework supports the creation of data-pipelines to stream batches of shuffled and augmented data from a distributed file system. This comes in handy for training Deep Learning models based on self-supervised, semi-supervised or representation learning algorithms over large training datasets. We demonstrate the framework's reliability and efficiency by running a BERT pre-training job over a large training corpus using pre-emptible GPU compute targets. Despite the inherent unreliability of the underlying compute nodes, the framework is able to complete such long running jobs at 30{\%} of the cost with a marginal increase in wall-clock time. The framework also comes with a service to monitor jobs and ensures reproducibility of any result. {\copyright} 2020 IEEE.},
 author = {Narayan, S. and Krishna, C. S. and Mishra, V. and Rai, A. and Rai, H. and Bharti, C. and Sodhi, G. S. and Gupta, A. and Singh, N.},
 title = {Ultron-AutoML: An open-source, distributed, scalable framework for efficient hyper-parameter optimization},
 keywords = {Artefact Design},
 pages = {1584--1593},
 booktitle = {2020 IEEE International Conference on Big Data (Big Data)},
 doi = {10.1109/BigData50022.2020.9378071},
 file = {Narayan, Krishna et al 2020 - Ultron-AutoML:Attachments/Narayan, Krishna et al 2020 - Ultron-AutoML.pdf:application/pdf}
}


@article{Nath.2024,
 abstract = {Tropospheric ozone is an air pollutant at the ground level and a greenhouse gas which significantly contributes to the global warming. Strong anthropogenic emissions in and around urban environments enhance surface ozone pollution impacting the human health and vegetation adversely. However, observations are often scarce and the factors driving ozone variability remain uncertain in the developing regions of the world. In this regard, here, we conducted machine learning (ML) simulations of ozone variability and comprehensively examined the governing factors over a major urban environment (Ahmedabad) in western India. Ozone precursors (NO2, NO, CO, C5H8 and CH2O) from the CAMS (Copernicus Atmosphere Monitoring Service) reanalysis and meteorological parameters from the ERA5 (European Centre for Medium-Range Weather Forecast's (ECMWF) fifth-generation reanalysis) were included as features in the ML models. Automated ML (AutoML) fitted the deep learning model optimally and simulated the daily ozone with root mean square error (RMSE) of {\~{}}2 ppbv reproducing 84--88{\%} of variability. The model performance achieved here is comparable to widely used ML models (RF---Random Forest and XGBoost---eXtreme Gradient Boosting). Explainability of the models is discussed through different schemes of feature importance, including SAGE (Shapley Additive Global importancE) and permutation importance. The leading features are found to be different from different feature importance schemes. We show that urban ozone could be simulated well (RMSE = 2.5 ppbv and R2 = 0.78) by considering first four leading features, from different schemes, which are consistent with ozone photochemistry. Our study underscores the need to conduct science-informed analysis of feature importance from multiple schemes to infer the roles of input variables in ozone variability. AutoML-based studies, exploiting potentials of long-term observations, can strongly complement the conventional chemistry-transport modelling and can also help in accurate simulation and forecast of urban ozone. {\copyright} The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.},
 author = {Nath, S. J. and Girach, I. A. and Harithasree, S. and Bhuyan, K. and Ojha, N. and Kumar, M.},
 year = {2024},
 title = {Urban ozone variability using automated machine learning: inference from different feature importance schemes},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188470227&doi=10.1007%2fs10661-024-12549-7&partnerID=40&md5=56fd8c94d5acdf38e6b63d92ff16246d},
 keywords = {ML implementation},
 volume = {196},
 number = {4},
 journal = {Environmental Monitoring and Assessment},
 doi = {10.1007/s10661-024-12549-7},
 file = {Nath, Girach et al. 2024 - Urban ozone variability using automated:Attachments/Nath, Girach et al. 2024 - Urban ozone variability using automated.pdf:application/pdf}
}


@article{Nesbit.2021,
 abstract = {Background: An increase in blood brain barrier permeability commonly precedes neuro-inflammation and cognitive impairment in models of dementia. Common methods to estimate capillary permeability have potential confounders, or require laborious and subjective semi-manual analysis. New method: Here we used snap frozen mouse and rat brain sections that were double-immunofluorescent labeled for immunoglobulin G (IgG; plasma protein) and laminin-\textgreek{a}4 (capillary basement membrane). A Machine Learning Image Analysis program (Zeiss ZEN Intellisis) was trained to recognize and segment laminin-\textgreek{a}4 to equivocally identify blood vessels in large sets of images. An IgG subclass based on a threshold intensity was segmented and quantitated only in extravascular regions. The residual parenchymal IgG fluorescence is indicative of blood-to-brain extravasation of IgG and was accurately quantitated. Results: Automated machine-learning and threshold based segmentation of only parenchymal IgG extravasation accentuates otherwise indistinct capillary permeability, particularly frequent in minor BBB leakage. Comparison with Existing Methods: Large datasets can be processed and analyzed quickly and robustly to provide an overview of vascular permeability throughout the brain. All human bias or ambiguity involved in classifying and measuring leakage is removed. Conclusion: Here we describe a fast and precise method of visualizing and quantitating BBB permeability in mouse and rat brain tissue, while avoiding the confounding influence of unphysiological conditions such as perfusion and eliminating any human related bias from analysis. {\copyright} Copyright {\copyright} 2021 Nesbit, Mamo, Majimbi, Lam and Takechi.},
 author = {Nesbit, M. and Mamo, J. C. and Majimbi, M. and Lam, V. and Takechi, R.},
 year = {2021},
 title = {Automated Quantitative Analysis of ex vivo Blood-Brain Barrier Permeability Using Intellesis Machine-Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105252340&doi=10.3389%2ffnins.2021.617221&partnerID=40&md5=2e6f881486cf17c25b6504d17530743c},
 keywords = {ML implementation},
 volume = {15},
 journal = {Frontiers in Neuroscience},
 doi = {10.3389/fnins.2021.617221},
 file = {Nesbit, Mamo et al. 2021 - Automated Quantitative Analysis of ex:Attachments/Nesbit, Mamo et al. 2021 - Automated Quantitative Analysis of ex.pdf:application/pdf}
}


@article{ODonncha.2022,
 abstract = {This paper presents a novel spatio-temporal LSTM (SPATIAL) architecture for time series forecasting applied to environmental datasets. The framework was applied for three different ocean datasets: current speed, temperature, and dissolved oxygen. Network implementation proceeded in two directions that are nominally separated but connected as part of a natural environmental system -- across the spatial (between individual sensors) and temporal dimensions of the sensor data. Data from twenty ocean sensors were used to train the model. Results were compared against four baseline models: two machine learning algorithms generated by robust autoML frameworks, and two deep neural networks based on CNN and LSTM, respectively. Results demonstrated ability to accurately replicate complex signals and provide comparable performance to state-of-the-art benchmarks. Learning from multiple sensors simultaneously increased robustness to missing data. This paper addresses two fundamental challenges related to environmental applications of machine learning: 1) data sparsity, particularly in a challenging ocean environment, and 2) environmental datasets are inherently connected in the spatial and temporal directions while classical ML approaches only consider one of these at a time. Furthermore, sharing of parameters across all input steps makes SPATIAL a fast, scalable, and easily-parameterized forecasting framework. {\copyright} 2022 Elsevier B.V.},
 author = {O'Donncha, F. and Hu, Y. and Palmes, P. and Burke, M. and Filgueira, R. and Grant, J.},
 year = {2022},
 title = {A spatio-temporal LSTM model to forecast across multiple temporal and spatial scales},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130875471&doi=10.1016%2fj.ecoinf.2022.101687&partnerID=40&md5=987448d1716a5d16ad5cf65b23b84006},
 keywords = {ML implementation},
 volume = {69},
 journal = {Ecological Informatics},
 doi = {10.1016/j.ecoinf.2022.101687}
}


@article{Oliveira.2023,
 abstract = {Redshift measurement has always been a constant need in modern astronomy and cosmology. And as new surveys have been providing an immense amount of data on astronomical objects, the need to process such data automatically proves to be increasingly necessary. In this article, we use simulated data from the Dark Energy Survey, and from a pipeline originally created to classify supernovae, we developed a linear regression algorithm optimized through novel automated machine learning (AutoML) frameworks achieving an error score better than ordinary data pre-processing methods when compared with other modern algorithms (such as XGBOOST). Numerically, the photometric prediction RMSE of type Ia supernovae events was reduced from 0.16 to 0.09 and the RMSE of all supernovae types decreased from 0.20 to 0.14. Our pipeline consists of four steps: through spectroscopic data points we interpolate the light curve using Gaussian process fitting algorithm, then using a wavelet transform we extract the most important features of such curves; in sequence we reduce the dimensionality of such features through principal component analysis, and in the end we applied super learning techniques (stacked ensemble methods) through an AutoML framework dedicated to optimize the parameters of several different machine learning models, better resolving the problem. As a final check, we obtained probability distribution functions (PDFs) using Gaussian kernel density estimations through the predictions of more than 50 models trained and optimized by AutoML. Those PDFs were calculated to replicate the original curves that used SALT2 model, a model used for the simulation of the raw data itself. {\copyright} 2022 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.},
 author = {de Oliveira, F.M.F. and {dos Santos}, M. V. and Reis, R.R.R.},
 year = {2023},
 title = {Data-driven photometric redshift estimation from type Ia supernovae light curves},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159608063&doi=10.1093%2fmnras%2fstac3202&partnerID=40&md5=fca9625cd694110db85d9507daa411cd},
 keywords = {ML implementation},
 pages = {2385--2397},
 volume = {518},
 number = {2},
 journal = {Monthly Notices of the Royal Astronomical Society},
 doi = {10.1093/mnras/stac3202},
 file = {Oliveira, dos Santos et al. 2023 - Data-driven photometric redshift estimation:Attachments/Oliveira, dos Santos et al. 2023 - Data-driven photometric redshift estimation.pdf:application/pdf}
}


@article{Orfanoudaki.2017,
 abstract = {More than a third of the cellular proteome is non-cytoplasmic. Most secretory proteins use the Sec system for export and are targeted to membranes using signal peptides and mature domains. To specifically analyze bacterial mature domain features, we developed MatureP, a classifier that predicts secretory sequences through features exclusively computed from their mature domains. MatureP was trained using Just Add Data Bio, an automated machine learning tool. Mature domains are predicted efficiently with $\sim$92{\%} success, as measured by the Area Under the Receiver Operating Characteristic Curve (AUC). Predictions were validated using experimental datasets of mutated secretory proteins. The features selected by MatureP reveal prominent differences in amino acid content between secreted and cytoplasmic proteins. Amino-terminal mature domain sequences have enhanced disorder, more hydroxyl and polar residues and less hydrophobics. Cytoplasmic proteins have prominent amino-terminal hydrophobic stretches and charged regions downstream. Presumably, secretory mature domains comprise a distinct protein class. They balance properties that promote the necessary flexibility required for the maintenance of non-folded states during targeting and secretion with the ability of post-secretion folding. These findings provide novel insight in protein trafficking, sorting and folding mechanisms and may benefit protein secretion biotechnology.},
 author = {Orfanoudaki, G. and Markaki, M. and Chatzi, K. and Tsamardinos, I. and Economou, A.},
 year = {2017},
 title = {MatureP: Prediction of secreted proteins with exclusive information from their mature regions},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020748649&doi=10.1038%2fs41598-017-03557-4&partnerID=40&md5=165cce78552526c31f2a5def33a77ded},
 keywords = {ML implementation},
 volume = {7},
 number = {1},
 journal = {Scientific Reports},
 doi = {10.1038/s41598-017-03557-4},
 file = {Orfanoudaki, Markaki et al. 2017 - MatureP Prediction of secreted proteins:Attachments/Orfanoudaki, Markaki et al. 2017 - MatureP Prediction of secreted proteins.pdf:application/pdf}
}


@inproceedings{Orlenko.2018,
 abstract = {With the maturation of metabolomics science and proliferation of biobanks, clinical metabolic profiling is an increasingly opportunistic frontier for advancing translational clinical research. Automated Machine Learning (AutoML) approaches provide exciting opportunity to guide feature selection in agnostic metabolic profiling endeavors, where potentially thousands of independent data points must be evaluated. In previous research, AutoML using high-dimensional data of varying types has been demonstrably robust, outperforming traditional approaches. However, considerations for application in clinical metabolic profiling remain to be evaluated. Particularly, regarding the robustness of AutoML to identify and adjust for common clinical confounders. In this study, we present a focused case study regarding AutoML considerations for using the Tree-Based Optimization Tool (TPOT) in metabolic profiling of exposure to metformin in a biobank cohort. First, we propose a tandem rank-accuracy measure to guide agnostic feature selection and corresponding threshold determination in clinical metabolic profiling endeavors. Second, while AutoML, using default parameters, demonstrated potential to lack sensitivity to low-effect confounding clinical covariates, we demonstrated residual training and adjustment of metabolite features as an easily applicable approach to ensure AutoML adjustment for potential confounding characteristics. Finally, we present increased homocysteine with long-term exposure to metformin as a potentially novel, non-replicated metabolite association suggested by TPOT; an association not identified in parallel clinical metabolic profiling endeavors. While warranting independent replication, our tandem rank-accuracy measure suggests homocysteine to be the metabolite feature with largest effect, and corresponding priority for further translational clinical research. Residual training and adjustment for a potential confounding effect by BMI only slightly modified the suggested association. Increased homocysteine is thought to be associated with vitamin B12 deficiency -- evaluation for potential clinical relevance is suggested. While considerations for clinical metabolic profiling are recommended, including adjustment approaches for clinical confounders, AutoML presents an exciting tool to enhance clinical metabolic profiling and advance translational research endeavors. {\copyright} 2017 The Authors.},
 author = {Orlenko, A. and Moore, J. H. and Orzechowski, P. and Olson, R. S. and Cairns, J. and Caraballo, P. J. and Weinshilboum, R. M. and Wang, L. and Breitenstein, M. K.},
 title = {Considerations for automated machine learning in clinical metabolic profiling: Altered homocysteine plasma concentration associated with metformin exposure},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048503369&doi=10.1142%2f9789813235533_0042&partnerID=40&md5=201787c2fcad719859aa293d5965c200},
 keywords = {ML implementation},
 pages = {460--471},
 year = {2018},
 doi = {10.1142/9789813235533{\textunderscore }0042}
}


@article{Osipov.2024,
 abstract = {Contemporary analyses focused on a limited number of clinical and molecular biomarkers have been unable to accurately predict clinical outcomes in pancreatic ductal adenocarcinoma. Here we describe a precision medicine platform known as the Molecular Twin consisting of advanced machine-learning models and use it to analyze a dataset of 6,363 clinical and multi-omic molecular features from patients with resected pancreatic ductal adenocarcinoma to accurately predict disease survival (DS). We show that a full multi-omic model predicts DS with the highest accuracy and that plasma protein is the top single-omic predictor of DS. A parsimonious model learning only 589 multi-omic features demonstrated similar predictive performance as the full multi-omic model. Our platform enables discovery of parsimonious biomarker panels and performance assessment of outcome prediction models learning from resource-intensive panels. This approach has considerable potential to impact clinical care and democratize precision cancer medicine worldwide. {\copyright} The Author(s) 2024.},
 author = {Osipov, A. and Nikolic, O. and Gertych, A. and Parker, S. and Hendifar, A. and Singh, P. and Filippova, D. and Dagliyan, G. and Ferrone, C. R. and Zheng, L. and Moore, J. H. and Tourtellotte, W. and {van Eyk}, J. E. and Theodorescu, D.},
 year = {2024},
 title = {The Molecular Twin artificial-intelligence platform integrates multi-omic data to predict outcomes for pancreatic adenocarcinoma patients},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182851315&doi=10.1038%2fs43018-023-00697-7&partnerID=40&md5=4fca54107d7ac5c0ca1468ee77760f49},
 keywords = {ML implementation},
 pages = {299--314},
 volume = {5},
 number = {2},
 journal = {Nature Cancer},
 doi = {10.1038/s43018-023-00697-7},
 file = {Osipov, Nikolic et al. 2024 - The Molecular Twin artificial-intelligence platform:Attachments/Osipov, Nikolic et al. 2024 - The Molecular Twin artificial-intelligence platform.pdf:application/pdf}
}


@article{Papanikolaou.2023,
 abstract = {Network traffic analysis can raise privacy concerns due to its ability to reveal sensitive information about individuals and organizations. This paper proposes a privacy-preserving Block-chained AutoML Network Traffic Analyzer (BANTA). The system securely stores network traffic logs in a decentralized manner, providing transparency and security. Differential privacy algorithms protect sensitive information in the network flow logs while allowing administrators to analyze network traffic without the risk of leakages. The BANTA uses blockchain technology, where smart contracts automate the process of network traffic analysis, and a multi-signature system ensures the system's security, safety, and reliability. The proposed approach was evaluated using a real-world network traffic dataset. The results demonstrate the system's high accuracy and real-time anomaly detection capabilities, which makes it well-suited for scalable cybersecurity operations. The system's privacy protection, decentralized storage, automation, multi-signature system, and real-world effectiveness ensure that the organization's data is private, secure, and effectively protected from cyber threats, which are the most vexing issue of modern cyber-physical systems. {\copyright} 2023 by the authors.},
 author = {Papanikolaou, A. and Alevizopoulos, A. and Ilioudis, C. and Demertzis, K. and Rantos, K.},
 year = {2023},
 title = {A Blockchained AutoML Network Traffic Analyzer to Industrial Cyber Defense and Protection},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152382542&doi=10.3390%2felectronics12061484&partnerID=40&md5=4426d16fbdfdced0ffebf32f83cbb466},
 keywords = {Artefact Design;ML implementation},
 volume = {12},
 number = {6},
 journal = {Electronics (Switzerland)},
 doi = {10.3390/electronics12061484},
 file = {Papanikolaou, Alevizopoulos et al. 2023 - A Blockchained AutoML Network Traffic:Attachments/Papanikolaou, Alevizopoulos et al. 2023 - A Blockchained AutoML Network Traffic.pdf:application/pdf}
}


@article{Parente.2022,
 abstract = {The proper inspection of a cracks pattern over time is a critical diagnosis step to provide a thorough knowledge of the health state of a structure. When monitoring cracks propagating on a planar surface, adopting a single-image-based approach is a more convenient (costly and logistically) solution compared to subjective operators-based solutions. Machine learning (ML)-based monitoring solutions offer the advantage of automation in crack detection; however, complex and time-consuming training must be carried out. This study presents a simple and automated ML-based crack monitoring approach implemented in open sources software that only requires a single image for training. The effectiveness of the approach is assessed conducting work in controlled and real case study sites. For both sites, the generated outputs are significant in terms of accuracy ({\~{}}1 mm), repeatability (sub-mm) and precision (sub-pixel). The presented results highlight that the successful detection of cracks is achievable with only a straightforward ML-based training procedure conducted on only a single image of the multi-temporal sequence. Furthermore, the use of an innovative camera kit allowed exploiting automated acquisition and transmission fundamental for Internet of Things (IoTs) for structural health monitoring and to reduce user-based operations and increase safety. {\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
 author = {Parente, L. and Falvo, E. and Castagnetti, C. and Grassi, F. and Mancini, F. and Rossi, P. and Capra, A.},
 year = {2022},
 title = {Image-Based Monitoring of Cracks: Effectiveness Analysis of an Open-Source Machine Learning-Assisted Procedure},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123918139&doi=10.3390%2fjimaging8020022&partnerID=40&md5=6ed290512c8b1f387546a38bcf1f9ab0},
 keywords = {ML implementation},
 volume = {8},
 number = {2},
 journal = {Journal of Imaging},
 doi = {10.3390/jimaging8020022},
 file = {Parente, Falvo et al. 2022 - Image-Based Monitoring of Cracks:Attachments/Parente, Falvo et al. 2022 - Image-Based Monitoring of Cracks.pdf:application/pdf}
}


@inproceedings{Patil.2024,
 abstract = {Acute myeloid leukemia (AML) is a rapidly progressing disease that affects myeloid cells in blood and bone marrow. These abnormal cancerous cells called blast cells are non-functional cells that increase rapidly in bone marrow and are released into blood stream which crowd out the healthy functional cells leading to weak immune system. This life-threatening disease needs to be diagnosed at early stage and hence requires fully automated system for early detection of leukemia to aid pathologists and doctors. Most of the automated machine learning and AI models are not transparent and require techniques to explain model prediction. This paper presents methods to classify blood microscopic images into healthy or acute myeloid leukemia. Among all the methods implemented, Gradient Boosting outperforms with an accuracy of 96.67{\%}. This paper also focuses on explainable AI to interpret model prediction and feature importance which further helps in understanding decision-making process of classification model and optimize it. {\copyright} 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
 author = {Patil, A. P. and Hiremath, M. and Pawar, V.},
 title = {A Unified Approach to Predict and Understand Acute Myeloid Leukemia Diagnosis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184280202&doi=10.1007%2f978-981-99-7383-5_18&partnerID=40&md5=dbd26b286e33f0012295dd712498e7f3},
 keywords = {ML implementation},
 pages = {233--242},
 year = {2024},
 doi = {10.1007/978-981-99-7383-5{\textunderscore }18}
}


@inproceedings{Patron.2020,
 abstract = {Credit risk prediction is one of the most recurrent problems in the financial industry. While machine learning techniques such as Neural Networks can have a stunning power of prediction accuracy when done right, the results of such models are not easily interpretable and hence, are difficult to explain and to integrate into financial regulation. Building strong and robust models requires a high degree of expertise, time and testing, and as the list of the available model grows, their complexity also increases. This is why meta-heuristic search and optimization techniques are being built to tackle this task. However, this often means that such models may not be easily interpretable. This work proposes a fast, reproducible pipeline that targets these two salient needs: solid, comparable model-building and reliable interpretability. An automated machine learning process is implemented via Genetic Algorithms to obtain a locally optimal model for our data that is comparable to top Kagglers' performance for the same classification problem and then, an interpretation engine is added on top to perform sanity checks on our results and identify the most important causals of prediction. This process greatly reduces time, cost and barrier of entry for model-building while providing the reasons for prediction, which can be easily contrasted with expert knowledge to check for correctness and extracting key insights. {\copyright} 2020, Springer Nature Switzerland AG.},
 author = {Patron, G. and Leon, D. and Lopez, E. and Hernandez, G.},
 title = {An Interpretable Automated Machine Learning Credit Risk Model},
 keywords = {Artefact Design},
 pages = {16--23},
 year = {2020},
 doi = {10.1007/978-3-030-61834-6{\textunderscore }2},
 file = {Patron, Leon et al 2020 - An Interpretable Automated Machine Learning:Attachments/Patron, Leon et al 2020 - An Interpretable Automated Machine Learning.pdf:application/pdf}
}


@article{Perestrelo.2017,
 abstract = {Image-based assays, such as alkaline phosphatase staining or immunocytochemistry for pluripotent markers, are common methods used in the stem cell field to assess pluripotency. Although an increased number of image-analysis approaches have been described, there is still a lack of software availability to automatically quantify pluripotency in large images after pluripotency staining. To address this need, we developed a robust and rapid image processing software, Pluri-IQ, which allows the automatic evaluation of pluripotency in large low-magnification images. Using mouse embryonic stem cells (mESC) as a model, we combined an automated segmentation algorithm with a supervised machine-learning platform to classify colonies as pluripotent, mixed, or differentiated. In addition, Pluri-IQ allows the automatic comparison between different culture conditions. This efficient user-friendly open-source software can be easily implemented in images derived from pluripotent cells or cells that express pluripotent markers (e.g., OCT4-GFP) and can be routinely used, decreasing image assessment bias. {\copyright} 2017 The Authors},
 author = {Perestrelo, T. and Chen, W. and Correia, M. and Le, C. and Pereira, S. and Rodrigues, A. S. and Sousa, M. I. and Ramalho-Santos, J. and Wirtz, D.},
 year = {2017},
 title = {Pluri-IQ: Quantification of Embryonic Stem Cell Pluripotency through an Image-Based Analysis Software},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023606018&doi=10.1016%2fj.stemcr.2017.06.006&partnerID=40&md5=cea7c243b8a2c023e895d8dc3ab63596},
 keywords = {Artefact Design;ML implementation},
 pages = {697--709},
 volume = {9},
 number = {2},
 journal = {Stem Cell Reports},
 doi = {10.1016/j.stemcr.2017.06.006},
 file = {Perestrelo, Chen et al. 2017 - Pluri-IQ Quantification of Embryonic Stem:Attachments/Perestrelo, Chen et al. 2017 - Pluri-IQ Quantification of Embryonic Stem.pdf:application/pdf}
}


@article{Pham.2021,
 abstract = {The success of machine learning (ML) techniques implemented in different industries heavily rely on operator expertise and domain knowledge, which is used in manually choosing an algorithm and setting up the specific algorithm parameters for a problem. Due to the manual nature of model selection and parameter tuning, it is impossible to quantify or evaluate the quality of this manual process, which in turn limits the ability to perform comparison studies between different algorithms. In this study, we propose a new hybrid approach for developing machine learning workflows to help automated algorithm selection and hyperparameter optimization. The proposed approach provides a robust, reproducible, and unbiased workflow that can be quantified and validated using different scoring metrics. We have used the most common workflows implemented in the application of artificial intelligence (AI) and ML in engineering problems including grid/random search, Bayesian search and optimization, genetic programming, and compared that with our new hybrid approach that includes the integration of Tree-based Pipeline Optimization Tool (TPOT) and Bayesian optimization. The performance of each workflow is quantified using different scoring metrics such as Pearson correlation (i.e., R2 correlation) and Mean Square Error (i.e., MSE). For this purpose, actual field data obtained from 1567 gas wells in Marcellus Shale, with 121 features from reservoir, drilling, completion, stimulation, and operation is tested using different proposed workflows. A proposed new hybrid workflow is then used to evaluate the type well used for evaluation of Marcellus shale gas production. In conclusion, our automated hybrid approach showed significant improvement in comparison to other proposed workflows using both scoring matrices. The new hybrid approach provides a practical tool that supports the automated model and hyperparameter selection, which is tested using real field data that can be implemented in solving different engineering problems using artificial intelligence and machine learning. The new hybrid model is tested in a real field and compared with conventional type wells developed by field engineers. It is found that the type well of the field is very close to P50 predictions of the field, which shows great success in the completion design of the field performed by field engineers. It also shows that the field average production could have been improved by 8{\%} if shorter cluster spacing and higher proppant loading per cluster were used during the frac jobs.},
 author = {Pham, V. V. and Fathi, E. and Belyadi, F.},
 year = {2021},
 title = {New Hybrid Approach for Developing Automated Machine Learning Workflows: A Real Case Application in Evaluation of Marcellus Shale Gas Production},
 keywords = {Artefact Design;Empirical Study},
 pages = {286--303},
 volume = {2},
 number = {3},
 issn = {2673-3994},
 journal = {FUELS},
 file = {Pham, Fathi et al. 2021 - New Hybrid Approach for Developing:Attachments/Pham, Fathi et al. 2021 - New Hybrid Approach for Developing.pdf:application/pdf}
}


@proceedings{Pinto.2017,
 abstract = {Machine Learning (ML) has been successfully applied to a wide range of domains and applications. One of the techniques behind most of these successful applications is Ensemble Learning (EL), the field of ML that gave birth to methods such as Random Forests or Boosting. The complexity of applying these techniques together with the market scarcity on ML experts, has created the need for systems that enable a fast and easy drop-in replacement for ML libraries. Automated machine learning (autoML) is the field of ML that attempts to answers these needs. We propose autoBagging, an autoML system that automatically ranks 63 bagging workflows by exploiting past performance and metalearning. Results on 140 classification datasets from the OpenML platform show that autoBagging can yield better performance than the Average Rank method and achieve results that are not statistically different from an ideal model that systematically selects the best workflow for each dataset. For the purpose of reproducibility and generalizability, autoBagging is publicly available as an R package on CRAN.},
 year = {2017},
 title = {Autobagging: Learning to rank bagging workows with metalearning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037057823&partnerID=40&md5=fcef07a2a36426486775fafeea0f88bf},
 keywords = {Artefact Design},
 volume = {1998},
 editor = {Pinto, F. and Cerqueira, V. and Soares, C. and Mendes-Moreira, J.}
}


@article{Piri.2022,
 abstract = {Background: We aimed to establish and test an automated AI-based method for rapid segmentation of the aortic wall in positron emission tomography/computed tomography (PET/CT) scans. Methods: For segmentation of the wall in three sections: the arch, thoracic, and abdominal aorta, we developed a tool based on a convolutional neural network (CNN), available on the Research Consortium for Medical Image Analysis (RECOMIA) platform, capable of segmenting 100 different labels in CT images. It was tested on 18F-sodium fluoride PET/CT scans of 49 subjects (29 healthy controls and 20 angina pectoris patients) and compared to data obtained by manual segmentation. The following derived parameters were compared using Bland--Altman Limits of Agreement: segmented volume, and maximal, mean, and total standardized uptake values (SUVmax, SUVmean, SUVtotal). The repeatability of the manual method was examined in 25 randomly selected scans. Results: CNN-derived values for volume, SUVmax, and SUVtotal were all slightly, i.e., 13-17{\%}, lower than the corresponding manually obtained ones, whereas SUVmean values for the three aortic sections were virtually identical for the two methods. Manual segmentation lasted typically 1-2 hours per scan compared to about one minute with the CNN-based approach. The maximal deviation at repeat manual segmentation was 6{\%}. Conclusions: The automated CNN-based approach was much faster and provided parameters that were about 15{\%} lower than the manually obtained values, except for SUVmean values, which were comparable. AI-based segmentation of the aorta already now appears as a trustworthy and fast alternative to slow and cumbersome manual segmentation. {\copyright} 2021, American Society of Nuclear Cardiology.},
 author = {Piri, R. and Edenbrandt, L. and Larsson, M. and Enqvist, O. and N{\o}ddeskou-Fink, A. H. and Gerke, O. and H{\o}ilund-Carlsen, P. F.},
 year = {2022},
 title = {Aortic wall segmentation in 18F-sodium fluoride PET/CT scans: Head-to-head comparison of artificial intelligence-based versus manual segmentation},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105852768&doi=10.1007%2fs12350-021-02649-z&partnerID=40&md5=a64b232a87f55c7cda314309b2c94234},
 keywords = {Artefact Design;ML implementation},
 pages = {2001--2010},
 volume = {29},
 number = {4},
 journal = {Journal of Nuclear Cardiology},
 doi = {10.1007/s12350-021-02649-z},
 file = {Piri, Edenbrandt et al. 2022 - Aortic wall segmentation in 18F-sodium:Attachments/Piri, Edenbrandt et al. 2022 - Aortic wall segmentation in 18F-sodium.pdf:application/pdf}
}


@article{Portelli.2020,
 abstract = {Context: The observation of the earth by humans has advanced our understanding of the physical patterns and processes that shape the landscape. Over time, the act of scientific interpretation has transformed into one mediated through machines, creating distance between the observer and the observed. Machine learning is expanding this gap and transforming how we gain knowledge about the world. Raising the question is there something to be lost by advancing machine learning at the expense of human visual interpretation? Objectives: Recognizing the usefulness of these computational algorithms for dealing with massive, heterogeneous, and dynamic ecological datasets,~scientists should~not~abandon the important contributions of human intelligence to understanding landscape patterns, processes, and relationships. Methods: This paper presents a review of social, cultural, and political or military influences on the relationship between humans and remote sensing images of the landscape. This review highlights tensions between automated machine learning approaches and human interpretation. Results: Support for the use of human--machine integrated systems through the use of interactive, visual display, and the development of transparent machine learning methods is suggested. Conclusions: The human analyst should remain central in the design of landscape ecology applications when deploying machine learning algorithms. The complementary strengths of the human and machine in data processing suggest that the most informative insights regarding pattern and process can happen in the implementation of carefully designed Human in the Loop systems. {\copyright} 2020, Springer Nature B.V.},
 author = {Portelli, R. A.},
 year = {2020},
 title = {Don't throw the baby out with the bathwater: reappreciating the dynamic relationship between humans, machines, and landscape images},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082859757&doi=10.1007%2fs10980-020-00992-z&partnerID=40&md5=9280f1d3d444b7e6f9e5948c91ba60ed},
 keywords = {Literature Review},
 pages = {815--822},
 volume = {35},
 number = {4},
 journal = {Landscape Ecology},
 doi = {10.1007/s10980-020-00992-z},
 file = {Portelli 2020 - Don't throw the baby out:Attachments/Portelli 2020 - Don't throw the baby out.pdf:application/pdf}
}


@article{Qian.2021,
 abstract = {The coronavirus disease 2019 (COVID-19) global pandemic poses the threat of overwhelming healthcare systems with unprecedented demands for intensive care resources. Managing these demands cannot be effectively conducted~without a nationwide collective effort that relies on data to forecast hospital demands on the national, regional, hospital and individual levels. To this end, we developed the COVID-19 Capacity Planning and Analysis System (CPAS)---a machine learning-based system for hospital resource planning that we have successfully deployed at individual hospitals and across regions in the UK in coordination with NHS Digital. In this paper, we discuss the main challenges of deploying a machine learning-based decision support system at national scale, and explain how CPAS addresses these challenges by (1) defining the appropriate learning problem,~(2) combining bottom-up and top-down analytical approaches, (3) using state-of-the-art machine learning algorithms, (4) integrating heterogeneous data sources, and (5) presenting the result with an interactive and transparent interface. CPAS is one of the first machine learning-based systems to be deployed in hospitals on a national scale to address the COVID-19 pandemic---we conclude the paper with a summary of the lessons learned from this experience. {\copyright} 2020, The Author(s).},
 author = {Qian, Z. and Alaa, A. M. and {van der Schaar}, M.},
 year = {2021},
 title = {CPAS: the UK's national machine learning-based hospital capacity planning system for COVID-19},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096461146&doi=10.1007%2fs10994-020-05921-4&partnerID=40&md5=d66a7271c2e40e6fb6b8cf7d9ccbaef5},
 keywords = {Artefact Design;ML implementation},
 pages = {15--35},
 volume = {110},
 number = {1},
 journal = {Machine Learning},
 doi = {10.1007/s10994-020-05921-4},
 file = {Qian, Alaa et al. 2021 - CPAS the UK's national machine:Attachments/Qian, Alaa et al. 2021 - CPAS the UK's national machine.pdf:application/pdf}
}


@article{Rahman.2022,
 abstract = {Privacy laws and app stores (e.g., Google Play Store) require mobile apps to have transparent privacy policies to disclose sensitive actions and data collection, such as accessing the phonebook, camera, storage, GPS, and microphone. However, many mobile apps do not accurately disclose their sensitive data access that requires sensitive ('dangerous') permissions. Thus, analyzing discrepancies between apps' permissions and privacy policies facilitates the identification of compliance issues upon which privacy regulators and marketplace operators can act. This paper proposes {\textless}italic{\textgreater}PermPress{\textless}/italic{\textgreater} -- an automated machine-learning system to evaluate an Android app's permission-completeness, i.e., whether its privacy policy matches its dangerous permissions. {\textless}italic{\textgreater}PermPress{\textless}/italic{\textgreater} combines machine learning techniques with human annotation of privacy policies to establish whether app policies contain permission-relevant information. {\textless}italic{\textgreater}PermPress{\textless}/italic{\textgreater} leverages MPP-270, an annotated policy corpus, for establishing a gold standard dataset of permission completeness. This corpus shows that only 31{\%} of apps disclose all dangerous permissions in privacy policies. By leveraging the annotated dataset and machine learning techniques, {\textless}italic{\textgreater}PermPress{\textless}/italic{\textgreater} achieves an AUC score of 0.92 in predicting the permission-completeness of apps. A large-scale evaluation of 164, 156 Android apps shows that, on average, 7{\%} of apps do not disclose more than half of their declared dangerous permissions in privacy policies, whereas 60{\%} of apps omit to disclose at least one dangerous permission-related data collection in privacy policies. This paper's investigation uncovers the non-transparent state of app privacy policies and highlights the need to standardize app privacy policies' compliance and completeness checking process. Author},
 author = {Rahman, M. S. and Naghavi, P. and Kojusner, B. and Afroz, S. and Williams, B. and Rampazzi, S. and Bindschaedler, V.},
 year = {2022},
 title = {PermPress: Machine Learning-Based Pipeline to Evaluate Permissions in App Privacy Policies},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136656374&doi=10.1109%2fACCESS.2022.3199882&partnerID=40&md5=ec004f5e1bd5de7a6533e79bc0815560},
 keywords = {ML implementation},
 pages = {1},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2022.3199882},
 file = {Rahman, Naghavi et al. 2022 - PermPress Machine Learning-Based Pipeline:Attachments/Rahman, Naghavi et al. 2022 - PermPress Machine Learning-Based Pipeline.pdf:application/pdf}
}


@article{Rahman.2023,
 abstract = {Machine learning has been very promising in solving real problems, but the implementation involved difficulties mainly for the inexpert data scientists. Therefore, this paper presents an automated machine learning (AutoML) to simplify and accelerate the modeling tasks. Focused on Python and RapidMiner rapid modeling tools, Tree-based Pipeline Optimization Tool (TPOT) and AutoModel were used. This paper presents a comprehensive comparison between these tools with regard to the prediction accuracy and Area Under Curve (AUC) in classifying real cases of whistleblowing academic dishonesty among undergraduate students of two universities in Indonesia. Additionally, the correlations weight from demographic and Theory of Planned Behavior (TOB) attributes in the different machine learning models are also discussed. All the machine learning algorithms from TPOT and AutoModel are considerable powerful to generate good accuracy level (between 70--93{\%} of AUC) in classifying both cases of whistleblowing and non-whistleblowing on the hold-out samples from the testing process. Generally, based on the validation results of the prediction models, demographic attributes presented more importance than the TBP attributes. The findings of this study will be a great interest of many research scholars to conduct a more in-depth analysis on AutoML for many domains mainly in education and academic misconduct fields. • AutoML is the first of its kind to be empirically compared between TPOT and AutoModel in an application to predict academic dishonesty whistleblowing. • Besides accuracy performances of the AutoML, the proportion of the variance of each attribute from demographic and Theory of Planned Behavior (TPB) is also presented in the prediction models of academic dishonesty whistleblowing. • AutoML is a convenient and reproducible rapid modeling method of machine learning to be used in many kinds of prediction problem. {\copyright} 2023},
 author = {Rahman, R. A. and Masrom, S. and Mohamad, M. and Sari, E. N. and Saragih, F. and Rahman, A.S.A.},
 year = {2023},
 title = {Comparisons of automated machine learning (AutoML) in predicting whistleblowing of academic dishonesty with demographic and theory of planned behavior},
 keywords = {Case Study;ML implementation;Technical Review},
 volume = {11},
 journal = {MethodsX},
 doi = {10.1016/j.mex.2023.102364},
 file = {Rahman, Masrom et al. 2023 - Comparisons of automated machine learning:Attachments/Rahman, Masrom et al. 2023 - Comparisons of automated machine learning.pdf:application/pdf}
}


@inproceedings{Rajagopal.2024,
 abstract = {Artificial Intelligence (AI) and nanotechnology are promising areas for the future of humanity. While deep learning-based computer vision has found applications in many fields from medicine to automotive, its application in nanotechnology can open doors for new scientific discoveries. Can we apply AI to explore objects that our eyes can't see such as nanoscale-sized objects? An AI platform to visualize nanoscale patterns learnt by a deep learning neural network can open new frontiers for nanotechnology. The objective of this paper is to develop a deep learning-based visualization system on images of nanomaterials obtained by scanning electron microscope (SEM). This paper contributes an AI platform to enable any nanoscience researchers to use AI in the visual exploration of nanoscale morphologies of nanomaterials. This AI is developed by a technique of visualizing intermediate activations of a Convolutional AutoEncoder (CAE). In this method, a nanoscale specimen image is transformed into its feature representations by a Convolution Neural Network (CNN). The convolutional autoencoder is trained on a 100{\%} SEM dataset from NFFA-EUROPE, and then CNN visualization is applied. This AI generates various conceptual feature representations of the nanomaterial. While deep learning-based image classification of SEM images is widely published in literature, there are not many publications that have visualized deep neural networks of nanomaterials. This is significant to gain insights from the learnings extracted by machine learning. This paper unlocks the potential of applying deep learning-based visualization on electron microscopy to offer AI-extracted features and architectural patterns of various nanomaterials. This is a contribution to explainable AI in nanoscale objects, and to learn from otherwise black box neural networks. This paper contributes an open-source AI with reproducible results at URL (https://sites.google.com/view/aifornanotechnology ). {\copyright} 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
 author = {Rajagopal, A. and Nirmala, V. and Andrew, J. and Vedamanickam, A. M.},
 title = {AI Visualization in Nanoscale Microscopy},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180147147&doi=10.1007%2f978-981-99-3481-2_54&partnerID=40&md5=41d7e60ac918aea7e6c2371b99621487},
 keywords = {Artefact Design},
 pages = {707--719},
 year = {2024},
 doi = {10.1007/978-981-99-3481-2{\textunderscore }54},
 file = {Rajagopal, Nirmala et al 2024 - AI Visualization in Nanoscale Microscopy:Attachments/Rajagopal, Nirmala et al 2024 - AI Visualization in Nanoscale Microscopy.pdf:application/pdf}
}


@article{Rewcastle.2023,
 abstract = {Endometrial hyperplasia is a precursor to endometrial cancer, characterized by excessive prolifer-ation of glands that is distinguishable from normal endometrium. Current classifications define 2 types of EH, each with a different risk of progression to endometrial cancer. However, these schemes are based on visual assessments and, therefore, subjective, possibly leading to overtreatment or undertreatment. In this study, we developed an automated artificial intelligence tool (ENDOAPP) for the measurement of morphologic and cytologic features of endometrial tissue using the software Visiopharm. The ENDOAPP was used to extract features from whole-slide images of PAN-CK thorn estained formalin-fixed paraffin-embedded tissue sections from 388 patients diagnosed with endometrial hyperplasia between 1980 and 2007. Follow-up data were available for all patients (mean 1/4 140 months). The most prognostic features were identified by a logistic regression model and used to assign a low-risk or high-risk progression score. Performance of the ENDOAPP was assessed for the following variables: images from 2 different scanners (Hamamatsu XR and S60) and automated placement of a region of interest versus manual placement by an operator. Then, the performance of the application was compared with that of current classification schemes: WHO94, WHO20, and EIN, and the computerized-morphometric risk classification method: D-score. The most significant prognosticators were percentage stroma and the standard deviation of the lesser diameter of epithelial nuclei. The ENDOAPP had an acceptable discriminative power with an area under the curve of 0.765. Furthermore, strong to moderate agreement was observed between manual operators (intraclass correlation coefficient: 0.828) and scanners (intraclass correlation coefficient: 0.791). Comparison of the prognostic capability of each classification scheme revealed that the ENDOAPP had the highest accuracy of 88{\%}-91{\%} alongside the D-score method (91{\%}). The other classification schemes had an accuracy between 83{\%} and 87{\%}. This study demonstrated the use of computer-aided prognosis to classify progression risk in EH for improved patient treatment.(c) 2023 THE AUTHORS. Published by Elsevier Inc. on behalf of the United States {\&} Canadian Academy of Pathology. This is an open access article under the CC BY license (http://creativecommons.org/ licenses/by/4.0/).},
 author = {Rewcastle, E. and Gudlaugsson, E. and Lillesand, M. and Skaland, I. and Baak, J. P.A. and Janssen, E. A.M.},
 year = {2023},
 title = {Automated Prognostic Assessment of Endometrial Hyperplasia for Progression Risk Evaluation Using Artificial Intelligence},
 keywords = {Artefact Design},
 volume = {36},
 number = {5},
 issn = {1530-0285},
 journal = {MODERN PATHOLOGY},
 doi = {10.1016/j.modpat.2023.100116}
}


@article{Rouhollahi.2023,
 abstract = {Aortic stenosis (AS) is the most prevalent heart valve disease in western countries that poses a significant public health challenge due to the lack of a medical treatment to prevent valve calcification. Given the aging population demographic, the prevalence of AS is projected to rise, resulting in a progressively significant healthcare and economic burden. While surgical aortic valve replacement (SAVR) has been the gold standard approach, the less invasive transcatheter aortic valve replacement (TAVR) is poised to become the dominant method for high- and medium-risk interventions. Computational simulations using patient-specific models, have opened new research avenues for optimizing emerging devices and predicting clinical outcomes. The traditional techniques of generating digital replicas of patients' aortic root, native valve, and calcification are time-consuming and labor-intensive processes requiring specialized tools and expertise in anatomy. Alternatively, deep learning models, such as the U-Net architecture, have emerged as reliable and fully automated methods for medical image segmentation. Two-dimensional U-Nets have been shown to produce comparable or more accurate results than trained clinicians' manual segmentation while significantly reducing computational costs. In this study, we have developed a fully automatic AI tool capable of reconstructing the digital twin geometry and analyzing the calcification distribution on the aortic valve. The developed automatic segmentation package enables the modeling of patient-specific anatomies, which can then be used to simulate virtual interventional procedures, optimize emerging prosthetic devices, and predict clinical outcomes. {\copyright} 2023 Elsevier Ltd},
 author = {Rouhollahi, A. and Willi, J. N. and Haltmeier, S. and Mehrtash, A. and Straughan, R. and Javadikasgari, H. and Brown, J. and Itoh, A. and de {La Cruz}, K. I. and Aikawa, E. and Edelman, E. R. and Nezami, F. R.},
 year = {2023},
 title = {CardioVision: A fully automated deep learning package for medical image segmentation and reconstruction generating digital twins for patients with aortic stenosis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172446208&doi=10.1016%2fj.compmedimag.2023.102289&partnerID=40&md5=6a242a8cb1b405907dd55f642ecf9aa5},
 keywords = {Artefact Design},
 volume = {109},
 journal = {Computerized Medical Imaging and Graphics},
 doi = {10.1016/j.compmedimag.2023.102289}
}


@article{Rubinic.2024,
 abstract = {This paper aims to explore the possibility of employing large language models (LLMs) -- a type of artificial intelligence (AI) -- in clinical pharmacology, with a focus on its possible misuse in bioweapon development. Additionally, ethical considerations, legislation and potential risk reduction measures are analysed. The existing literature is reviewed to investigate the potential misuse of AI and LLMs in bioweapon creation. The search includes articles from PubMed, Scopus and Web of Science Core Collection that were identified using a specific protocol. To explore the regulatory landscape, the OECD.ai platform was used. The review highlights the dual-use vulnerability of AI and LLMs, with a focus on bioweapon development. Subsequently, a case study is used to illustrate the potential of AI manipulation resulting in harmful substance synthesis. Existing regulations inadequately address the ethical concerns tied to AI and LLMs. Mitigation measures are proposed, including technical solutions (explainable AI), establishing ethical guidelines through collaborative efforts, and implementing policy changes to create a comprehensive regulatory framework. The integration of AI and LLMs into clinical pharmacology presents invaluable opportunities, while also introducing significant ethical and safety considerations. Addressing the dual-use nature of AI requires robust regulations, as well as adopting a strategic approach grounded in technical solutions and ethical values following the principles of transparency, accountability and safety. Additionally, AI's potential role in developing countermeasures against novel hazardous substances is underscored. By adopting a proactive approach, the potential benefits of AI and LLMs can be fully harnessed while minimizing the associated risks. {\copyright} 2023 British Pharmacological Society.},
 author = {Rubinic, I. and Kurtov, M. and Likic, R. and Dargan, P. I. and Wood, D. M.},
 year = {2024},
 title = {Artificial intelligence in clinical pharmacology: A case study and scoping review of large language models and bioweapon potential},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172384710&doi=10.1111%2fbcp.15899&partnerID=40&md5=6665cd6e341e8a4d7c885151a9fbbbd5},
 keywords = {Literature Review},
 pages = {620--628},
 volume = {90},
 number = {3},
 journal = {British Journal of Clinical Pharmacology},
 doi = {10.1111/bcp.15899}
}


@article{Sa.2022,
 abstract = {Remote sensing (RS) is now a standard tool used for grassland monitoring thanks to the availability of data at an unprecedented spatial and temporal resolution. The approaches to monitor grasslands often rely on the use of vegetation indices (e.g. NDVI) and empirical models trained on field data collected in tandem with the RS data. The best combination of models and features is often found by ad-hoc experimentation by the expert. This {\textquotedbl}classic{\textquotedbl} approach does not necessarily result in the best possible model. Automatic machine learning (AutoML) allows to automate this procedure by identifying the best possible pipeline in a data-driven manner. This study assesses the applicability of two distinct AutoML algorithms - AutoSklearn and AutoGluon - to monitor grass height from RS data and to systematically compare them to {\textquotedbl}classic{\textquotedbl} RS approaches. Grass height was estimated from Landsat ETM+ and OLI for a well-known conservation area as a case study. The {\textquotedbl}classic{\textquotedbl} RS approach emulated all possible ad hoc decisions by comparing all combinations of bands and vegetation indices against a naive use of the AutoML systems. While model selection and optimization are automated within AutoML models, for the {\textquotedbl}classic{\textquotedbl} RS approach, we used Bayesian optimization for hyperparameter tuning. We found that AutoML methods outperformed {\textquotedbl}classic{\textquotedbl} methods with the test error varying between similar to 1.73 cm +/- 0.02 and similar to 1.78 cm +/- 0.03 while for the {\textquotedbl}classic{\textquotedbl} methods it varied between similar to 1.84 cm +/- 0.03 and similar to 2.81 cm +/- 0.02. In the case of the {\textquotedbl}classic{\textquotedbl} methods, our exhaustive exploration of the possible feature combinations showed that while vegetation indices were always selected for the best models, which index got selected depended on the algorithm. The performance of AutoML compared to {\textquotedbl}classic{\textquotedbl} RS approaches clearly demonstrates the ability of these methods to quickly and effectively identify high-performing models. However, as this work focused on a single case-study, the results cannot be directly generalized to other study areas. Nevertheless, it provided a number of insights into future research opportunities to improve the use of AutoML in RS.},
 author = {de S{\'a}, N. C. and Baratchi, M. and Buitenhuis, V. and Cornelissen, P. and {van Bodegom}, P. M.},
 year = {2022},
 title = {AutoML for estimating grass height from ETM plus /OLI data from field measurements at a nature reserve},
 keywords = {Technical Review},
 pages = {2164--2183},
 volume = {59},
 number = {1},
 issn = {1943-7226},
 journal = {GISCIENCE {\&} REMOTE SENSING},
 doi = {10.1080/15481603.2022.2152304}
}


@proceedings{Saleem.2023,
 abstract = {Although numerous open-source tools exist for machine learning with tabular data, there is a scarcity of comparable resources tailored specifically for NLP. The lack of transparency in the inner workings of existing AutoNLP tools is a significant obstacle in scientific research. AutoML tools are known for their ability to perform model selection with little human intervention, improving the accuracy and reliability of the results. However, conducting a model space search among pre-trained NLP models can be computationally infeasible, making it challenging to determine the optimal NLP model for a given dataset. This research aims to enhance the performance of NLP model selection. Our approach has resulted in higher accuracy than existing methods on the dataset that was created. In our future work, we plan to benchmark our algorithm against datasets created by other researchers to validate its effectiveness. Additionally, we intend to use the same system to perform model selection among popular large language Transformer models.  {\copyright} 2023 ITMA.},
 year = {2023},
 title = {AutoNLP: A Framework for Automated Model Selection in Natural Language Processing},
 keywords = {Artefact Design},
 volume = {2023-June},
 editor = {Saleem, S. and Kumarapathirage, S.},
 doi = {10.23919/CISTI58278.2023.10212030},
 file = {Saleem, Kumarapathirage (Hg) 2023 - AutoNLP A Framework for Automated:Attachments/Saleem, Kumarapathirage (Hg) 2023 - AutoNLP A Framework for Automated.pdf:application/pdf}
}


@article{Salmanpour.2020,
 abstract = {Purpose: It is vital to appropriately power clinical trials towards discovery of novel disease-modifying therapies for Parkinson's disease (PD). Thus, it is critical to improve prediction of outcome in PD patients. Methods: We systematically probed a range of robust predictor algorithms, aiming to find best combinations of features for significantly improved prediction of motor outcome (MDS-UPDRS-III) in PD. We analyzed 204 PD patients with 18 features (clinical measures; dopamine-transporter (DAT) SPECT imaging measures), performing different randomized arrangements and utilizing data from 64{\%}/6{\%}/30{\%} of patients in each arrangement for training/training validation/final testing. We pursued 3 approaches: i) 10 predictor algorithms (accompanied with automated machine learning hyperparameter tuning) were first applied on 32 experimentally created combinations of 18 features, ii) we utilized Feature Subset Selector Algorithms (FSSAs) for more systematic initial feature selection, and iii) considered all possible combinations between 18 features (262,143 states) to assess contributions of individual features. Results: A specific set (set 18) applied to the LOLIMOT (Local Linear Model Trees) predictor machine resulted in the lowest absolute error 4.32 $\pm$ 0.19, when we firstly experimentally created 32 combinations of 18 features. Subsequently, 2 FSSAs (Genetic Algorithm (GA) and Ant Colony Optimization (ACO)) selecting 5 features, combined with LOLIMOT, reached an error of 4.15 $\pm$ 0.46. Our final analysis indicated that longitudinal motor measures (MDS-UPDRS-III years 0 and 1) were highly significant predictors of motor outcome. Conclusions: We demonstrate excellent prediction of motor outcome in PD patients by employing automated hyperparameter tuning and optimal utilization of FSSAs and predictor algorithms. {\copyright} 2019 Associazione Italiana di Fisica Medica},
 author = {Salmanpour, M. R. and Shamsaei, M. and Saberi, A. and Klyuzhin, I. S. and Tang, J. and Sossi, V. and Rahmim, A.},
 year = {2020},
 title = {Machine learning methods for optimal prediction of motor outcome in Parkinson's disease},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077466602&doi=10.1016%2fj.ejmp.2019.12.022&partnerID=40&md5=761033cde0416f9f4fab2a556bc6ae3d},
 keywords = {ML implementation},
 pages = {233--240},
 volume = {69},
 journal = {Physica Medica},
 doi = {10.1016/j.ejmp.2019.12.022}
}


@article{Salvucci.2019,
 abstract = {PURPOSE Dynamic network models predict clinical prognosis and inform therapeutic intervention by elucidating disease-driven aberrations at the systems level. However, the personalization of model predictions requires the profiling of multiple model inputs, which hampers clinical translation. PATIENTS AND METHODS We applied APOPTO-CELL, a prognostic model of apoptosis signaling, to showcase the establishment of computational platforms that require a reduced set of inputs. We designed two distinct and complementary pipelines: a probabilistic approach to exploit a consistent subpanel of inputs across the whole cohort (Ensemble) and a machine learning approach to identify a reduced protein set tailored for individual patients (Tree). Development was performed on a virtual cohort of 3,200,000 patients, with inputs estimated from clinically relevant protein profiles. Validation was carried out in an in-house stage III colorectal cancer cohort, with inputs profiled in surgical resections by reverse phase protein array (n = 120) and/or immunohistochemistry (n = 117). RESULTS Ensemble and Tree reproduced APOPTO-CELL predictions in the virtual patient cohort with 92{\%} and 99{\%} accuracy while decreasing the number of inputs to a consistent subset of three proteins (40{\%} reduction) or a personalized subset of 2.7 proteins on average (46{\%} reduction), respectively. Ensemble and Tree retained prognostic utility in the in-house colorectal cancer cohort. The association between the Ensemble accuracy and prognostic value (Spearman \textgreek{r} = 0.43; P = .02) provided a rationale to optimize the input composition for specific clinical settings. Comparison between profiling by reverse phase protein array (gold standard) and immunohistochemistry (clinical routine) revealed that the latter is a suitable technology to quantify model inputs. CONCLUSION This study provides a generalizable framework to optimize the development of network-based prognostic assays and, ultimately, to facilitate their integration in the routine clinical workflow. {\copyright} 2019 by American Society of Clinical Oncology.},
 author = {Salvucci, M. and Rahman, A. and Resler, A. J. and Udupi, G. M. and McNamara, D. A. and Kay, E. W. and Laurent-Puig, P. and Longley, D. B. and Johnston, P. G. and Lawler, M. and Wilson, R. and Salto-Tellez, M. and {van Schaeybroeck}, S. and Rafferty, M. and Gallagher, W. M. and Rehm, M. and Prehn, J.H.M.},
 year = {2019},
 title = {A machine learning platform to optimize the translation of personalized network models to the clinic},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077488710&doi=10.1200%2fCCI.18.00056&partnerID=40&md5=029efb945a06dd716dc32d06f2d3b816},
 keywords = {ML implementation},
 pages = {1--17},
 volume = {3},
 journal = {JCO Clinical Cancer Informatics},
 doi = {10.1200/CCI.18.00056},
 file = {Salvucci, Rahman et al. 2019 - A machine learning platform:Attachments/Salvucci, Rahman et al. 2019 - A machine learning platform.pdf:application/pdf}
}


@article{Samtani.2022,
 abstract = {Background: Quantification of left ventricular ejection fraction (LVEF) by transthoracic echocardiography (TTE) is operator-dependent, time-consuming, and error-prone. LVivoEF by DIA is a new artificial intelligence (AI) software, which displays the tracking of endocardial borders and rapidly quantifies LVEF. We sought to assess the accuracy of LVivoEF compared to cardiac magnetic resonance imaging (cMRI) as the reference standard and to compare LVivoEF to the standard-of-care physician-measured LVEF (MD-EF) including studies with ultrasound enhancing agents (UEAs). Methods: In 273 consecutive patients, we compared MD-EF and AI-derived LVEF to cMRI. AI-derived LVEF was obtained from a non-UEA four-chamber view without manual correction. Thirty-one patients were excluded: 25 had interval interventions or incomplete TTE or cMRI studies and six had uninterpretable non-UEA apical views. Results: In the 242 subjects, the correlation between AI and cMRI was r~=.890, similar to MD-EF and cMRI with r~=.891 (p~=~0.48). Of the 126 studies performed with UEAs, the correlation of AI using the unenhanced four-chamber view was r~=.89, similar to MD-EF with r~=.90. In the 116 unenhanced studies, AI correlation was r~=.87, similar to MD-EF with r~=.84. From Bland-Altman analysis, LVivoEF underreported the LVEF with a bias of 3.63 $\pm$ 7.40{\%} EF points compared to cMRI while MD-EF to cMRI had a bias of.33 $\pm$ 7.52{\%} (p~=~0.80). Conclusions: Compared to cMRI, LVivoEF can accurately quantify LVEF from a standard apical four-chamber view without manual correction. Thus, LVivoEF has the ability to improve and expedite LVEF quantification. {\copyright} 2022 Wiley Periodicals LLC},
 author = {Samtani, R. and Bienstock, S. and Lai, A. C. and Liao, S. and Baber, U. and Croft, L. and Stern, E. and Beerkens, F. and Ting, P. and Goldman, M. E.},
 year = {2022},
 title = {Assessment and validation of a novel fast fully automated artificial intelligence left ventricular ejection fraction quantification software},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124754834&doi=10.1111%2fecho.15318&partnerID=40&md5=f76acab128f91bb91da6f42b3a56c973},
 keywords = {Technical Review},
 pages = {473--482},
 volume = {39},
 number = {3},
 journal = {Echocardiography},
 doi = {10.1111/echo.15318}
}


@inproceedings{SanchezVicarte.2021,
 abstract = {Active learning is widely used in data labeling services to support real-world machine learning applications. By selecting and labeling the samples that have the highest impact on model retraining, active learning can reduce labeling efforts, and thus reduce cost. In this paper, we present a novel attack called Double Cross, which aims to manipulate data labeling and model training in active learning settings. To perform a double-cross attack, the adversary crafts inputs with a special trigger pattern and sends the triggered inputs to the victim model retraining pipeline. The goals of the triggered inputs are (1) to get selected for labeling and retraining by the victim; (2) to subsequently mislead human annotators into assigning an adversary-selected label; and (3) to change the victim model's behavior after retraining occurs. After retraining, the attack causes the victim to mislabel any samples with this trigger pattern to the adversary-chosen label. At the same time, labeling other samples, without the trigger pattern, is not affected. We develop a trigger generation method that simultaneously achieves these three goals. We evaluate the attack on multiple existing image classifiers and demonstrate that both gray-box and black-box attacks are successful. Furthermore, we perform experiments on a real-world machine learning platform (Amazon SageMaker) to evaluate the attack with human annotators in the loop, to confirm the practicality of the attack. Finally, we discuss the implications of the results and the open research questions moving forward. {\copyright} 2021 by The USENIX Association. All rights reserved.},
 author = {{Sanchez Vicarte}, J. R. and Wang, G. and Fletcher, C. W.},
 title = {Double-cross attacks: Subverting active learning systems},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114499583&partnerID=40&md5=6b597370266fd415f2694dfed5af5cc0},
 keywords = {Artefact Design},
 pages = {1593--1610},
 year = {2021}
}


@article{Sang.2021,
 abstract = {Plant-originated compounds, also known as phytochemicals, could improve health through either daily consumption or medical treatments. Conventional developments of phytochemical therapies are often based on the action mechanism of single compounds. However, compounds within a plant or across different plants are numerous, and their interactions potentially mold its therapeutic effect, which is hard to explore with conventional approaches. To address such challenges, an Artificial Intelligence (AI) platform featuring Monte Carlo method-based modeling (MCM2) for rapid identification of the optimal dose combinations is devised. MCM2 utilizes both the mean and variances within multiple replicates, and thus maximizes the optimization efficiency. MCM2 is applied to optimize the therapeutic effect on alcoholic hepatic injury of a quaternary dose combination of products from Gynostemma pentaphyllum and Lotus leaf, which are identified through in vitro screening of 50 products extracted from 24 herbal medicine candidates. The optimal combination identified by MCM2 is then tested on murine model. Both in vitro and in vivo results confirmed the AI-optimized phytochemical combination significantly reduced acute alcoholic liver injury. The AI platform introduced in this study could serve to promptly identify optimal combinatorial regimens with potential synergy, which enhances the efficiency during the development of compound medicine. {\copyright} 2021 Wiley-VCH GmbH},
 author = {Sang, X. and Wang, B. and Zhao, P. and Ding, X. and Ahmad, K. Z. and Yu, J.},
 year = {2021},
 title = {Harnessing Artificial Intelligence to Expedite Identification of Therapeutic Phytochemical Combination for Alcoholic Hepatic Injury},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104709675&doi=10.1002%2fadtp.202100042&partnerID=40&md5=cbd555ae9523f2edde4b7f9c32ce85d6},
 keywords = {Artefact Design},
 volume = {4},
 number = {6},
 journal = {Advanced Therapeutics},
 doi = {10.1002/adtp.202100042}
}


@article{SchmidtErfurth.2018,
 abstract = {Major advances in diagnostic technologies are offering unprecedented insight into the condition of the retina and beyond ocular disease. Digital images providing millions of morphological datasets can fast and non-invasively be analyzed in a comprehensive manner using artificial intelligence (AI). Methods based on machine learning (ML) and particularly deep learning (DL) are able to identify, localize and quantify pathological features in almost every macular and retinal disease. Convolutional neural networks thereby mimic the path of the human brain for object recognition through learning of pathological features from training sets, supervised ML, or even extrapolation from patterns recognized independently, unsupervised ML. The methods of AI-based retinal analyses are diverse and differ widely in their applicability, interpretability and reliability in different datasets and diseases. Fully automated AI-based systems have recently been approved for screening of diabetic retinopathy (DR). The overall potential of ML/DL includes screening, diagnostic grading as well as guidance of therapy with automated detection of disease activity, recurrences, quantification of therapeutic effects and identification of relevant targets for novel therapeutic approaches. Prediction and prognostic conclusions further expand the potential benefit of AI in retina which will enable personalized health care as well as large scale management and will empower the ophthalmologist to provide high quality diagnosis/therapy and successfully deal with the complexity of 21st century ophthalmology. {\copyright} 2018 The Authors},
 author = {Schmidt-Erfurth, U. and Sadeghipour, A. and Gerendas, B. S. and Waldstein, S. M. and Bogunovi{\'c}, H.},
 year = {2018},
 title = {Artificial intelligence in retina},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051146019&doi=10.1016%2fj.preteyeres.2018.07.004&partnerID=40&md5=61884df556b62d3f803e7ed1969fb413},
 keywords = {ML implementation},
 pages = {1--29},
 volume = {67},
 journal = {Progress in Retinal and Eye Research},
 doi = {10.1016/j.preteyeres.2018.07.004}
}


@article{Schnur.2022,
 abstract = {Objective: After left hemisphere stroke, 20{\%}--50{\%} of people experience language deficits, including difficulties in naming. Naming errors that are semantically related to the intended target (e.g., producing ``violin'' for picture HARP) indicate a potential impairment in accessing knowledge of word forms and their meanings. Understanding the cause of naming impairments is crucial to better modeling of language production as well as for tailoring individualized rehabilitation. However, evaluation of naming errors is typically by subjective and laborious dichotomous classification. As a result, these evaluations do not capture the degree of semantic similarity and are susceptible to lower interrater reliability because of subjectivity. Method: We investigated whether a computational linguistic measure using word2vec (Mikolov, Chen, et al., 2013) addressed these limitations by evaluating errors during object naming in a group of patients during the acute stage of a left-hemisphere stroke (N = 105). Results: Pearson correlations demonstrated excellent convergent validity of word2vec's semantically related estimates of naming errors and independent tests of access to lexical--semantic knowledge ( p {\textless}.0001). Further, multiple regression analysis showed word2vec's semantically related estimates were significantly better than human error classification at predicting performance on tests of lexical--semantic knowledge. Conclusions: Useful to both theorists and clinicians, our word2vec-based method provides an automated, continuous, and objective psychometric measure of access to lexical--semantic knowledge during naming. {\copyright} 2022 American Psychological Association},
 author = {Schnur, T. T. and Lei, C.-M.},
 year = {2022},
 title = {Assessing Naming Errors Using an Automated Machine Learning Approach},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139327726&doi=10.1037%2fneu0000860&partnerID=40&md5=97059ac4c2a6d45ac38a30b030ff840c},
 keywords = {ML implementation;Technical Review},
 pages = {709--718},
 volume = {36},
 number = {8},
 journal = {Neuropsychology},
 doi = {10.1037/neu0000860},
 file = {Schnur, Lei 2022 - Assessing Naming Errors Using:Attachments/Schnur, Lei 2022 - Assessing Naming Errors Using.pdf:application/pdf}
}


@article{Schwen.2022,
 abstract = {Image analysis tasks in computational pathology are commonly solved using convolutional neural networks (CNNs). The selection of a suitable CNN architecture and hyperparameters is usually done through exploratory iterative optimization, which is computationally expensive and requires substantial manual work. The goal of this article is to evaluate how generic tools for neural network architecture search and hyperparameter optimization perform for common use cases in computational pathology. For this purpose, we evaluated one on-premises and one cloud-based tool for three different classification tasks for histological images: tissue classification, mutation prediction, and grading. We found that the default CNN architectures and parameterizations of the evaluated AutoML tools already yielded classification performance on par with the original publications. Hyperparameter optimization for these tasks did not substantially improve performance, despite the additional computational effort. However, performance varied substantially between classifiers obtained from individual AutoML runs due to non-deterministic effects. Generic CNN architectures and AutoML tools could thus be a viable alternative to manually optimizing CNN architectures and parametrizations. This would allow developers of software solutions for computational pathology to focus efforts on harder-to-automate tasks such as data curation. {\copyright} 2022 The Author(s)},
 author = {Schwen, L. O. and Schacherer, D. and Gei{\ss}ler, C. and Homeyer, A.},
 year = {2022},
 title = {Evaluating generic AutoML tools for computational pathology},
 keywords = {Technical Review},
 volume = {29},
 journal = {Informatics in Medicine Unlocked},
 file = {Schwen, Schacherer et al 2022 - Evaluating generic AutoML tools:Attachments/Schwen, Schacherer et al 2022 - Evaluating generic AutoML tools.pdf:application/pdf}
}


@article{Seibold.2021,
 abstract = {Computational reproducibility is a corner stone for sound and credible research. Especially in complex statistical analyses-such as the analysis of longitudinal data-reproducing results is far from simple, especially if no source code is available. In this work we aimed to reproduce analyses of longitudinal data of 11 articles published in PLOS ONE. Inclusion criteria were the availability of data and author consent. We investigated the types of methods and software used and whether we were able to reproduce the data analysis using open source software. Most articles provided overview tables and simple visualisations. Generalised Estimating Equations (GEEs) were the most popular statistical models among the selected articles. Only one article used open source software and only one published part of the analysis code. Replication was difficult in most cases and required reverse engineering of results or contacting the authors. For three articles we were not able to reproduce the results, for another two only parts of them. For all but two articles we had to contact the authors to be able to reproduce the results. Our main learning is that reproducing papers is difficult if no code is supplied and leads to a high burden for those conducting the reproductions. Open data policies in journals are good, but to truly boost reproducibility we suggest adding open code policies.},
 author = {Seibold, H. and Czerny, S. and Decke, S. and Dieterle, R. and Eder, T. and Fohr, S. and Hahn, N. and Hartmann, R. and Heindl, C. and Kopper, P. and Lepke, D. and Loidl, V. and Mandl, M. and Musiol, S. and Peter, J. and Piehler, A. and Rojas, E. and Schmid, S. and Schmidt, H. and Schmoll, M. and Schneider, L. and To, X. Y. and Tran, V. and V{\"o}lker, A. and Wagner, M. and Wagner, J. and Waize, M. and Wecker, H. and Yang, R. and Zellner, S. and Nalenz, M.},
 year = {2021},
 title = {A computational reproducibility study of PLOS ONE articles featuring longitudinal data analyses},
 keywords = {Experiment},
 volume = {16},
 number = {6},
 journal = {PLoS ONE},
 doi = {10.1371/journal.pone.0251194},
 file = {Seibold, Czerny et al. 2021 - A computational reproducibility study:Attachments/Seibold, Czerny et al. 2021 - A computational reproducibility study.pdf:application/pdf}
}


@article{Sevgi.2023,
 abstract = {The use of artificial intelligence in neurosurgical education has been growing in recent times. ChatGPT, a free and easily accessible language model, has been gaining popularity as an alternative education method. It is necessary to explore the potential of this program in neurosurgery education and to evaluate its reliability. This study aimed to show the reliability of ChatGPT by asking various questions to the chat engine, how it can contribute to neurosurgery education by preparing case reports or questions, and its contributions when writing academic articles. The results of the study showed that while ChatGPT provided intriguing and interesting responses, it should not be considered a dependable source of information. The absence of citations for scientific queries raises doubts about the credibility of the answers provided. Therefore, it is not advisable to solely rely on ChatGPT as an educational resource. With further updates and more specific prompts, it may be possible to improve its accuracy. In conclusion, while ChatGPT has potential as an educational tool, its reliability needs to be further evaluated and improved before it can be widely adopted in neurosurgical education. {\copyright} 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
 author = {Sevgi, U. T. and Erol, G. and Do{\u{g}}ruel, Y. and S{\"o}nmez, O. F. and Tubbs, R. S. and G{\"u}ngor, A.},
 year = {2023},
 title = {The role of an open artificial intelligence platform in modern neurosurgical education: a preliminary study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152514660&doi=10.1007%2fs10143-023-01998-2&partnerID=40&md5=97ed87e71e43f59a99e27675a983042f},
 keywords = {Case Study},
 volume = {46},
 number = {1},
 journal = {Neurosurgical Review},
 doi = {10.1007/s10143-023-01998-2},
 file = {Sevgi, Erol et al. 2023 - The role of an open:Attachments/Sevgi, Erol et al. 2023 - The role of an open.pdf:application/pdf}
}


@inproceedings{Shirazi.2022,
 abstract = {Designing Computer Vision (CV) algorithms for production applications requires extensive knowledge of Machine Learning (ML), programming, and software architecture. Accordingly, keeping humans, especially domain users, in the loop of model decisions is a challenge. The algorithms created by the CV community usually omit to provide clear interpretable information on the finally selected model, except for numerical metrics. This leads to a transparency challenge, known as black-box ML, which may negatively impact the users' trust in the sensitive use cases. Furthermore, the majority of interpretable methods for giving feedback to domain users are usually made after training, i.e., proposing a final model without the user participation.In this paper, we present a Visual Interpretation-based Control (VIC) technique, which is a simple but principled model evaluation criteria. VIC offers a decision-making strategy for enabling non-experts to dictate their intuition from the most important areas in an image and incorporate this within CV pipelines. Specifically, we supervise a generative adversarial network to penalize its generator for the specified region of interest. We further validate our method through an architecture selection strategy in one of the common AutoML benchmarks.  {\copyright} 2022 IEEE.},
 author = {Shirazi, M. and Safronov, G. and Rizk, A.},
 title = {Rethinking of Domain Users Control in Computer Vision Pipelines by Customized Attention},
 keywords = {Artefact Design},
 pages = {1018--1025},
 year = {2022},
 doi = {10.1109/ICMLA55696.2022.00170},
 file = {Shirazi, Safronov et al 2022 - Rethinking of Domain Users Control:Attachments/Shirazi, Safronov et al 2022 - Rethinking of Domain Users Control.pdf:application/pdf}
}


@article{Sibley.2022,
 abstract = {Background: Parkinson's disease severity is typically measured using the Movement Disorder Society Unified Parkinson's disease rating scale (MDS-UPDRS). While training for this scale exists, users may vary in how they score a patient with the consequence of intra-rater and inter-rater variability. Objective: In this study we explored the consistency of an artificial intelligence platform compared with traditional clinical scoring in the assessment of motor severity in PD. Methods: Twenty-two PD patients underwent simultaneous MDS-UPDRS scoring by two experienced MDS-UPDRS raters and the two sets of accompanying video footage were also scored by an artificial intelligence video analysis platform known as KELVIN. Results: KELVIN was able to produce a summary score for 7 MDS-UPDRS part 3 items with good inter-rater reliability (Intraclass Correlation Coefficient (ICC) 0.80 in the OFF-medication state, ICC 0.73 in the ON-medication state). Clinician scores had exceptionally high levels of inter-rater reliability in both the OFF (0.99) and ON (0.94) medication conditions (possibly reflecting the highly experienced team). There was an ICC of 0.84 in the OFF-medication state and 0.31 in the ON-medication state between the mean Clinician and mean Kelvin scores for the equivalent 7 motor items, possibly due to dyskinesia impacting on the KELVIN scores. Conclusion: We conclude that KELVIN may prove useful in the capture and scoring of multiple items of MDS-UPDRS part 3 with levels of consistency not far short of that achieved by experienced MDS-UPDRS clinical raters, and is worthy of further investigation.  {\copyright} 2022 - IOS Press. All rights reserved.},
 author = {Sibley, K. and Girges, C. and Candelario, J. and Milabo, C. and Salazar, M. and Esperida, J. O. and Dushin, Y. and Limousin, P. and Foltynie, T.},
 year = {2022},
 title = {An Evaluation of KELVIN, an Artificial Intelligence Platform, as an Objective Assessment of the MDS UPDRS Part III},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140144370&doi=10.3233%2fJPD-223493&partnerID=40&md5=93dbc6471e324aab5d18f734a63521f7},
 keywords = {ML implementation},
 pages = {2223--2233},
 volume = {12},
 number = {7},
 journal = {Journal of Parkinson's Disease},
 doi = {10.3233/JPD-223493}
}


@inproceedings{Siiman.2023,
 abstract = {Recent advances in the usability of generative AI platforms, such as ChatGPT, suggest that artificial intelligence can seemingly capture many of the rules and meanings underlying human language. As a result, AI can potentially automate many tasks requiring human-like understanding and generation of natural language. Qualitative data analysis tends to be a time-consuming task which is susceptible to human bias and mistakes. In this study, we explored the use of ChatGPT and the GPT-4 model to assist with analyzing qualitative data from a previous study by Rannastu-Avalos, M{\"a}eots, and Siiman (2022). In that prior study, pairs of adults collaborated via a free-form, text-based chat interface to solve a computer simulation problem about balancing a seesaw. To re-analyze the data using AI assistance, both deductive and inductive approaches were applied and the results compared to human coding and human interpretation of the data. The results show that it is important to structure and phrase prompts so that AI responses best align with human interpretation. Deductive analysis performed better than inductive analysis, presumably because prompts with richer contextual information referring to specific theoretical perspectives could be crafted. Our results suggest that AI-assisted qualitative analysis has the potential to improve transparency in the coding of qualitative data by encouraging human analysts to report AI prompts that agree with their interpretations of the data, and in turn can be reused by other researchers. {\copyright} 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
 author = {Siiman, L. A. and Rannastu-Avalos, M. and P{\"o}ys{\"a}-Tarhonen, J. and H{\"a}kkinen, P. and Pedaste, M.},
 title = {Opportunities and Challenges for AI-Assisted Qualitative Data Analysis: An Example from Collaborative Problem-Solving Discourse Data},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172233907&doi=10.1007%2f978-3-031-40113-8_9&partnerID=40&md5=bd7104e1c2239c74c185b784da848f35},
 keywords = {ML implementation},
 pages = {87--96},
 year = {2023},
 doi = {10.1007/978-3-031-40113-8{\textunderscore }9}
}


@article{Silva.2024,
 abstract = {Our objective was to establish and test a machine learning-based screening process that would be applicable to systematic reviews in pharmaceutical sciences. We used the SPIDER (Sample, Phenomenon of Interest, Design, Evaluation, Research type) model, a broad search strategy, and a machine learning tool (Research Screener) to identify relevant references related to y-site compatibility of 95 intravenous drugs used in neonatal intensive care settings. Two independent reviewers conducted pilot studies, including manual screening and evaluation of Research Screener, and used the kappa-coefficient for inter-reviewer reliability. After initial deduplication of the search strategy results, 27 597 references were available for screening. Research Screener excluded 1735 references, including 451 duplicate titles and 1269 reports with no abstract/title, which were manually screened. The remainder (25 862) were subject to the machine learning screening process. All eligible articles for the systematic review were extracted from {\textless}10{\%} of the references available for screening. Moderate inter-reviewer reliability was achieved, with kappa-coefficient $\geq$0.75. Overall, 324 references were subject to full-text reading and 118 were deemed relevant for the systematic review. Our study showed that a broad search strategy to optimize the literature captured for systematic reviews can be efficiently screened by the semi-automated machine learning tool, Research Screener. {\copyright} 2024 The Authors. Pharmacology Research {\&} Perspectives published by British Pharmacological Society and American Society for Pharmacology and Experimental Therapeutics and John Wiley {\&} Sons Ltd.},
 author = {de Silva, D.T.N. and Moore, B. R. and Strunk, T. and Petrovski, M. and Varis, V. and Chai, K. and Ng, L. and Batty, K.},
 year = {2024},
 title = {Development of a pharmaceutical science systematic review process using a semi-automated machine learning tool: Intravenous drug compatibility in the neonatal intensive care setting},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181987610&doi=10.1002%2fprp2.1170&partnerID=40&md5=439ea783c895eeacb6c49f21c8091712},
 keywords = {Literature Review;ML implementation},
 volume = {12},
 number = {1},
 journal = {Pharmacology Research and Perspectives},
 doi = {10.1002/prp2.1170},
 file = {Silva, Moore et al. 2024 - Development of a pharmaceutical science:Attachments/Silva, Moore et al. 2024 - Development of a pharmaceutical science.pdf:application/pdf}
}


@inproceedings{Singh.2021,
 abstract = {To create a provenance-enabled, extensible machine learning (ML) platform is an exercise that can involve different pieces of software: databases which supply the data to create the model and can also potentially store the result of predictions, web services that allow for ingestion of data into databases and also supply data out of the database to the consumer, code repositories that can be used to track the versioning of models for the sake of provenance and last but not the least the machine learning models themselves. Here we present an approach that we used to create a Soil Spectral inference platform by integrating these software components along with the use of Docker to create a technology agnostic, loosely coupled ML platform. The approach we show here demonstrates how metadata stored in a database can be used not only to drive a workflow but also that a database driven approach allows for extension points where new logic can be plugged-in to expand the capability of the platform. In our approach, the database plays the central role in the application design. It faithfully reflects the entities involved in problem space. Instead of starting off with a more agile approach of creating, or prototyping, applications to reflect the ML workflow, we invested some time beforehand, understanding the domain to tease out the domain objects, their relationships to one other and the likely workflows that the ML platform will support. We also involved potential future external users in key discussions to understand their workflows and make sure our database is flexible enough to accommodate their concepts. In all this, we used the database schema diagram as a key artifact to consolidate the understanding of the domain, to make explicit the relationships between more prominent entities and to communicate with different team members on the project. Once the schema was in a stable enough state that we started designing the applications that would run off it. The ML platform applications use the metadata about extensions point stored in the database to implement the machine learning workflow. At the project onset it was decided to use a tech stack that was portable and for those components where we could not control the technologies being used, such as the ML models themselves, we used Docker containers to abstract away the implementation details and expose the model to the platform as a set of interfaces that would remain common for all the models hosted on the platform. For provenance, we decided to use Subversion, which is an open-source version control system and can handle large files. The ML models and the input data that was used to build the models is versioned in subversion, and that version information is used as ML model metadata in the database to track provenance. The loosely coupled framework template that we used is modular in nature and the subsystems are connected to each other through web service end points. It can handle future changes by combining, modifying or adding more subsystems visible through the database, each exposing its capabilities through web services. {\copyright} 2021 Proceedings of the International Congress on Modelling and Simulation, MODSIM. All rights reserved.},
 author = {Singh, R. M. and Wilson, P. and Gregory, L. and Seaton, S. and Malone, B.},
 title = {Towards a provenance-enabled, reproducible, and extensible machine learning platform by integrating databases, web services, containers, and code repositories in a loosely coupled manner},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177057163&partnerID=40&md5=c3a74e9a4240e7818696a49ac5b729f1},
 keywords = {Artefact Design},
 pages = {239--245},
 year = {2021}
}


@article{Singh.2021b,
 abstract = {The growing number of next-generation sequencing (NGS) data presents a unique opportunity to study the combined impact of mitochondrial and nuclear-encoded genetic variation in complex disease. Mitochondrial DNA variants and in particular, heteroplasmic variants, are critical for determining human disease severity. While there are approaches for obtaining mitochondrial DNA variants from NGS data, these software do not account for the unique characteristics of mitochondrial genetics and can be inaccurate even for homoplasmic variants. We introduce MitoScape, a novel, big-data, software for extracting mitochondrial DNA sequences from NGS. MitoScape adopts a novel departure from other algorithms by using machine learning to model the unique characteristics of mitochondrial genetics. We also employ a novel approach of using rho-zero (mitochondrial DNA-depleted) data to model nuclear-encoded mitochondrial sequences. We showed that MitoScape produces accurate heteroplasmy estimates using gold-standard mitochondrial DNA data. We provide a comprehensive comparison of the most common tools for obtaining mtDNA variants from NGS and showed that MitoScape had superior performance to compared tools in every statistically category we compared, including false positives and false negatives. By applying MitoScape to common disease examples, we illustrate how MitoScape facilitates important heteroplasmy-disease association discoveries by expanding upon a reported association between hypertrophic cardiomyopathy and mitochondrial haplogroup T in men (adjusted p-value = 0.003). The improved accuracy of mitochondrial DNA variants produced by MitoScape will be instrumental in diagnosing disease in the context of personalized medicine and clinical diagnostics.

Author summary

Recent studies have highlighted the importance of mitochondrial DNA variation in both primary mitochondrial disease and complex, human pathology including COVID-19, and space-flight stress. The vast amount of existing, next-generation sequencing (NGS) data can be leveraged to interrogate both nuclear and mitochondrial DNA (mtDNA) sequence simultaneously, allowing for analysis of the interplay between mitochondrial and nuclear encoded genes in mitochondrial function. Identifying mtDNA sequence accurately is complicated by the presence of nuclear encoded mitochondrial sequences (NUMTs), which are homologous to mtDNA. Current software for analyzing mtDNA from NGS do not accurately model the unique characteristics of mitochondrial genetics. We introduce MitoScape, a novel, big-data, software which models mitochondrial genetics through machine learning to accurately identify mtDNA sequence from NGS data. MitoScape takes advantage of rho-zero cell data to model the characteristics of NUMTs. We show that MitoScape produces more accurate heteroplasmy estimates compared to published software. We provide an example of applying MitoScape in replicating an association between hypertrophic cardiomyopathy and mitochondrial haplogroup T in men. MitoScape is an important contribution to mitochondrial genomics allowing for accurate mtDNA variants, and the ability to tailor mtDNA analysis in different population and disease contexts, which is not available in other software.},
 author = {Singh, L. N. and Ennis, B. and Loneragan, B. and Tsao, N. L. and Sanchez, MIGL and Li, J. and Acheampong, P. and Tran, O. and Trounce, I. A. and Zhu, Y. and Potluri, P. and Emanuel, B. S. and Rader, D. J. and Arany, Z. and Damrauer, S. M. and Resnick, A. C. and Anderson, S. A. and Wallace, D. C. and {Regeneron Genetics Ctr}},
 year = {2021},
 title = {MitoScape: A big-data, machine-learning platform for obtaining mitochondrial DNA from next-generation sequencing data},
 keywords = {Artefact Design},
 volume = {17},
 number = {11},
 journal = {PLoS Computational Biology},
 doi = {10.1371/journal.pcbi.1009594},
 file = {Singh, Ennis et al. 2021 - MitoScape A big-data:Attachments/Singh, Ennis et al. 2021 - MitoScape A big-data.pdf:application/pdf}
}


@article{Song.2015,
 abstract = {Objectives: Pure-tone audiometry has been a staple of hearing assessments for decades. Many different procedures have been proposed for measuring thresholds with pure tones by systematically manipulating intensity one frequency at a time until a discrete threshold function is determined. The authors have developed a novel nonparametric approach for estimating a continuous threshold audiogram using Bayesian estimation and machine learning classification. The objective of this study was to assess the accuracy and reliability of this new method relative to a commonly used threshold measurement technique. Design: The authors performed air conduction pure-tone audiometry on 21 participants between the ages of 18 and 90 years with varying degrees of hearing ability. Two repetitions of automated machine learning audiogram estimation and one repetition of conventional modified Hughson-Westlake ascending-descending audiogram estimation were acquired by an audiologist. The estimated hearing thresholds of these two techniques were compared at standard audiogram frequencies (i.e., 0.25, 0.5, 1, 2, 4, 8 kHz). Results: The two threshold estimate methods delivered very similar estimates at standard audiogram frequencies. Specifically, the mean absolute difference between estimates was 4.16 $\pm$ 3.76 dB HL. The mean absolute difference between repeated measurements of the new machine learning procedure was 4.51 $\pm$ 4.45 dB HL. These values compare favorably with those of other threshold audiogram estimation procedures. Furthermore, the machine learning method generated threshold estimates from significantly fewer samples than the modified Hughson-Westlake procedure while returning a continuous threshold estimate as a function of frequency. Conclusions: The new machine learning audiogram estimation technique produces continuous threshold audiogram estimates accurately, reliably, and efficiently, making it a strong candidate for widespread application in clinical and research audiometry. {\copyright} 2015 Wolters Kluwer Health, Inc. All rights reserved • Printed in the U.S.A.},
 author = {Song, X. D. and Wallace, B. M. and Gardner, J. R. and Ledbetter, N. M. and Weinberger, K. Q. and Barbour, D. L.},
 year = {2015},
 title = {Fast, Continuous Audiogram Estimation Using Machine Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943539010&doi=10.1097%2fAUD.0000000000000186&partnerID=40&md5=cc405c27c0edfa6fc7ed3e27db2fbd85},
 keywords = {ML implementation},
 pages = {e326-e335},
 volume = {36},
 number = {6},
 journal = {Ear and Hearing},
 doi = {10.1097/AUD.0000000000000186},
 file = {Song, Wallace et al. 2015 - Fast, Continuous Audiogram Estimation Using:Attachments/Song, Wallace et al. 2015 - Fast, Continuous Audiogram Estimation Using.pdf:application/pdf}
}


@article{Spanig.2021,
 abstract = {Owing to the great variety of distinct peptide encodings, working on a biomedical classification task at hand is challenging. Researchers have to determine encodings capable to represent underlying patterns as numerical input for the subsequent machine learning. A general guideline is lacking in the literature, thus, we present here the first large-scale comprehensive study to investigate the performance of a wide range of encodings on multiple datasets from different biomedical domains. For the sake of completeness, we added additional sequence- and structure-based encodings. In particular, we collected 50 biomedical datasets and defined a fixed parameter space for 48 encoding groups, leading to a total of 397 700 encoded datasets. Our results demonstrate that none of the encodings are superior for all biomedical domains. Nevertheless, some encodings often outperform others, thus reducing the initial encoding selection substantially. Our work offers researchers to objectively compare novel encodings to the state of the art. Our findings pave the way for a more sophisticated encoding optimization, for example, as part of automated machine learning pipelines. The work presented here is implemented as a large-scale, end-to-end workflow designed for easy reproducibility and extensibility. All standardized datasets and results are available for download to comply with FAIR standards. {\copyright} 2021 The Author(s). Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.},
 author = {Sp{\"a}nig, S. and Mohsen, S. and Hattab, G. and Hauschild, A.-C. and Heider, D.},
 year = {2021},
 title = {A large-scale comparative study on peptide encodings for biomedical classification},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111524765&doi=10.1093%2fnargab%2flqab039&partnerID=40&md5=eda43486f974e92c89e025855c153e71},
 keywords = {Technical Review},
 pages = {1--13},
 volume = {3},
 number = {2},
 journal = {NAR Genomics and Bioinformatics},
 doi = {10.1093/nargab/lqab039},
 file = {Sp{\"a}nig, Mohsen et al. 2021 - A large-scale comparative study:Attachments/Sp{\"a}nig, Mohsen et al. 2021 - A large-scale comparative study.pdf:application/pdf}
}


@article{Stalring.2011,
 abstract = {Background: Machine learning has a vast range of applications. In particular, advanced machine learning methods are routinely and increasingly used in quantitative structure activity relationship (QSAR) modeling. QSAR data sets often encompass tens of thousands of compounds and the size of proprietary, as well as public data sets, is rapidly growing. Hence, there is a demand for computationally efficient machine learning algorithms, easily available to researchers without extensive machine learning knowledge. In granting the scientific principles of transparency and reproducibility, Open Source solutions are increasingly acknowledged by regulatory authorities. Thus, an Open Source state-of-the-art high performance machine learning platform, interfacing multiple, customized machine learning algorithms for both graphical programming and scripting, to be used for large scale development of QSAR models of regulatory quality, is of great value to the QSAR community. Results: This paper describes the implementation of the Open Source machine learning package AZOrange. AZOrange is specially developed to support batch generation of QSAR models in providing the full work flow of QSAR modeling, from descriptor calculation to automated model building, validation and selection. The automated work flow relies upon the customization of the machine learning algorithms and a generalized, automated model hyper-parameter selection process. Several high performance machine learning algorithms are interfaced for efficient data set specific selection of the statistical method, promoting model accuracy. Using the high performance machine learning algorithms of AZOrange does not require programming knowledge as flexible applications can be created, not only at a scripting level, but also in a graphical programming environment. Conclusions: AZOrange is a step towards meeting the needs for an Open Source high performance machine learning platform, supporting the efficient development of highly accurate QSAR models fulfilling regulatory requirements. {\copyright} 2011 St{\aa}lring et al; licensee Chemistry Central Ltd.},
 author = {St{\aa}lring, J. C. and Carlsson, L. A. and Almeida, P. and Boyer, S.},
 year = {2011},
 title = {AZOrange - High performance Open Source machine learning for QSAR modeling in a graphical programming environment},
 keywords = {Artefact Design;ML implementation},
 volume = {3},
 number = {7},
 journal = {Journal of Cheminformatics},
 file = {St{\aa}lring, Carlsson et al. 2011 - AZOrange:Attachments/St{\aa}lring, Carlsson et al. 2011 - AZOrange.pdf:application/pdf}
}


@article{Stephansen.2023,
 abstract = {The body condition of dairy cows is a crucial health and welfare indicator that is widely acknowledged. Dairy herds with a well-management body condition tend to have more fertile and functional cows. Therefore, routine recording of high-quality body condition phenotypes is required. Automated prediction of body condition from 3D images can be a cost-effective approach to current manual recording by technicians. Using 3D-images, we aimed to build a reliable prediction model of body condition for Jersey cows. The dataset consisted of 808 individual Jersey cows with 2, 253 phenotypes from three herds in Denmark. Body condition was scored on a 1 to 9 scale and transformed into a 1 to 5 scale with 0.5-unit differences. The cows' back images were recorded using a 3D camera (Microsoft Xbox One Kinect v2). We used contour and back height features from 3D-images as predictors, together with class predictors (evaluator, herd, evaluation round, parity, lactation week). The performance of machine learning algorithms was assessed using H2O AutoML algorithm (h2o.ai). Based on outputs from AutoML, DeepLearning (DL; multilayer feedforward artificial neural network) and Gradient Boosting Machine (GBM) algorithms were implemented for classification and regression tasks and compared on prediction accuracy. In addition, we compared the Partial Least Square (PLS) method for regression. The training and validation data were divided either through a random 7:3 split for 10 replicates or by allocating two herds for training and one herd for validation. The accuracy of classification models showed the DL algorithm performed better than the GBM algorithm. The DL model achieved a mean accuracy of 48.1{\%} on the exact phenotype and 93.5{\%} accuracy with a 0.5-unit deviation. The performances of PLS and DL regression methods were comparable, with mean coefficient of determination of 0.67 and 0.66, respectively. When we used data from two herds for training and the third herd as validation, we observed a slightly decreased prediction accuracy compared to the 7:3 split of the dataset. The accuracies for DL and PLS in the herd validation scenario were {\textgreater} 38{\%} on the exact phenotype and {\textgreater} 87{\%} accuracy with 0.5-unit deviation. This study demonstrates the feasibility of a reliable body condition prediction model in Jersey cows using 3D-images. The approach developed can be used for reliable and frequent prediction of cows' body condition to improve dairy farm management and genetic evaluations. {\copyright} The Author(s) 2023. Published by Oxford University Press on behalf of the American Society of Animal Science. All rights reserved.},
 author = {Stephansen, R. B. and Manzanilla-Pech, C.I.V. and Gebreyesus, G. and Sahana, G. and Lassen, J.},
 year = {2023},
 title = {Prediction of body condition in Jersey dairy cattle from 3D-images using machine learning techniques},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181178603&doi=10.1093%2fjas%2fskad376&partnerID=40&md5=587b884801bc5b3bb9060a53589fe62f},
 keywords = {ML implementation},
 volume = {101},
 journal = {Journal of Animal Science},
 doi = {10.1093/jas/skad376}
}


@article{Stevenson.2019,
 abstract = {Importance: Artificial intelligence (AI) algorithms are under development for use in diabetic retinopathy photo screening pathways. To be clinically acceptable, such systems must also be able to classify other fundus abnormalities and clinical features at the point of care. Background: We aimed to develop an AI system that can detect several fundus pathologies and report relevant clinical features. Design: Convolutional neural network training with retrospective data set. Participants: Colour fundus photos were obtained from publicly available fundus image databases. Methods: Images were uploaded to a web-based AI platform for training and validation of AI classifiers. Separate classifiers were created for each fundus pathology and clinical feature. Main Outcome Measures: Accuracy, sensitivity, specificity and area under receiver operating characteristic curve (AUC) for each classifier. Results: We obtained 4435 images from publicly available fundus image databases. AI classifiers were developed for each disease state above. Although statistical performance was limited by the small sample size, average accuracy was 89{\%}, average sensitivity was 75{\%}, average specificity was 89{\%} and average AUC was 0.58. Conclusion and Relevance: This study is a proof-of-concept AI system that could be implemented within a diabetic photo-screening pathway. Performance was promising but not yet at the level that would be required for clinical application. We have shown that it is possible for clinicians to develop AI classifiers with no previous programming or AI knowledge, using standard laptop computers. {\copyright} 2018 Royal Australian and New Zealand College of Ophthalmologists},
 author = {Stevenson, C. H. and Hong, S. C. and Ogbuehi, K. C.},
 year = {2019},
 title = {Development of an artificial intelligence system to classify pathology and clinical features on retinal fundus images},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056649506&doi=10.1111%2fceo.13433&partnerID=40&md5=f9d8854afca40f68b6f24bfcea4b82c8},
 keywords = {ML implementation},
 pages = {484--489},
 volume = {47},
 number = {4},
 journal = {Clinical and Experimental Ophthalmology},
 doi = {10.1111/ceo.13433}
}


@article{Stotter.2023,
 abstract = {Purpose: The aim of this study was to investigate the performance of an artificial intelligence (AI)-based software for fully automated analysis of leg alignment pre- and postoperatively after high tibial osteotomy (HTO) on long-leg radiographs (LLRs). Methods: Long-leg radiographs of 95 patients with varus malalignment that underwent medial open-wedge HTO were analyzed pre- and postoperatively. Three investigators and an AI software using deep learning algorithms (LAMA{\texttrademark}, ImageBiopsy Lab, Vienna, Austria) evaluated the hip--knee--ankle angle (HKA), mechanical axis deviation (MAD), joint line convergence angle (JLCA), medial proximal tibial angle (MPTA), and mechanical lateral distal femoral angle (mLDFA). All measurements were performed twice and the performance of the AI software was compared with individual human readers using a Bayesian mixed model. In addition, the inter-observer intraclass correlation coefficient (ICC) for inter-observer reliability was evaluated by comparing measurements from manual readers. The intra-reader variability for manual measurements and the AI-based software was evaluated using the intra-observer ICC. Results: Initial varus malalignment was corrected to slight valgus alignment after HTO. Measured by the AI algorithm and manually HKA (5.36° $\pm$ 3.03° and 5.47° $\pm$ 2.90° to $-$ 0.70 $\pm$ 2.34 and $-$ 0.54 $\pm$ 2.31), MAD (19.38~mm $\pm$ 11.39~mm and 20.17~mm $\pm$ 10.99~mm to $-$ 2.68 $\pm$ 8.75 and $-$ 2.10 $\pm$ 8.61) and MPTA (86.29° $\pm$ 2.42° and 86.08° $\pm$ 2.34° to 91.6 $\pm$ 3.0 and 91.81 $\pm$ 2.54) changed significantly from pre- to postoperative, while JLCA and mLDFA were not altered. The fully automated AI-based analyses showed no significant differences for all measurements compared with manual reads neither in native preoperative radiographs nor postoperatively after HTO. Mean absolute differences between the AI-based software and mean manual observer measurements were 0.5° or less for all measurements. Inter-observer ICCs for manual measurements were good to excellent for all measurements, except for JLCA, which showed moderate inter-observer ICCs. Intra-observer ICCs for manual measurements were excellent for all measurements, except for JLCA and for MPTA postoperatively. For the AI-aided analyses, repeated measurements showed entirely consistent results for all measurements with an intra-observer ICC of 1.0. Conclusions: The AI-based software can provide fully automated analyses of native long-leg radiographs in patients with varus malalignment and after HTO with great accuracy and reproducibility and could support clinical workflows. Level of evidence: Diagnostic study, Level III. {\copyright} 2023, The Author(s).},
 author = {Stotter, C. and Klestil, T. and Chen, K. and Hummer, A. and Salzlechner, C. and Angele, P. and Nehrer, S.},
 year = {2023},
 title = {Artificial intelligence-based analyses of varus leg alignment and after high tibial osteotomy show high accuracy and reproducibility},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176782211&doi=10.1007%2fs00167-023-07644-0&partnerID=40&md5=98e9c362d43f3cff9b7c77349387621d},
 keywords = {ML implementation},
 pages = {5885--5895},
 volume = {31},
 number = {12},
 journal = {Knee Surgery, Sports Traumatology, Arthroscopy},
 doi = {10.1007/s00167-023-07644-0},
 file = {Stotter, Klestil et al. 2023 - Artificial intelligence-based analyses of varus:Attachments/Stotter, Klestil et al. 2023 - Artificial intelligence-based analyses of varus.pdf:application/pdf}
}


@article{Sun.2023,
 abstract = {The precise prediction of concrete compressive strength is essential for ensuring safe and reliable infrastructure design and construction. However, traditional empirical models often struggle to accurately predict compressive strength due to the complex nonlinear relationship between concrete properties and target strength. This study introduces an AutoML-SHAP (Automatic Machine Learning - SHapley Additive exPlanations) strategy, designed to automatically predict the compressive strength of concrete and provide insightful interpretations of the predictive outcomes. The AutoML model uses K-fold bagging and multilayer stacking to automate model selection and hyperparameter tuning. The integration of AutoML and SHAP offers synergistic benefits, facilitating the development of a precise, efficient, and comprehensively interpretable model. Results demonstrate that AutoML-SHAP model outperforms other machine learning models for predicting compressive strength without human intervention. The AutoML model is automatically established within 174 s and exhibits comparable predictive performance with R2 = 0.96, RMSE = 3.63, and MAE = 2.41. SHAP provides a global explanation of the impact of mixing parameters on compressive strength, and a local explanation of feature contribution to each prediction, making the process transparent and reliable. Feature dependence analysis reveals the influence tendency of mixing parameters on strength. {\copyright} 2023},
 author = {Sun, B. and Cui, W. and Liu, G. and Zhou, B. and Zhao, W.},
 year = {2023},
 title = {A hybrid strategy of AutoML and SHAP for automated and explainable concrete strength prediction},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168416019&doi=10.1016%2fj.cscm.2023.e02405&partnerID=40&md5=2727800864ee5d742e9e3867265691eb},
 keywords = {ML implementation},
 volume = {19},
 journal = {Case Studies in Construction Materials},
 doi = {10.1016/j.cscm.2023.e02405},
 file = {Sun, Cui et al 2023 - A hybrid strategy of AutoML:Attachments/Sun, Cui et al 2023 - A hybrid strategy of AutoML.pdf:application/pdf}
}


@proceedings{Sun.2023b,
 abstract = {Automated machine learning (AutoML) is envisioned to make ML techniques accessible to ordinary users. Recent work has investigated the role of humans in enhancing AutoML functionality throughout a standard ML workflow. However, it is also critical to understand how users adopt existing AutoML solutions in complex, real-world settings from a holistic perspective. To fill this gap, this study conducted semi-structured interviews of AutoML users (N = 19) focusing on understanding (1) the limitations of AutoML encountered by users in their real-world practices, (2) the strategies users adopt to cope with such limitations, and (3) how the limitations and workarounds impact their use of AutoML. Our findings reveal that users actively exercise user agency to overcome three major challenges arising from customizability, transparency, and privacy. Furthermore, users make cautious decisions about whether and how to apply AutoML on a case-by-case basis. Finally, we derive design implications for developing future AutoML solutions. {\copyright} 2023 ACM.},
 year = {2023},
 title = {AutoML in The Wild: Obstacles, Workarounds, and Expectations},
 keywords = {Empirical Study},
 editor = {Sun, Y. and Song, Q. and Gui, X. and Ma, F. and Wang, T.},
 doi = {10.1145/3544548.3581082},
 file = {Sun, Song et al (Hg) 2023 - AutoML in The Wild:Attachments/Sun, Song et al (Hg) 2023 - AutoML in The Wild.pdf:application/pdf}
}


@article{Sveric.2023,
 abstract = {Left ventricular ejection fraction (LVEF) is a key parameter in evaluating left ventricular (LV) function using echocardiography (Echo), but its manual measurement by the modified biplane Simpson (MBS) method is time consuming and operator dependent. We investigated the feasibility of a server-based, commercially available and ready-to use-artificial intelligence (AI) application based on convolutional neural network methods that integrate fully automatic view selection and measurement of LVEF from an entire Echo exam into a single workflow. We prospectively enrolled 1083 consecutive patients who had been referred to Echo for diagnostic or therapeutic purposes. LVEF was measured independently using MBS and AI. Test--retest variability was assessed in 40 patients. The reliability, repeatability, and time efficiency of LVEF measurements were compared between the two methods. Overall, 889 Echos were analyzed by cardiologists with the MBS method and by the AI. Over the study period of 10 weeks, the feasibility of both automatic view classification and seamlessly measured LVEF rose to 81{\%} without user involvement. LVEF, LV end-diastolic and end-systolic volumes correlated strongly between MBS and AI (R = 0.87, 0.89 and 0.93, p {\textless} 0.001 for all) with a mean bias of +4.5{\%} EF, $-$12 mL and $-$11 mL, respectively, due to impaired image quality and the extent of LV function. Repeatability and reliability of LVEF measurement (n = 40, test--retest) by AI was excellent compared to MBS (coefficient of variation: 3.2{\%} vs. 5.9{\%}), although the median analysis time of the AI was longer than that of the operator-dependent MBS method (258 s vs. 171 s). This AI has succeeded in identifying apical LV views and measuring EF in one workflow with comparable results to the MBS method and shows excellent reproducibility. It offers realistic perspectives for fully automated AI-based measurement of LVEF in routine clinical settings. {\copyright} 2023 by the authors.},
 author = {Sveric, K. M. and Botan, R. and Dindane, Z. and Winkler, A. and Nowack, T. and Heitmann, C. and Schleu{\ss}ner, L. and Linke, A.},
 year = {2023},
 title = {Single-Site Experience with an Automated Artificial Intelligence Application for Left Ventricular Ejection Fraction Measurement in Echocardiography},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152515694&doi=10.3390%2fdiagnostics13071298&partnerID=40&md5=4a9ac2f73072cae1874f7859b57ed4c4},
 keywords = {ML implementation},
 volume = {13},
 number = {7},
 journal = {Diagnostics},
 doi = {10.3390/diagnostics13071298},
 file = {Sveric, Botan et al. 2023 - Single-Site Experience with an Automated:Attachments/Sveric, Botan et al. 2023 - Single-Site Experience with an Automated.pdf:application/pdf}
}


@article{TamayoVera.2024,
 abstract = {The interplay of machine learning (ML) and deep learning (DL) within the agroclimatic domain is pivotal for addressing the multifaceted challenges posed by climate change on agriculture. This paper embarks on a systematic review to dissect the current utilization of ML and DL in agricultural research, with a pronounced emphasis on agroclimatic impacts and adaptation strategies. Our investigation reveals a dominant reliance on conventional ML models and uncovers a critical gap in the documentation of methodologies. This constrains the replicability, scalability, and adaptability of these technologies in agroclimatic research. In response to these challenges, we advocate for a strategic pivot toward Automated Machine Learning (AutoML) frameworks. AutoML not only simplifies and standardizes the model development process but also democratizes ML expertise, thereby catalyzing the advancement in agroclimatic research. The incorporation of AutoML stands to significantly enhance research scalability, adaptability, and overall performance, ushering in a new era of innovation in agricultural practices tailored to mitigate and adapt to climate change. This paper underscores the untapped potential of AutoML in revolutionizing agroclimatic research, propelling forward the development of sustainable and efficient agricultural solutions that are responsive to the evolving climate dynamics. {\copyright} 2024 by the authors.},
 author = {Tamayo-Vera, D. and Wang, X. and Mesbah, M.},
 year = {2024},
 title = {A Review of Machine Learning Techniques in Agroclimatic Studies},
 keywords = {Literature Review},
 volume = {14},
 number = {3},
 journal = {Agriculture (Switzerland)},
 file = {Tamayo-Vera, Wang et al. 2024 - A Review of Machine Learning:Attachments/Tamayo-Vera, Wang et al. 2024 - A Review of Machine Learning.pdf:application/pdf}
}


@article{Tan.2021,
 abstract = {Background: By 2050, almost 5 billion people globally are projected to have myopia, of whom 20{\%} are likely to have high myopia with clinically significant risk of sight-threatening complications such as myopic macular degeneration. These are diagnoses that typically require specialist assessment or measurement with multiple unconnected pieces of equipment. Artificial intelligence (AI) approaches might be effective for risk stratification and to identify individuals at highest risk of visual loss. However, unresolved challenges for AI medical studies remain, including paucity of transparency, auditability, and traceability. Methods: In this retrospective multicohort study, we developed and tested retinal photograph-based deep learning algorithms for detection of myopic macular degeneration and high myopia, using a total of 226 686 retinal images. First we trained and internally validated the algorithms on datasets from Singapore, and then externally tested them on datasets from China, Taiwan, India, Russia, and the UK. We also compared the performance of the deep learning algorithms against six human experts in the grading of a randomly selected dataset of 400 images from the external datasets. As proof of concept, we used a blockchain-based AI platform to demonstrate the real-world application of secure data transfer, model transfer, and model testing across three sites in Singapore and China. Findings: The deep learning algorithms showed robust diagnostic performance with areas under the receiver operating characteristic curves [AUC] of 0·969 (95{\%} CI 0·959--0·977) or higher for myopic macular degeneration and 0·913 (0·906--0·920) or higher for high myopia across the external testing datasets with available data. In the randomly selected dataset, the deep learning algorithms outperformed all six expert graders in detection of each condition (AUC of 0·978 [0·957--0·994] for myopic macular degeneration and 0·973 [0·941--0·995] for high myopia). We also successfully used blockchain technology for data transfer, model transfer, and model testing between sites and across two countries. Interpretation: Deep learning algorithms can be effective tools for risk stratification and screening of myopic macular degeneration and high myopia among the large global population with myopia. The blockchain platform developed here could potentially serve as a trusted platform for performance testing of future AI models in medicine. Funding: None. {\copyright} 2021 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license},
 author = {Tan, T.-E. and Anees, A. and Chen, C. and Li, S. and Xu, X. and Li, Z. and Xiao, Z. and Yang, Y. and Lei, X. and Ang, M. and Chia, A. and Lee, S. Y. and Wong, E.Y.M. and Yeo, I.Y.S. and Wong, Y. L. and Hoang, Q. V. and Wang, Y. X. and Bikbov, M. M. and Nangia, V. and Jonas, J. B. and Chen, Y.-P. and Wu, W.-C. and Ohno-Matsui, K. and Rim, T. H. and Tham, Y.-C. and Goh, R.S.M. and Lin, H. and Liu, H. and Wang, N. and Yu, W. and Tan, D.T.H. and Schmetterer, L. and Cheng, C.-Y. and Chen, Y. and Wong, C. W. and Cheung, G.C.M. and Saw, S.-M. and Wong, T. Y. and Liu, Y. and Ting, D.S.W.},
 year = {2021},
 title = {Retinal photograph-based deep learning algorithms for myopia and a blockchain platform to facilitate artificial intelligence medical research: a retrospective multicohort study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104341673&doi=10.1016%2fS2589-7500%2821%2900055-8&partnerID=40&md5=799d5c697e83feac7742bd765e9fb24b},
 keywords = {ML implementation},
 pages = {e317-e329},
 volume = {3},
 number = {5},
 journal = {The Lancet Digital Health},
 doi = {10.1016/S2589-7500(21)00055-8}
}


@article{TayebiArasteh.2024,
 abstract = {A knowledge gap persists between machine learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to ChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Following the re-implementation and optimization of the published models, the head-to-head comparison of the ChatGPT ADA-crafted ML models and their respective manually crafted counterparts revealed no significant differences in traditional performance metrics (p $\geq$ 0.072). Strikingly, the ChatGPT ADA-crafted ML models often outperformed their counterparts. In conclusion, ChatGPT ADA offers a promising avenue to democratize ML in medicine by simplifying complex data analyses, yet should enhance, not replace, specialized training and resources, to promote broader applications in medical research and practice. {\copyright} The Author(s) 2024.},
 author = {{Tayebi Arasteh}, S. and Han, T. and Lotfinia, M. and Kuhl, C. and Kather, J. N. and Truhn, D. and Nebelung, S.},
 year = {2024},
 title = {Large language models streamline automated machine learning for clinical studies},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185654701&doi=10.1038%2fs41467-024-45879-8&partnerID=40&md5=3af7e17cf572798b8c9f05825f41ead8},
 keywords = {Technical Review},
 volume = {15},
 number = {1},
 journal = {Nature Communications},
 doi = {10.1038/s41467-024-45879-8},
 file = {Tayebi Arasteh, Han et al. 2024 - Large language models streamline automated:Attachments/Tayebi Arasteh, Han et al. 2024 - Large language models streamline automated.pdf:application/pdf}
}


@article{Tesfaye.2022,
 abstract = {Field-scale prediction methods that use remote sensing are significant in many global projects; however, the existing methods have several limitations. In particular, the characteristics of smallholder systems pose a unique challenge in the development of reliable prediction methods. Therefore, in this study, a fast and reproducible new approach to wheat prediction is developed by combining predictors derived from optical (Sentinel-2) and radar (Sentinel-1) sensors using a diverse set of machine learning and deep learning methods under a small dataset domain. This study takes place in the wheat belt region of Ethiopia and evaluates forty-two predictors that represent the major vegetation index categories of green, water, chlorophyll, dry biomass, and VH polarization SAR indices. The study also applies field-collected agronomic data from 165 farm fields for training and validation. According to results, compared to other methods, a combined automated machine learning (AutoML) approach with a generalized linear model (GLM) showed higher performance. AutoML, which reduces training time, delivered ten influential parameters. For the combined approach, the mean RMSE of wheat yield was from 0.84 to 0.98 ton/ha using ten predictors from the test dataset, achieving a 99{\%} confidence interval. It also showed a correlation coefficient as high as 0.69 between the estimated yield and measured yield, and it was less sensitive to the small datasets used for model training and validation. A deep neural network with three hidden layers using the ten influential parameters was the second model. For this model, the mean RMSE of wheat yield was between 1.31 and 1.36 ton/ha on the test dataset, achieving a 99{\%} confidence interval. This model used 55 neurons with respective values of 0.1, 0.5, and 1 $\times$ 10$-$4 for the hidden dropout ratio, input dropout ratio, and l2 regularization. The approaches implemented in this study are fast and reproducible and beneficial to predict yield at scale. These approaches could be adapted to predict grain yields of other cereal crops grown under smallholder systems in similar global production systems. {\copyright} 2022 by the authors.},
 author = {Tesfaye, A. A. and Awoke, B. G. and Sida, T. S. and Osgood, D. E.},
 year = {2022},
 title = {Enhancing Smallholder Wheat Yield Prediction through Sensor Fusion and Phenology with Machine Learning and Deep Learning Methods},
 keywords = {ML implementation},
 volume = {12},
 number = {9},
 journal = {Agriculture (Switzerland)},
 file = {Tesfaye, Awoke et al. 2022 - Enhancing Smallholder Wheat Yield Prediction:Attachments/Tesfaye, Awoke et al. 2022 - Enhancing Smallholder Wheat Yield Prediction.pdf:application/pdf}
}


@article{Tessler.2023,
 abstract = {Background: Artificial intelligence (AI) is broadly defined as the ability of machines to apply human-like reasoning to problem solving. Recent years have seen a rapid growth of AI in many disciplines. This review will focus on AI applications in the assessment of thyroid nodules. Summary: AI encompasses two related computational techniques: machine learning, in which computers learn by observing data provided by humans, and deep learning, which employs neural networks that mimic brain structure and function to analyze data. Some experts believe the way AI systems reach a conclusion should be transparent, or explainable, while others disagree. Most AI platforms in thyroid disease have focused on malignancy risk stratification of nodules. To date, four have been approved by the United States Food and Drug Administration. While the results of validation studies have been mixed, there is ample evidence that AI can exceed the performance of some humans, particularly physicians with less experience. AI has also been applied to assessment of lymph nodes and cytopathology specimens. Conclusions: Adoption of AI in thyroid disease will require vendors to demonstrate that their software works as intended, is readily usable in real-world settings, and is cost effective. AI platforms that perform best in head-to-head comparisons will dominate and spur wider adoption. ª Mary Ann Liebert, Inc.},
 author = {Tessler, F. N. and Thomas, J.},
 year = {2023},
 title = {Artificial Intelligence for Evaluation of Thyroid Nodules: A Primer},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148307344&doi=10.1089%2fthy.2022.0560&partnerID=40&md5=5b2b2d89dbcd9a28d61e4f359d582803},
 keywords = {Literature Review},
 pages = {150--158},
 volume = {33},
 number = {2},
 journal = {Thyroid},
 doi = {10.1089/thy.2022.0560}
}


@article{Themistocleous.2021,
 abstract = {Background: The classification of patients with primary progressive aphasia (PPA) into variants is time-consuming, costly, and requires combined expertise by clinical neurologists, neuropsychologists, speech pathologists, and radiologists. Objective: The aim of the present study is to determine whether acoustic and linguistic variables provide accurate classification of PPA patients into one of three variants: nonfluent PPA, semantic PPA, and logopenic PPA. Methods: In this paper, we present a machine learning model based on deep neural networks (DNN) for the subtyping of patients with PPA into three main variants, using combined acoustic and linguistic information elicited automatically via acoustic and linguistic analysis. The performance of the DNN was compared to the classification accuracy of Random Forests, Support Vector Machines, and Decision Trees, as well as to expert clinicians' classifications. Results: The DNN model outperformed the other machine learning models as well as expert clinicians' classifications with 80{\%} classification accuracy. Importantly, 90{\%} of patients with nfvPPA and 95{\%} of patients with lvPPA was identified correctly, providing reliable subtyping of these patients into their corresponding PPA variants. Conclusion: We show that the combined speech and language markers from connected speech productions can inform variant subtyping in patients with PPA. The end-to-end automated machine learning approach we present can enable clinicians and researchers to provide an easy, quick, and inexpensive classification of patients with PPA. {\copyright} 2021 - IOS Press. All rights reserved.},
 author = {Themistocleous, C. and Ficek, B. and Webster, K. and den Ouden, D.-B. and Hillis, A. E. and Tsapkini, K.},
 year = {2021},
 title = {Automatic subtyping of individuals with primary progressive aphasia},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100580168&doi=10.3233%2fJAD-201101&partnerID=40&md5=a2634b59ef72d8c078c55cf9ad7f926f},
 keywords = {ML implementation},
 pages = {1185--1194},
 volume = {79},
 number = {3},
 journal = {Journal of Alzheimer's Disease},
 doi = {10.3233/JAD-201101},
 file = {Themistocleous, Ficek et al. 2021 - Automatic subtyping of individuals:Attachments/Themistocleous, Ficek et al. 2021 - Automatic subtyping of individuals.pdf:application/pdf}
}


@article{TimpeLaughlin.2023,
 abstract = {Accomplishing oral interactive workplace tasks requires various language abilities, including pragmatics. While technology-mediated tasks are thought to offer many possibilities for teaching and assessing second language (L2) pragmatics, their effectiveness -- especially those facilitated by an AI agent (artificial intelligence agent) -- remains to be explored. This study investigated how 47 tertiary-level learners of English as a second language (ESL) performed on an oral interactive task that required them to make requests to their boss in two distinct modalities. Each participant completed the same task with a fully automated AI agent and with a human interlocutor in a face-to-face format. Findings showed that both modalities elicited language use relevant to the pragmatics target. However, fully automated interactions were found to be more transactional, while face-to-face interactions were more functionally oriented (e.g. more frequent/varied supportive moves). Although fully automated interactive tasks may be useful for eliciting requests, replicating human-to-human interactions remains a challenge. {\copyright} The Author(s) 2023.},
 author = {Timpe-Laughlin, V. and Dombi, J. and Sydorenko, T. and Sasayama, S.},
 year = {2023},
 title = {L2 learners' pragmatic output in a face-to-face vs. a computer-guided role-play task: Implications for TBLT},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166904877&doi=10.1177%2f13621688231188310&partnerID=40&md5=62a08462d92e8ed0b6f8d6e54745ea65},
 keywords = {Experiment},
 journal = {Language Teaching Research},
 doi = {10.1177/13621688231188310},
 file = {Timpe-Laughlin, Dombi et al. 2023 - L2 learners' pragmatic output:Attachments/Timpe-Laughlin, Dombi et al. 2023 - L2 learners' pragmatic output.pdf:application/pdf}
}


@inproceedings{Toan.2021,
 abstract = {One of the fields that has detonated lately dependent on the AI platform is humanoid robots. The facial expressions of robots, then again, are getting less and less attention. The design of a robot head that communicates a wide range of human-like expressions is described in this paper. The eye-eyelid, lip-jaw, and neck were the three main parts of the robot head under investigation. Mechanical structure of human head such as number of degrees of freedom, size, ability of joints, and major muscle groups to create movements in each part are determined based on anatomy. Then, facial expressions are analyzed dependent on AUs to find out reasonable control points. The mechanical structures of each part were designed to try to replicate the movement and expression as closely as expected. Additionally, proportions, sizes, distances of the designs, such as CoP-CoP, tr-n, n-sn, and so on, must closely resemble those of a human head. The robot head in this article is designed and modeled on an Asian human head. The result of the design, the head section is made up of three mechanisms connected by a shaft that runs through the entire head. Through the experimental process, the robot head can perform 6 of the basic expressions and movements like a real human head.  {\copyright} 2021 ICROS.},
 author = {Toan, N. K. and Thuan, L. D. and Long, L. B. and Thinh, N. T.},
 title = {Mechanical Design of Robot Head with Human-like Emotions},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124186839&doi=10.23919%2fICCAS52745.2021.9649916&partnerID=40&md5=457d322d5973d3befe3149e9cab3f6e8},
 keywords = {Artefact Design},
 pages = {1720--1725},
 year = {2021},
 doi = {10.23919/ICCAS52745.2021.9649916}
}


@article{Togo.2023,
 abstract = {Herein, a robust and reproducible eXplainable Artificial Intelligence (XAI) approach is presented, which allows prediction of developmental toxicity, a challenging human-health endpoint in toxicology. The application of XAI as an alternative method is of the utmost importance with developmental toxicity being one of the most animal-intensive areas of regulatory toxicology. In this work, the established CAESAR (Computer Assisted Evaluation of industrial chemical Substances According to Regulations) training set made of 234 chemicals for model learning is employed. Two test sets, including as a whole 585 chemicals, were instead used for validation and generalization purposes. The proposed framework favorably compares with the state-of-the-art approaches in terms of accuracy, sensitivity, and specificity, thus resulting in a reliable support system for developmental toxicity ensuring informativeness, uncertainty estimation, generalization, and transparency. Based on the eXtreme Gradient Boosting (XGB) algorithm, our predictive model provides easy interpretative keys based on specific molecular descriptors and structural alerts enabling one to distinguish toxic and nontoxic chemicals. Inspired by the Organisation for Economic Co-operation and Development (OECD) principles for the validation of Quantitative Structure-Activity Relationships (QSARs) for regulatory purposes, the results are summarized in a standard report in portable document format, enclosing also details concerned with a density-based model applicability domain and SHAP (SHapley Additive exPlanations) explainability, the latter particularly useful to better understand the effective roles played by molecular features. Notably, our model has been implemented in TIRESIA (Toxicology Intelligence and Regulatory Evaluations for Scientific and Industry Applications), a free of charge web platform available at http://tiresia.uniba.it. {\copyright} 2022 American Chemical Society.},
 author = {Togo, M. V. and Mastrolorito, F. and Ciriaco, F. and Trisciuzzi, D. and Tondo, A. R. and Gambacorta, N. and Bellantuono, L. and Monaco, A. and Leonetti, F. and Bellotti, R. and Altomare, C. D. and Amoroso, N. and Nicolotti, O.},
 year = {2023},
 title = {TIRESIA: An eXplainable Artificial Intelligence Platform for Predicting Developmental Toxicity},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144342381&doi=10.1021%2facs.jcim.2c01126&partnerID=40&md5=4d183974248f4f98848663ee4ed54d81},
 keywords = {Artefact Design;ML implementation},
 pages = {56--66},
 volume = {63},
 number = {1},
 journal = {Journal of Chemical Information and Modeling},
 doi = {10.1021/acs.jcim.2c01126}
}


@article{Tornede.2023,
 abstract = {Automated machine learning (AutoML) strives for the automatic configuration of machine learning algorithms and their composition into an overall (software) solution - a machine learning pipeline - tailored to the learning task (dataset) at hand. Over the last decade, AutoML has developed into an independent research field with hundreds of contributions. At the same time, AutoML is being criticized for its high resource consumption as many approaches rely on the (costly) evaluation of many machine learning pipelines, as well as the expensive large-scale experiments across many datasets and approaches. In the spirit of recent work on Green AI, this paper proposes Green AutoML, a paradigm to make the whole AutoML process more environmentally friendly. Therefore, we first elaborate on how to quantify the environmental footprint of an AutoML tool. Afterward, different strategies on how to design and benchmark an AutoML tool w.r.t. their {\textquotedbl}greenness{\textquotedbl}, i.e., sustainability, are summarized. Finally, we elaborate on how to be transparent about the environmental footprint and what kind of research incentives could direct the community in a more sustainable AutoML research direction. As part of this, we propose a sustainability checklist to be attached to every AutoML paper featuring all core aspects of Green AutoML.  {\copyright} 2023 The Authors.},
 author = {Tornede, T. and Tornede, A. and Hanselle, J. and Mohr, F. and Wever, M. and H{\"u}llermeier, E.},
 year = {2023},
 title = {Towards Green Automated Machine Learning: Status Quo and Future Directions},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162139256&doi=10.1613%2fjair.1.14340&partnerID=40&md5=481642dd2a5d19af0d537a99530d66d7},
 keywords = {Artefact Design},
 pages = {427--457},
 volume = {77},
 journal = {Journal of Artificial Intelligence Research},
 doi = {10.1613/jair.1.14340},
 file = {Tornede, Tornede et al. 2023 - Towards Green Automated Machine Learning:Attachments/Tornede, Tornede et al. 2023 - Towards Green Automated Machine Learning.pdf:application/pdf}
}


@inproceedings{Tosic.2023,
 abstract = {Industry 4.0 has been mainly driven by IoT devices and artificial intelligence developments rising heterogeneity of the data acquired by sensing devices as well as data from existing legacy systems (such as ERP) the crucial for digital transformation. Until recently, migration of enterprise applications to Cloud has been considered the only viable long-term solution. However, after hidden infrastructure costs of the Cloud-only based approach have been discovered, a number of businesses have begun considering hybrid Cloud-Edge architectures where Micro-Services Architectures (MSA) on backend are complemented with Micro-Font-End (MFE) applications. However, the architecture must be very carefully optimized in order to avoid high risks and costs due to increased system's complexity. In this paper, a semantic-driven approach based on Enterprise Knowledge Graph (EKG) and ontologies with their automated mapping is introduced in order to manage the complexity. Ontologies are adopted for automated, low-code approach to composition and deployment of MFE components targeting enterprise productivity applications. MFE applications generated this way are built upon Semantic Micro Services backend that can transparently be distributed between Cloud and Edge. Our approach is illustrated on the case study for semantic annotation of manufacturing area which utilizes a shared marketplace component for IoT-based indoor positioning. Copyright {\copyright} 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
 author = {Tosic, M. and Petrovic, N. and Tosic, O.},
 title = {Semantic Micro-Front-End Approach to Enterprise Knowledge Graph Applications Development},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179583436&doi=10.5220%2f0012236200003584&partnerID=40&md5=d4b0be954956abc740fd6b7f4a9225de},
 keywords = {Artefact Design;Case Study},
 pages = {488--495},
 year = {2023},
 doi = {10.5220/0012236200003584}
}


@inproceedings{Trifan.2023,
 abstract = {Despite recent progress made in the theory and practice of Artificial Intelligence (AI), there is still a lack of tools and algorithms for the design and implementation of Neural Networks (NN) capable of explaining how the processes producing decisions are made. Therefore there is a need for devising new algorithms for endowing NN to address the transparency and accountancy in the use of AI, especially in areas where AI decisions have a significant impact on people's lives. This led to the research and investigation of algorithms for explainable AI (XAI), a field which is at the very beginning of its activities. In this paper, a Finite State Machine (FSM) approach is applied to the design and implementation of a blend of Machine Learning (ML) frameworks capable to auto-build, auto-train, and auto-deploy AI models using abstract representations. It is demonstrated, in this paper, that FSMs can be generated and applied at the design of engineering and AI automation tools such that platforms such as AutoML, AutoGluon and others can be controlled at their turn. The automation of AI models is achieved by parsing the FSM states, which results in the creation of Python artifacts that can be executed. The proposed FSM controls the Automated AI Platforms (AAIP) based on the decisions made by the FSM Controller (FSMC). The FSMC is the root of the FSM graph possessing in its structure the rules driving the NNs. The parsed diagrams are nodes of the diagrammatic reasoning (DR) algorithms. These algorithms are capable of selecting the proper FSM and PlantUML diagrams from a database of such diagrams. An implementation of the combination of FSM, FSMC and RNN is described in this paper plainly illustrating the advantages of the FSM approach to AI. The PlantUML diagrams used to generate the figures in this article can be found at the following URL 1.1https://github.com/mirceat/FSM2ML-diagrams {\copyright} 2023 IEEE.},
 author = {Trifan, M. and Ionescu, B. and Ionescu, D.},
 title = {A Combined Finite State Machine and PlantUML Approach to Machine Learning Applications},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165161770&doi=10.1109%2fSACI58269.2023.10158543&partnerID=40&md5=9054592b73d78a3f0b4c0a4d7b02c131},
 keywords = {Artefact Design},
 pages = {631--636},
 year = {2023},
 doi = {10.1109/SACI58269.2023.10158543},
 file = {Trifan, Ionescu et al 2023 - A Combined Finite State Machine:Attachments/Trifan, Ionescu et al 2023 - A Combined Finite State Machine.pdf:application/pdf}
}


@article{Tsiakmaki.2020,
 abstract = {Educational Data Mining (EDM) has emerged over the last two decades, concerning with the development and implementation of data mining methods in order to facilitate the analysis of vast amounts of data originating from a wide variety of educational contexts. Predicting students' progression and learning outcomes, such as dropout, performance and course grades, is regarded among the most important tasks of the EDM field. Therefore, applying appropriate machine learning algorithms for building accurate predictive models is of outmost importance for both educators and data scientists. Considering the high-dimensional input space and the complexity of machine learning algorithms, the process of building accurate and robust learning models requires advanced data science skills, while is time-consuming and error-prone in most cases. In addition, choosing the proper method for a given problem formulation and configuring the optimal parameters' values for a specific model is a demanding task, whilst it is often very difficult to understand and explain the produced results. In this context, the main purpose of the present study is to examine the potential use of advanced machine learning strategies on educational settings from the perspective of hyperparameter optimization. More specifically, we investigate the effectiveness of automated Machine Learning (autoML) for the task of predicting students' learning outcomes based on their participation in online learning platforms. At the same time, we limit the search space to tree-based and rule-based models in order to achieving transparent and interpretable results. To this end, a plethora of experiments were carried out, revealing that autoML tools achieve consistently superior results. Hopefully our work will help nonexpert users (e.g., educators and instructors) in the field of EDM to conduct experiments with appropriate automated parameter configurations, thus achieving highly accurate and comprehensible results. {\copyright} 2019 by the authors.},
 author = {Tsiakmaki, M. and Kostopoulos, G. and Kotsiantis, S. and Ragos, O.},
 year = {2020},
 title = {Implementing autoML in educational data mining for prediction tasks},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079293035&doi=10.3390%2fapp10010090&partnerID=40&md5=494d12cf2ff360af5b832e0cbf9a5b1e},
 keywords = {Experiment;ML implementation;Technical Review},
 volume = {10},
 number = {1},
 journal = {Applied Sciences (Switzerland)},
 doi = {10.3390/app10010090},
 file = {Tsiakmaki, Kostopoulos et al. 2020 - Implementing autoML in educational data:Attachments/Tsiakmaki, Kostopoulos et al. 2020 - Implementing autoML in educational data.pdf:application/pdf}
}


@article{Tsiakmaki.2021,
 abstract = {Predicting students' learning outcomes is one of the main topics of interest in the area of Educational Data Mining and Learning Analytics. To this end, a plethora of machine learning methods has been successfully applied for solving a variety of predictive problems. However, it is of utmost importance for both educators and data scientists to develop accurate learning models at low cost. Fuzzy logic constitutes an appropriate approach for building models of high performance and transparency. In addition, active learning reduces both the time and cost of labeling effort, by exploiting a small set of labeled data along with a large set of unlabeled data in the most efficient way. In addition, choosing the proper method for a given problem formulation and configuring the optimal parameter setting is a demanding task, considering the high-dimensional input space and the complexity of machine learning algorithms. As such, exploring the potential of automated machine learning (autoML) strategies from the perspective of machine learning adeptness is important. In this context, the present study introduces a fuzzy-based active learning method for predicting students' academic performance which combines, in a modular way, autoML practices. A lot of experiments was carried out, revealing the efficiency of the proposed method for the accurate prediction of students at risk of failure. These insights may have the potential to support the learning experience and be useful the wider science of learning. {\copyright} 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
 author = {Tsiakmaki, M. and Kostopoulos, G. and Kotsiantis, S. and Ragos, O.},
 year = {2021},
 title = {Fuzzy-based active learning for predicting student academic performance using autoML: a step-wise approach},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105859242&doi=10.1007%2fs12528-021-09279-x&partnerID=40&md5=161826cbbce5376b1961f19840129746},
 keywords = {Artefact Design;Experiment},
 pages = {635--667},
 volume = {33},
 number = {3},
 journal = {Journal of Computing in Higher Education},
 doi = {10.1007/s12528-021-09279-x},
 file = {Tsiakmaki, Kostopoulos et al. 2021 - Fuzzy-based active learning for predicting:Attachments/Tsiakmaki, Kostopoulos et al. 2021 - Fuzzy-based active learning for predicting.pdf:application/pdf}
}


@article{Turton.2024,
 abstract = {Oral health is an essential part of healthy aging and very little data exists around the disease burden for older adults in a long-term care setting. The aim of this scoping review was to estimate the disease burden of dental caries, periodontal disease, and tooth loss among older adults in Long-Term Care (LTC). This scoping review was conducted in accordance with the Joanna Briggs Institute methodology. A detailed strategy was used to conduct a comprehensive search of electronic databases: PubMed, Embase, and Dentistry and Oral Sciences Source (DOSS). The Rayyan AI platform was used to screen abstracts for assessment by one of five co-investigators. Results indicate that only one in three might have a functional dentition upon entry into LTC, and among those who are dentate, most might expect to develop at least one new coronal and one new root caries lesion each year. There is a need to better document the disease experiences of this group to tailor approaches to care that might reduce the avoidable suffering as a result of dental caries and periodontal disease. {\copyright} 2024 by the authors.},
 author = {Turton, B. and Alqunaybit, G. and Tembhe, A. and Qari, A. and Rawal, K. and Mandel, E. and Calabrese, J. and Henshaw, M.},
 year = {2024},
 title = {Estimation of Oral Disease Burden among Older Adults in LTC: A Scoping Review},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188793995&doi=10.3390%2fijerph21030248&partnerID=40&md5=f585b15cbbc76aab6b536ef14b82e948},
 keywords = {Literature Review},
 volume = {21},
 number = {3},
 journal = {International Journal of Environmental Research and Public Health},
 doi = {10.3390/ijerph21030248},
 file = {Turton, Alqunaybit et al. 2024 - Estimation of Oral Disease Burden:Attachments/Turton, Alqunaybit et al. 2024 - Estimation of Oral Disease Burden.pdf:application/pdf}
}


@article{Twa.2005,
 abstract = {Purpose. The volume and complexity of data produced during videokeratography examinations present a challenge of interpretation. As a consequence, results are often analyzed qualitatively by subjective pattern recognition or reduced to comparisons of summary indices. We describe the application of decision tree induction, an automated machine learning classification method, to discriminate between normal and keratoconic corneal shapes in an objective and quantitative way. We then compared this method with other known classification methods. Methods. The corneal surface was modeled with a seventh-order Zernike polynomial for 132 normal eyes of 92 subjects and 112 eyes of 71 subjects diagnosed with keratoconus. A decision tree classifier was induced using the C4.5 algorithm, and its classification performance was compared with the modified Rabinowitz-McDonnell index, Schwiegerling's Z3 index (Z3), Keratoconus Prediction Index (KPI), KISA{\%}, and Cone Location and Magnitude Index using recommended classification thresholds for each method. We also evaluated the area under the receiver operator characteristic (ROC) curve for each classification method. Results. Our decision tree classifier performed equal to or better than the other classifiers tested: accuracy was 92{\%} and the area under the ROC curve was 0.97. Our decision tree classifier reduced the information needed to distinguish between normal and keratoconus eyes using four of 36 Zernike polynomial coefficients. The four surface features selected as classification attributes by the decision tree method were inferior elevation, greater sagittal depth, oblique toricity, and trefoil. Conclusion. Automated decision tree classification of corneal shape through Zernike polynomials is an accurate quantitative method of classification that is interpretable and can be generated from any instrument platform capable of raw elevation data output. This method of pattern classification is extendable to other classification problems. Copyright {\copyright} 2005 American Academy of Optometry.},
 author = {Twa, M. D. and Parthasarathy, S. and Roberts, C. and Mahmoud, A. M. and Raasch, T. W. and Bullimore, M. A.},
 year = {2005},
 title = {Automated decision tree classification of corneal shape},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-29344464132&doi=10.1097%2f01.opx.0000192350.01045.6f&partnerID=40&md5=779a6262fba9f68bcef4d3fdd4eac85c},
 keywords = {ML implementation},
 pages = {1038--1046},
 volume = {82},
 number = {12},
 journal = {Optometry and Vision Science},
 doi = {10.1097/01.opx.0000192350.01045.6f},
 file = {Twa, Parthasarathy et al. 2005 - Automated decision tree classification:Attachments/Twa, Parthasarathy et al. 2005 - Automated decision tree classification.pdf:application/pdf}
}


@article{Tzachor.2022,
 abstract = {Global agriculture is poised to benefit from the rapid advance and diffusion of artificial intelligence (AI) technologies. AI in agriculture could improve crop management and agricultural productivity through plant phenotyping, rapid diagnosis of plant disease, efficient application of agrochemicals and assistance for growers with location-relevant agronomic advice. However, the ramifications of machine learning (ML) models, expert systems and autonomous machines for farms, farmers and food security are poorly understood and under-appreciated. Here, we consider systemic risk factors of AI in agriculture. Namely, we review risks relating to interoperability, reliability and relevance of agricultural data, unintended socio-ecological consequences resulting from ML models optimized for yields, and safety and security concerns associated with deployment of ML platforms at scale. As a response, we suggest risk-mitigation measures, including inviting rural anthropologists and applied ecologists into the technology design process, applying frameworks for responsible and human-centred innovation, setting data cooperatives for improved data transparency and ownership rights, and initial deployment of agricultural AI in digital sandboxes. {\copyright} 2022, Springer Nature Limited.},
 author = {Tzachor, A. and Devare, M. and King, B. and Avin, S. and {{\'O} h{\'E}igeartaigh}, S.},
 year = {2022},
 title = {Responsible artificial intelligence in agriculture requires systemic understanding of risks and externalities},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125783258&doi=10.1038%2fs42256-022-00440-4&partnerID=40&md5=bf972eec7f33c42f55bb6dbd848726c7},
 keywords = {Literature Review},
 pages = {104--109},
 volume = {4},
 number = {2},
 journal = {Nature Machine Intelligence},
 doi = {10.1038/s42256-022-00440-4}
}


@article{vanLeeuwen.2024,
 abstract = {Objectives: To map the clinical use of CE-marked artificial intelligence (AI)--based software in radiology departments in the Netherlands (n = 69) between 2020 and 2022. Materials and methods: Our AI network (one radiologist or AI representative per Dutch hospital organization) received a questionnaire each spring from 2020 to 2022 about AI product usage, financing, and obstacles to adoption. Products that were not listed on www.AIforRadiology.com by July 2022 were excluded from the analysis. Results: The number of respondents was 43 in 2020, 36 in 2021, and 33 in 2022. The number of departments using AI has been growing steadily (2020: 14, 2021: 19, 2022: 23). The diversity (2020: 7, 2021: 18, 2022: 34) and the number of total implementations (2020: 19, 2021: 38, 2022: 68) has rapidly increased. Seven implementations were discontinued in 2022. Four hospital organizations said to use an AI platform or marketplace for the deployment of AI solutions. AI is mostly used to support chest CT (17), neuro CT (17), and musculoskeletal radiograph (12) analysis. The budget for AI was reserved in 13 of the responding centers in both 2021 and 2022. The most important obstacles to the adoption of AI remained costs and IT integration. Of the respondents, 28{\%} stated that the implemented AI products realized health improvement and 32{\%} assumed both health improvement and cost savings. Conclusion: The adoption of AI products in radiology departments in the Netherlands is showing common signs of a developing market. The major obstacles to reaching widespread adoption are a lack of financial resources and IT integration difficulties. Clinical relevance statement: The clinical impact of AI starts with its adoption in daily clinical practice. Increased transparency around AI products being adopted, implementation obstacles, and impact may inspire increased collaboration and improved decision-making around the implementation and financing of AI products. Key Points: • The adoption of artificial intelligence products for radiology has steadily increased since 2020 to at least a third of the centers using AI in clinical practice in the Netherlands in 2022. • The main areas in which artificial intelligence products are used are lung nodule detection on CT, aided stroke diagnosis, and bone age prediction. • The majority of respondents experienced added value (decreased costs and/or improved outcomes) from using artificial intelligence--based software; however, major obstacles to adoption remain the costs and IT-related difficulties. {\copyright} 2023, The Author(s).},
 author = {{van Leeuwen}, K. G. and de Rooij, M. and Schalekamp, S. and {van Ginneken}, B. and Rutten, M.J.C.M.},
 year = {2024},
 title = {Clinical use of artificial intelligence products for radiology in the Netherlands between 2020 and 2022},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166228483&doi=10.1007%2fs00330-023-09991-5&partnerID=40&md5=5d2507c6d8fdb46c33e56c167e993ee6},
 keywords = {Empirical Study},
 pages = {348--354},
 volume = {34},
 number = {1},
 journal = {European Radiology},
 doi = {10.1007/s00330-023-09991-5},
 file = {van Leeuwen, Rooij et al. 2024 - Clinical use of artificial intelligence:Attachments/van Leeuwen, Rooij et al. 2024 - Clinical use of artificial intelligence.pdf:application/pdf}
}


@article{Veiga.2023,
 abstract = {IoT applications with their resource-constrained sensor devices can benefit from adjusting their operations to the phenomena they sense and the environments they operate in, leading to the paradigm of self-adaptive, autonomous, or cognitive IoT. On the other side, current AI deployment platforms focus on the provision and reuse of machine learning models through containers that can be wired together to build new applications. The challenge is that composition mechanisms of the AI platforms, albeit effective due to their simplicity, are in fact too simplistic to support cognitive IoT applications, in which sensor devices also benefit from the machine learning results. Our objective is to perform a gap analysis between the requirements of cognitive IoT applications on the one side and the current functionalities of AI deployment platforms on the other side. In this work, we provide an overview of the paradigms in AI deployment platforms and the requirements of cognitive IoT applications. We study a use case for person counting in a skiing area through camera sensors, and how this use case benefits from letting the IoT sensors have access to operational knowledge in the form of visual attention models. We describe the implementation of the IoT application using an AI deployment platform, analyze its shortcomings, and necessary workarounds. From the use case, we identify and generalize five gaps that limit the usage of deployment platforms: the transparent management of multiple instances of components, a more seamless integration with IoT devices, explicit definition of data flow triggers, and the availability of templates for cognitive IoT architectures and reuse below the top-level. {\copyright} 2022 The Author(s)},
 author = {Veiga, T. and Asad, H. A. and Kraemer, F. A. and Bach, K.},
 year = {2023},
 title = {Towards containerized, reuse-oriented AI deployment platforms for cognitive IoT applications},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145262652&doi=10.1016%2fj.future.2022.12.029&partnerID=40&md5=c7eefd29e34a5411c764f020b81b5f49},
 keywords = {Case Study;Literature Review},
 pages = {4--13},
 volume = {142},
 journal = {Future Generation Computer Systems},
 doi = {10.1016/j.future.2022.12.029},
 file = {Veiga, Asad et al. 2023 - Towards containerized:Attachments/Veiga, Asad et al. 2023 - Towards containerized.pdf:application/pdf}
}


@article{Vitsios.2020,
 abstract = {Access to large-scale genomics datasets has increased the utility of hypothesis-free genome-wide analyses. However, gene signals are often insufficiently powered to reach experiment-wide significance, triggering a process of laborious triaging of genomic-association-study results. We introduce mantis-ml, a multi-dimensional, multi-step machine-learning framework that allows objective assessment of the biological relevance of genes to disease studies. Mantis-ml is an automated machine-learning framework that follows a multi-model approach of stochastic semi-supervised learning to rank disease-associated genes through iterative learning sessions on random balanced datasets across the protein-coding exome. When applied to a range of human diseases, including chronic kidney disease (CKD), epilepsy, and amyotrophic lateral sclerosis (ALS), mantis-ml achieved an average area under curve (AUC) prediction performance of 0.81--0.89. Critically, to prove its value as a tool that can be used to interpret exome-wide association studies, we overlapped mantis-ml predictions with data from published cohort-level association studies. We found a statistically significant enrichment of high mantis-ml predictions among the highest-ranked genes from hypothesis-free cohort-level statistics, indicating a substantial improvement over the performance of current state-of-the-art methods and pointing to the capture of true prioritization signals for disease-associated genes. Finally, we introduce a generic mantis-ml score (GMS) trained with over 1,200 features as a generic-disease-likelihood estimator, outperforming published gene-level scores. In addition to our tool, we provide a gene prioritization atlas that includes mantis-ml's predictions across ten disease areas and empowers researchers to interactively navigate through the gene-triaging framework. Mantis-ml is an intuitive tool that supports the objective triaging of large-scale genomic discovery studies and enhances our understanding of complex genotype-phenotype associations. {\copyright} 2020 The Author(s)},
 author = {Vitsios, D. and Petrovski, S.},
 year = {2020},
 title = {Mantis-ml: Disease-Agnostic Gene Prioritization from High-Throughput Genomic Screens by Stochastic Semi-supervised Learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084148107&doi=10.1016%2fj.ajhg.2020.03.012&partnerID=40&md5=6a016a1480c792ccb99ab6550df37c7b},
 keywords = {Artefact Design},
 pages = {659--678},
 volume = {106},
 number = {5},
 journal = {American Journal of Human Genetics},
 doi = {10.1016/j.ajhg.2020.03.012},
 file = {Vitsios, Petrovski 2020 - Mantis-ml Disease-Agnostic Gene Prioritization:Attachments/Vitsios, Petrovski 2020 - Mantis-ml Disease-Agnostic Gene Prioritization.pdf:application/pdf}
}


@article{Volpato.2019,
 abstract = {Background: Although 3D echocardiography (3DE) circumvents many limitations of 2D echocardiography by allowing direct measurements of left ventricular (LV) mass, it is seldom used in clinical practice due to time-consuming analysis. A recently developed 3DE machine learning (ML) approach allows automated determination of LV mass. We aimed to evaluate the accuracy of this new approach by comparing it to cardiac magnetic resonance (CMR) reference and to conventional 3DE volumetric analysis. Methods: We prospectively studied 23 patients who underwent 3DE (Philips EPIQ) and CMR imaging on the same day. Single-beat wide-angle 3D datasets of the left ventricle were acquired. LV mass was quantified using the new automated software (Philips HeartModel) with manual corrections when necessary and using conventional volumetric analysis (TomTec). CMR analysis was performed by manual slice-by-slice tracing of LV endo- and epicardial boundaries. Reproducibility of the ML approach was assessed using repeated measurements and quantified by intra-class correlation (ICC) and coefficients of variation (CoV). Results: Automated LV mass measurements were feasible in 20 patients (87{\%}). The results were similar to CMR-derived values (Bland-Altman bias 5~g, limits of agreement $\pm$37~g) and also to the conventional 3DE analysis (bias 7~g, $\pm$27~g). Processing time was considerably shorter: 1.02~$\pm$~0.24~minutes (CMR: 2.20~$\pm$~0.13~minutes; TomTec: 2.36~$\pm$~0.09~minutes), although manual corrections were performed in most patients. Repeated measurements showed high reproducibility: ICC~=~0.99; CoV~=~4~$\pm$~5{\%}. Conclusions: 3D Echocardiography analysis of LV mass using novel ML-based algorithm is feasible, fast, and accurate and may thus facilitate the incorporation of 3DE measurements of LV mass into clinical practice. {\copyright} 2018 Wiley Periodicals, Inc.},
 author = {Volpato, V. and Mor-Avi, V. and Narang, A. and Prater, D. and Gon{\c{c}}alves, A. and Tamborini, G. and Fusini, L. and Pepi, M. and Patel, A. R. and Lang, R. M.},
 year = {2019},
 title = {Automated, machine learning-based, 3D echocardiographic quantification of left ventricular mass},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059248910&doi=10.1111%2fecho.14234&partnerID=40&md5=9572ed450d2cd71dafb9d5f63ce7b92f},
 keywords = {ML implementation},
 pages = {312--319},
 volume = {36},
 number = {2},
 journal = {Echocardiography},
 doi = {10.1111/echo.14234}
}


@article{Waltz.2020,
 abstract = {Low-dose computed tomography (LDCT) has been extensively validated for lung cancer screening in selected patient populations. Additionally, the use of gated cardiac CT to assess coronary artery calcium (CAC) burden has been validated to determine a patient's risk for major cardiovascular adverse events. This is typically performed by calculating an Agatston score based on density and overall burden of calcified plaque within the coronary arteries. Patients that qualify for LDCT for lung cancer screening commonly share major risk factors for coronary artery disease and would frequently benefit from an additional gated cardiac CT for the assessment of CAC. Given the widespread use of LDCT for lung cancer screening, we evaluated current literature regarding the use of non-gated chest CT, specifically LDCT, for the detection and grading of coronary artery calcifications. Additionally, given the evolving and increasing use of artificial intelligence (AI) in the interpretation of radiologic studies, current literature for the use of AI in CAC assessment was reviewed.

We reviewed primary scientific literature dating up to April 2020 using Pubmed and Google Scholar, with the search terms low dose CT, lung cancer screening, coronary artery calcium, EKG/cardiac gated CT, deep learning, machine learning, and AI. These publications were then independently evaluated by each member of our team. Overall, there was a consensus within these papers that LDCT for lung cancer screening plays a role in the evaluation of CAC. Most studies note the inherent problems with the evaluation of the density of coronary calcifications on LDCT to give an accurate numeric calcium or Agatston score. The current method of evaluating CAC on LDCT involves using a qualitative categorical system (none, mild, moderate, or severe). When performed by cardiac imaging experts, this method broadly correlates with traditional CAC score groups (0, 1 to 100, 101 to 400, and {\textgreater} 400). Furthermore, given the high sensitivity of a properly protocolled LDCT for coronary calcium, a negative study for CAC precludes the need for a dedicated gated CT assessment. However, qualitative methods are not as accurate or reproducible when performed by general radiologists. The implementation of AI in the LDCT screening process has the potential to give a quantifiable and reproducible numeric value to the calcium score, based on whole heart volume scoring of calcium. This more closely aligns with the Agatston score and serves as a better guide for treatment and risk assessment using current guidelines.

We conclude that CAC should be assessed on all LDCT performed for lung cancer screening and that a qualitative categorical scoring system should be provided in the impression for each patient. Early studies involving AI for the assessment of CAC are promising, but more extensive studies are needed before a final recommendation for its use can be given. The implementation of an accurate, automated AI CAC assessment tool would improve radiologist compliance and ease of overall workflow. Ultimately, the potential end result would be improved turnaround time, better patient outcomes, and reduced healthcare costs by maximizing preventative care in this high-risk population.},
 author = {Waltz, J. and Kocher, M. and Kahn, J. and Dirr, M. and Burt},
 year = {2020},
 title = {The Future of Concurrent Automated Coronary Artery Calcium Scoring on Screening Low-Dose Computed Tomography},
 keywords = {Literature Review},
 volume = {12},
 number = {6},
 issn = {2168-8184},
 journal = {CUREUS JOURNAL OF MEDICAL SCIENCE},
 doi = {10.7759/cureus.8574},
 file = {Waltz, Kocher et al. 2020 - The Future of Concurrent Automated:Attachments/Waltz, Kocher et al. 2020 - The Future of Concurrent Automated.pdf:application/pdf}
}


@article{Wan.2024,
 abstract = {Objective: Surveillance algorithms that predict patient decompensation are increasingly integrated with clinical workflows to help identify patients at risk of in-hospital deterioration. This scoping review aimed to identify the design features of the information displays, the types of algorithm that drive the display, and the effect of these displays on process and patient outcomes. Materials and methods: The scoping review followed Arksey and O'Malley's framework. Five databases were searched with dates between January 1, 2009 and January 26, 2022. Inclusion criteria were: participants---clinicians in inpatient settings; concepts---intervention as deterioration information displays that leveraged automated AI algorithms; comparison as usual care or alternative displays; outcomes as clinical, workflow process, and usability outcomes; and context as simulated or real-world in-hospital settings in any country. Screening, full-text review, and data extraction were reviewed independently by 2 researchers in each step. Display categories were identified inductively through consensus. Results: Of 14 575 articles, 64 were included in the review, describing 61 unique displays. Forty-one displays were designed for specific deteriorations (eg, sepsis), 24 provided simple alerts (ie, text-based prompts without relevant patient data), 48 leveraged well-accepted score-based algorithms, and 47 included nurses as the target users. Only 1 out of the 10 randomized controlled trials reported a significant effect on the primary outcome. Conclusions: Despite significant advancements in surveillance algorithms, most information displays continue to leverage well-understood, well-accepted score-based algorithms. Users' trust, algorithmic transparency, and workflow integration are significant hurdles to adopting new algorithms into effective decision support tools. {\copyright} The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.},
 author = {Wan, Y.-K.J. and Wright, M. C. and McFarland, M. M. and Dishman, D. and Nies, M. A. and Rush, A. and Madaras-Kelly, K. and Jeppesen, A. and {Del Fiol}, G.},
 year = {2024},
 title = {Information displays for automated surveillance algorithms of in-hospital patient deterioration: a scoping review},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181177891&doi=10.1093%2fjamia%2focad203&partnerID=40&md5=27992d2a818ca0b6aa506dd7821bac69},
 keywords = {Literature Review},
 pages = {256--273},
 volume = {31},
 number = {1},
 journal = {Journal of the American Medical Informatics Association},
 doi = {10.1093/jamia/ocad203}
}


@proceedings{Wang.2019,
 abstract = {To relieve the pain of manually selecting machine learning algorithms and tuning hyperparameters, automated machine learning (AutoML) methods have been developed to automatically search for good models. Due to the huge model search space, it is impossible to try all models. Users tend to distrust automatic results and increase the search budget as much as they can, thereby undermining the efficiency of AutoML. To address these issues, we design and implement ATMSeer, an interactive visualization tool that supports users in refining the search space of AutoML and analyzing the results. To guide the design of ATMSeer, we derive a workflow of using AutoML based on interviews with machine learning experts. A multi-granularity visualization is proposed to enable users to monitor the AutoML process, analyze the searched models, and refine the search space in real time. We demonstrate the utility and usability of ATMSeer through two case studies, expert interviews, and a user study with 13 end users. {\copyright} 2019 Copyright held by the owner/author(s).},
 year = {2019},
 title = {AtmSeer: Increasing transparency and controllability in automated machine learning},
 keywords = {Artefact Design},
 editor = {Wang, Q. and Ming, Y. and Jin, Z. and Shen, Q. and Liu, D. and Smith, M. J. and Veeramachaneni, K. and Qu, H.},
 doi = {10.1145/3290605.3300911},
 file = {Wang, Ming et al (Hg) 2019 - AtmSeer Increasing transparency and controllability:Attachments/Wang, Ming et al (Hg) 2019 - AtmSeer Increasing transparency and controllability.pdf:application/pdf}
}


@article{Wang.2021,
 abstract = {Objective: To investigate whether bone age (BA) of children living in Tibet Highland could be accurately assessed using a fully automated artificial intelligence (AI) system. Methods： Left hand radiographs of 385 children (300 Tibetan and 85 immigrant Han) aged 4--18 years who presented to the largest medical center of Tibet between September 2013 and November 2019 were consecutively collected. From these radiographs, BA was determined using the Greulich and Pyle (GP) method by experts in a consensus manner; furthermore, BA was estimated by a previously reported artificial intelligence (AI) BA system based on Han children from southern China. The performance of the AI system was compared with that of experts by using statistical analysis. Results: Compared with the experts' results, the accuracy of the AI system for Tibetan and Han children within 1 year was 84.67 and 89.41{\%}, respectively, and its mean absolute difference (MAD) was 0.65 and 0.56 years, respectively. The discrepancy in hand-wrist bone maturation was the main cause of low accuracy of the system in the 4- to 6-year-old group. Conclusion: The AI BA system developed for Han Chinese children living in flat regions could enable to assess BA accurately in Tibet where medical resources are limited. Advances in knowledge: AI-based BA system may serve as an effective and efficient solution to assess BA in Tibet. {\copyright} 2021 The Authors. Published by the British Institute of Radiology},
 author = {Wang, F. and Cidan, W. and Gu, X. and Chen, S. and Yin, W. and Liu, Y. and Shi, L. and Pan, H. and Jin, Z.},
 year = {2021},
 title = {Performance of an artificial intelligence system for bone age assessment in Tibet},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103305528&doi=10.1259%2fbjr.20201119&partnerID=40&md5=5631ebffcb9b3f43a7b37669d7da870f},
 keywords = {ML implementation},
 volume = {94},
 number = {1120},
 journal = {British Journal of Radiology},
 doi = {10.1259/bjr.20201119},
 file = {Wang, Cidan et al. 2021 - Performance of an artificial intelligence:Attachments/Wang, Cidan et al. 2021 - Performance of an artificial intelligence.pdf:application/pdf}
}


@inproceedings{Wang.2022,
 abstract = {Abstract. Credit transactions require an extra level of security, trust, and privacy. Although the bank's credit transaction system technology has gradually improved, it still has high labour costs, inefficiencies, and security concerns. This study proposes a standardized blockchain application model in fintech and discovers that blockchain has broad prospects that can be realized through distributed consensus, smart contract execution, cryptographic algorithms, and distributed ledgers, which increase transparency, efficiency, and fairness in the financial industry. This study also deployed an automated machine learning system at the stage where banks qualify borrowers, optimizing the original credit model.  {\copyright} 2022 ACM.},
 author = {Wang, Y. and Liu, Y.},
 title = {Loan Chain: A Blockchain-Based Framework for Smart Credit Lending},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141597307&doi=10.1145%2f3559795.3559797&partnerID=40&md5=f89c7586ba74834e4a19dd4503e152a5},
 keywords = {Artefact Design},
 pages = {11--15},
 year = {2022},
 doi = {10.1145/3559795.3559797}
}


@inproceedings{Wang.2023,
 abstract = {The web is a treasure trove for data that is increasingly used by computer scientists for building large machine learning models as well as non-computer scientists for social studies or marketing analyses. As such, web-crawling is an essential tool for both computational and non-computational scientists to conduct research. However, most of the existing web crawler frameworks and software products either require professional coding skills without an easy-to-use graphic user interface or are expensive and limited in features. They are thus not friendly to newbies and inconvenient for complicated web-crawling tasks. In this paper, we present an easy-to-use visual web crawler system, EasySpider, for designing and executing web crawling tasks without coding. The workflow of a new web crawling task can be visually programmed by following EasySpider's visual wizard on the target webpages using an intuitive point-and-click interface. The generated crawler task can then be easily invoked locally or as a web service. Our EasySpider is cross-platform and flexible to adapt to different web-resources. It also supports advanced configuration for complicated tasks and extension. The whole system is open-sourced and transparent for free-access at GitHub 1, which avoids possible privacy leakage.  {\copyright} 2023 Owner/Author.},
 author = {Wang, N. and Feng, W. and Yin, J. and Ng, S.-K.},
 title = {EasySpider: A No-Code Visual System for Crawling the Web},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159563376&doi=10.1145%2f3543873.3587345&partnerID=40&md5=28a2aba8b6ebe43d5571d8feddee0437},
 keywords = {Artefact Design},
 pages = {192--195},
 year = {2023},
 doi = {10.1145/3543873.3587345}
}


@article{Wang.2024,
 abstract = {Serum united urine metabolic analysis comprehensively reveals the disease status for kidney diseases in particular. Thus, the precise and convenient acquisition of metabolic molecular information from united biofluids is vitally important for clinical disease diagnosis and biomarker discovery. Laser desorption/ionization mass spectrometry (LDI-MS) presents various advantages in metabolic analysis; however, there remain challenges in ionization efficiency and MS signal reproducibility. Herein, we constructed a self-assembled hyperbranched black gold nanoarray (HyBrAuNA) assisted LDI-MS platform to profile serum united urine metabolic fingerprints (S-UMFs) for diagnosis of early stage renal cell carcinoma (RCC). The closely packed HyBrAuNA afforded strong electromagnetic field enhancement and high photothermal conversion efficacy, enabling effective ionization of low abundant metabolites for S-UMF collection. With a uniform nanoarray, the platform presented excellent reproducibility to ensure the accuracy of S-UMFs obtained in seconds. When it was combined with automated machine learning analysis of S-UMFs, early stage RCC patients were discriminated from the healthy controls with an area under the curve (AUC) {\textgreater} 0.99. Furthermore, we screened out a panel of 9 metabolites (4 from serum and 5 from urine) and related pathways toward early stage kidney tumor. In view of its high-throughput, fast analytical speed, and low sample consumption, our platform possesses potential in metabolic profiling of united biofluids for disease diagnosis and pathogenic mechanism exploration. {\copyright} 2024 American Chemical Society},
 author = {Wang, Y. and Xu, X. and Fang, Y. and Yang, S. and Wang, Q. and Liu, W. and Zhang, J. and Liang, D. and Zhai, W. and Qian, K.},
 year = {2024},
 title = {Self-Assembled Hyperbranched Gold Nanoarrays Decode Serum United Urine Metabolic Fingerprints for Kidney Tumor Diagnosis},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182550460&doi=10.1021%2facsnano.3c10717&partnerID=40&md5=5497c949520778cc53fe0559533ef389},
 keywords = {Artefact Design;ML implementation},
 pages = {2409--2420},
 volume = {18},
 number = {3},
 journal = {ACS Nano},
 doi = {10.1021/acsnano.3c10717}
}


@article{Wei.2024,
 abstract = {Graph classification is an important problem with applications across many domains, for which graph neural networks (GNNs) have been state-of-the-art (SOTA) methods. In the literature, to adopt GNNs for the graph classification task, there are two groups of methods: global pooling and hierarchical pooling. The global pooling methods obtain the graph representation vectors by globally pooling all of the node embeddings together at the end of several GNN layers, whereas the hierarchical pooling methods provide one extra pooling operation between the GNN layers to extract hierarchical information and improve the graph representations. Both global and hierarchical pooling methods are effective in different scenarios. Due to highly diverse applications, it is challenging to design data-specific pooling methods with human expertise. To address this problem, we propose PAS (Pooling Architecture Search) to design adaptive pooling architectures by using the neural architecture search (NAS). To enable the search space design, we propose a unified pooling framework consisting of four modules: Aggregation, Pooling, Readout, andMerge. Two variants, PAS-G and PAS-NE, are provided to design the pooling operations in different scales. A set of candidate operations is designed in the search space using this framework. Then, existing human-designed pooling methods, including global and hierarchical ones, can be incorporated. To enable efficient search, a coarsening strategy is developed to continuously relax the search space, and then a differentiable search method can be adopted.

We conduct extensive experiments on six real-world datasets, including the large-scale datasets MR and ogbg-molhiv. Experimental results in this article demonstrate the effectiveness and efficiency of the proposed PAS in designing the pooling architectures for graph classification. The Top-1 performance on two Open Graph Benchmark (OGB) datasets(1) further indicates the utility of PAS when facing diverse realistic data. The implementation of PAS is available at: https://github.com/AutoML-Research/PAS.},
 author = {Wei, L. N. and Zhao, H. and He, Z. Q. and Yao, Q. M.},
 year = {2024},
 title = {Neural Architecture Search for GNN-Based Graph Classification},
 keywords = {Artefact Design;Experiment},
 volume = {42},
 number = {1},
 issn = {1558-2868},
 journal = {ACM TRANSACTIONS ON INFORMATION SYSTEMS},
 doi = {10.1145/3584945}
}


@article{Winkelmann.2022,
 abstract = {Purpose {\^a} Evaluation of machine learning-based fully automated artery-specific coronary artery calcium (CAC) scoring software, using semi-Automated software as a reference. Methods {\^a} A total of 505 patients underwent non-contrast-enhanced calcium scoring computed tomography (CSCT). Automated, machine learning-based software quantified the Agatston score (AS), volume score (VS), and mass score (MS) of each coronary artery [right coronary artery (RCA), left main (LM), circumflex (CX) and left anterior descending (LAD)]. Identified CAC of readers who annotated the data with semi-Automated software served as a reference standard. Statistics included comparisons of evaluation time, agreement of identified CAC, and comparisons of the AS, VS, and MS of the reference standard and the fully automated algorithm. Results {\^a} The machine learning-based software correlated strongly with the reference standard for the AS, VS, and MS (Spearman{\^E} s rho {\textgreater}0.969) (p{\textless}0.001), with excellent agreement (ICC {\textgreater}0.919) (p{\textless}0.001). The mean assessment time of the reference standard was 59 seconds (IQR 39-140) and that of the automated algorithm was 5.9 seconds (IQR 3.9-16) (p{\textless}0.001). The Bland-Altman plots mean difference and 1.96 upper and lower limits of agreement for all arteries combined were: AS 0.996 (1.33 to 0.74), VS 0.995 (1.40 to 0.71), and MS 0.995 (1.35 to 0.74). The mean bias was minimal: 0.964-1.0429. Risk class assignment showed high accuracy for the AS in total (weighed \textgreek{k}=0.99) and for each individual artery (\textgreek{k}=0.96-0.99) with corresponding correct risk group assignment in 497 of 505 patients (98.4{\%}). Conclusion {\^a} The fully automated artery-specific coronary calcium scoring algorithm is a time-saving procedure and shows excellent correlation and agreement compared with the clinically established semi-Automated approach. Key points: {\^a} Very high correlation and agreement between fully automatic and semi-Automatic calcium scoring software. Less time-consuming than conventional semi-Automatic methods. Excellent tool for artery-specific calcium scoring in a clinical setting. Citation Format Winkelmann MT, Jacoby J, Schwemmer C etal. Fully Automated Artery-Specific Calcium Scoring Based on Machine Learning in Low-Dose Computed Tomography Screening. Fortschr R{\"o}ntgenstr 2022; 194: 763-770. {\copyright} 2022 Georg Thieme Verlag. All rights reserved.},
 author = {Winkelmann, M. T. and Jacoby, J. and Schwemmer, C. and Faby, S. and Krumm, P. and Artzner, C. and Bongers, M. N.},
 year = {2022},
 title = {Fully Automated Artery-Specific Calcium Scoring Based on Machine Learning in Low-Dose Computed Tomography Screening},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124238556&doi=10.1055%2fa-1717-2703&partnerID=40&md5=0b7215245fecde7ee5ac8a52ebbc12c9},
 keywords = {ML implementation},
 pages = {763--770},
 volume = {194},
 number = {7},
 journal = {RoFo Fortschritte auf dem Gebiet der Rontgenstrahlen und der Bildgebenden Verfahren},
 doi = {10.1055/a-1717-2703},
 file = {Winkelmann, Jacoby et al. 2022 - Fully Automated Artery-Specific Calcium Scoring:Attachments/Winkelmann, Jacoby et al. 2022 - Fully Automated Artery-Specific Calcium Scoring.pdf:application/pdf}
}


@article{Wong.2020,
 abstract = {Background Canada is an ethnically-diverse country, yet its lack of ethnicity information in many large databases impedes effective population research and interventions. Automated ethnicity classification using machine learning has shown potential to address this data gap but its performance in Canada is largely unknown. This study conducted a large-scale machine learning framework to predict ethnicity using a novel set of name and census location features. Methods Using census 1901, the multiclass and binary class classification machine learning pipelines were developed. The 13 ethnic categories examined were Aboriginal (First Nations, M{\'e}tis, Inuit, and all-combined)), Chinese, English, French, Irish, Italian, Japanese, Russian, Scottish, and others. Machine learning algorithms included regularized logistic regression, C-support vector, and na{\"i}ve Bayes classifiers. Name features consisted of the entire name string, substrings, double-metaphones, and various name-entity patterns, while location features consisted of the entire location string and substrings of province, district, and subdistrict. Predictive performance metrics included sensitivity, specificity, positive predictive value, negative predictive value, F1, Area Under the Curve for Receiver Operating Characteristic curve, and accuracy. Results The census had 4,812,958 unique individuals. For multiclass classification, the highest performance achieved was 76{\%} F1 and 91{\%} accuracy. For binary classifications for Chinese, French, Italian, Japanese, Russian, and others, the F1 ranged 68--95{\%} (median 87{\%}). The lower performance for English, Irish, and Scottish (F1 ranged 63--67{\%}) was likely due to their shared cultural and linguistic heritage. Adding census location features to the name-based models strongly improved the prediction in Aboriginal classification (F1 increased from 50{\%} to 84{\%}). Conclusions The automated machine learning approach using only name and census location features can predict the ethnicity of Canadians with varying performance by specific ethnic categories. {\copyright} 2020 Wong et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
 author = {Wong, K. O. and Za{\"i}ane, O. R. and Davis, F. G. and Yasui, Y.},
 year = {2020},
 title = {A machine learning approach to predict ethnicity using personal name and census location in Canada},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096407569&doi=10.1371%2fjournal.pone.0241239&partnerID=40&md5=01e7c61ef987c585b4e806a908cc6e3d},
 keywords = {ML implementation},
 volume = {15},
 number = {11 November},
 journal = {PLoS ONE},
 doi = {10.1371/journal.pone.0241239},
 file = {Wong, Za{\"i}ane et al. 2020 - A machine learning approach:Attachments/Wong, Za{\"i}ane et al. 2020 - A machine learning approach.pdf:application/pdf}
}


@proceedings{Xin.2021,
 abstract = {Efforts to make machine learning more widely accessible have led to a rapid increase in Auto-ML tools that aim to automate the process of training and deploying machine learning. To understand how Auto-ML tools are used in practice today, we performed a qualitative study with participants ranging from novice hobbyists to industry researchers who use Auto-ML tools.We present insights into the benefits and deficiencies of existing tools, as well as the respective roles of the human and automation in ML workflows. Finally, we discuss design implications for the future of Auto-ML tool development. We argue that instead of full automation being the ultimate goal of Auto-ML, designers of these tools should focus on supporting a partnership between the user and the Auto-ML tool. This means that a range of Auto-ML tools will need to be developed to support varying user goals such as simplicity, reproducibility, and reliability. {\copyright} 2021 ACM.},
 year = {2021},
 title = {Whither automl? understanding the role of automation in machine learningworkflows},
 keywords = {Empirical Study},
 editor = {Xin, D. and Wu, E. Y. and Lee, D.J.-L. and Salehi, N. and Parameswaran, A.},
 doi = {10.1145/3411764.3445306},
 file = {Xin, Wu et al (Hg) 2021 - Whither automl:Attachments/Xin, Wu et al (Hg) 2021 - Whither automl.pdf:application/pdf}
}


@article{Xue.2022,
 abstract = {BACKGROUND: Global longitudinal shortening (GL-Shortening) and the mitral annular plane systolic excursion (MAPSE) are known markers in heart failure patients, but measurement may be subjective and less frequently reported because of the lack of automated analysis. Therefore, a validated, automated artificial intelligence (AI) solution can be of strong clinical interest. METHODS AND RESULTS: The model was implemented on cardiac magnetic resonance scanners with automated in-line processing. Reproducibility was evaluated in a scan--rescan data set (n=160 patients). The prognostic association with adverse events (death or hospitalization for heart failure) was evaluated in a large patient cohort (n=1572) and compared with feature tracking global longitudinal strain measured manually by experts. Automated processing took $\approx$1.1 seconds for a typical case. On the scan--rescan data set, the model exceeded the precision of human expert (coefficient of variation 7.2{\%} versus 11.1{\%} for GL-Shortening, P=0.0024; 6.5{\%} versus 9.1{\%} for MAPSE, P=0.0124). The minimal detectable change at 90{\%} power was 2.53 percentage points for GL-Shortening and 1.84 mm for MAPSE. AI GL-Shortening correlated well with manual global longitudinal strain (R2=0.85). AI MAPSE had the strongest association with outcomes (\textgreek{q}2, 255; hazard ratio [HR], 2.5 [95{\%} CI, 2.2--2.8]), compared with AI GL-Shortening (\textgreek{q}2, 197; HR, 2.1 [95{\%} CI,1.9--2.4]), manual global longitudinal strain (\textgreek{q}2, 192; HR, 2.1 [95{\%} CI, 1.9--2.3]), and left ventricular ejection fraction (\textgreek{q}2, 147; HR, 1.8 [95{\%} CI, 1.6--1.9]), with P{\textless}0.001 for all. CONCLUSIONS: Automated in-line AI-measured MAPSE and GL-Shortening can deliver immediate and highly reproducible results during cardiac magnetic resonance scanning. These results have strong associations with adverse outcomes that exceed those of global longitudinal strain and left ventricular ejection fraction. {\copyright} 2022 The Authors.},
 author = {Xue, H. and Artico, J. and Davies, R. H. and Adam, R. and Shetye, A. and Augusto, J. B. and Bhuva, A. and Fr{\"o}jdh, F. and Wong, T. C. and Fukui, M. and Cavalcante, J. L. and Treibel, T. A. and Manisty, C. and Fontana, M. and Ugander, M. and Moon, J. C. and Schelbert, E. B. and Kellman, P.},
 year = {2022},
 title = {Automated In-Line Artificial Intelligence Measured Global Longitudinal Shortening and Mitral Annular Plane Systolic Excursion: Reproducibility and Prognostic Significance},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124636246&doi=10.1161%2fJAHA.121.023849&partnerID=40&md5=55bea92c92b4b5e1e8dbe7d5d22ae0d3},
 keywords = {ML implementation},
 volume = {11},
 number = {4},
 journal = {Journal of the American Heart Association},
 doi = {10.1161/JAHA.121.023849},
 file = {Xue, Artico et al. 2022 - Automated In-Line Artificial Intelligence Measured:Attachments/Xue, Artico et al. 2022 - Automated In-Line Artificial Intelligence Measured.pdf:application/pdf}
}


@inproceedings{Yan.2021,
 abstract = {While early research in neural architecture search (NAS) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-fidelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the final validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-fidelity algorithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11. {\copyright} 2021 Neural information processing systems foundation. All rights reserved.},
 author = {Yan, S. and White, C. and Savani, Y. and Hutter, F.},
 title = {NAS-Bench-x11 and the Power of Learning Curves},
 keywords = {Artefact Design},
 pages = {22534--22549},
 year = {2021},
 file = {Yan, White et al 2021 - NAS-Bench-x11 and the Power:Attachments/Yan, White et al 2021 - NAS-Bench-x11 and the Power.pdf:application/pdf}
}


@article{Yang.2020,
 abstract = {Due to the concerted efforts to utilize the microbial features to improve disease prediction capabilities, automated machine learning (AutoML) systems aiming to get rid of the tediousness in manually performing ML tasks are in great demand. Here we developed mAML, an ML model-building pipeline, which can automatically and rapidly generate optimized and interpretable models for personalized microbiome-based classification tasks in a reproducible way. The pipeline is deployed on a web-based platform, while the server is user-friendly and flexible and has been designed to be scalable according to the specific requirements. This pipeline exhibits high performance for 13 benchmark datasets including both binary and multi-class classification tasks. In addition, to facilitate the application of mAML and expand the human disease-related microbiome learning repository, we developed GMrepo ML repository (GMrepo Microbiome Learning repository) from the GMrepo database. The repository involves 120 microbiome-based classification tasks for 85 human-disease phenotypes referring to 12 429 metagenomic samples and 38 643 amplicon samples. The mAML pipeline and the GMrepo ML repository are expected to be important resources for researches in microbiology and algorithm developments. Database URL: http://lab.malab.cn/soft/mAML  {\copyright} 2020 The Author(s) 2020. Published by Oxford University Press.},
 author = {Yang, F. and Zou, Q.},
 year = {2020},
 title = {MAML: An automated machine learning pipeline with a microbiome repository for human disease classification},
 keywords = {Artefact Design},
 volume = {2020},
 journal = {Database},
 file = {Yang, Zou 2020 - MAML An automated machine learning:Attachments/Yang, Zou 2020 - MAML An automated machine learning.pdf:application/pdf}
}


@article{Yang.2024,
 abstract = {A chest X-ray radiography is still the global standard for diagnosing pneumonia. Despite several studies, doctors still have trouble correctly diagnosing and classifying pneumonia. Neural architecture search (NAS) has the potential to enhance diagnostic efficiency and accuracy. However, NAS methods fail to account for the security of data sources, and the result of model prediction cannot be communicated safely and consistently. To tackle these issues, we propose a trustworthy NAS method for pneumonia image classification using blockchain technology, which provides secure, reliable, and high-performance model automatic search and efficient data prediction capabilities. By synergistically combining NAS with blockchain technology, we enhance the transparency and interpretability of NAS-driven image classification processes, thereby safeguarding the confidentiality and integrity of sensitive medical information. Moreover, our approach automates the model construction process for pneumonia image classification, markedly reducing the reliance on manual intervention. Experimental results demonstrate that our method achieves comparable performance to state-of-the-art methods for pneumonia image classification while ensuring security. This provides a new solution for promoting medical aided diagnosis. {\copyright} 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
 author = {Yang, Y. and Wei, J. and Yu, Z. and Zhang, R.},
 year = {2024},
 title = {A trustworthy neural architecture search framework for pneumonia image classification utilizing blockchain technology},
 keywords = {Artefact Design},
 pages = {1694--1727},
 volume = {80},
 number = {2},
 journal = {Journal of Supercomputing},
 file = {Yang, Wei et al. 2024 - A trustworthy neural architecture search:Attachments/Yang, Wei et al. 2024 - A trustworthy neural architecture search.pdf:application/pdf}
}


@article{Yousefi.2022,
 abstract = {Purpose: To identify patterns of visual field (VF) loss based on unsupervised machine learning and to identify patterns that are associated with rapid progression. Design: Cross-sectional and longitudinal study. Participants: A total of 2231 abnormal VFs from 205 eyes of 176 Ocular Hypertension Treatment Study (OHTS) participants followed over approximately 16 years. Methods: Visual fields were assessed by an unsupervised deep archetypal analysis algorithm and an OHTS-certified VF reader to identify prevalent patterns of VF loss. Machine-identified patterns of glaucoma damage were compared against those patterns previously identified (expert-identified) in the OHTS in 2003. Based on the longitudinal VFs of each eye, VF loss patterns that were strongly associated with rapid glaucoma progression were identified. Main Outcome Measures: Machine-expert correspondence and type of patterns of VF loss associated with rapid progression. Results: The average VF mean deviation (MD) at conversion to glaucoma was $-$2.7 decibels (dB) (standard deviation [SD] = 2.4 dB), whereas the average MD of the eyes at the last visit was $-$5.2 dB (SD = 5.5 dB). Fifty out of 205 eyes had MD rate of $-$1 dB/year or worse and were considered rapid progressors. Eighteen machine-identified patterns of VF loss were compared with expert-identified patterns, in which 13 patterns of VF loss were similar. The most prevalent expert-identified patterns included partial arcuate, paracentral, and nasal step defects, and the most prevalent machine-identified patterns included temporal wedge, partial arcuate, nasal step, and paracentral VF defects. One of the machine-identified patterns of VF loss predicted future rapid VF progression after adjustment for age, sex, and initial MD. Conclusions: An automated machine learning system can identify patterns of VF loss and could provide objective and reproducible nomenclature for characterizing early signs of visual defects and rapid progression in patients with glaucoma. {\copyright} 2022 American Academy of Ophthalmology},
 author = {Yousefi, S. and Pasquale, L. R. and Boland, M. V. and Johnson, C. A.},
 year = {2022},
 title = {Machine-Identified Patterns of Visual Field Loss and an Association with Rapid Progression in the Ocular Hypertension Treatment Study},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138575342&doi=10.1016%2fj.ophtha.2022.07.001&partnerID=40&md5=6d0e8ab5c0ed69e4f71b43234012df8a},
 keywords = {ML implementation},
 pages = {1402--1411},
 volume = {129},
 number = {12},
 journal = {Ophthalmology},
 doi = {10.1016/j.ophtha.2022.07.001},
 file = {Yousefi, Pasquale et al. 2022 - Machine-Identified Patterns of Visual Field:Attachments/Yousefi, Pasquale et al. 2022 - Machine-Identified Patterns of Visual Field.pdf:application/pdf}
}


@article{Yuan.2021,
 abstract = {A hydrophilic interaction (HILIC) ultra-high performance liquid chromatography (UHPLC) with triple quadrupole tandem mass spectrometry (MS/MS) method was developed and validated for the quantification of 21 free amino acids (AAs). Compared to published reports, our method renders collectively improved sensitivity with lower limit of quantification (LLOQ) at 0.5{\~{}}42.19 ng/mL with 0.3 \textgreek{m}L injection volume (or equivalently 0.15{\~{}}12.6 pg injected on column), robust linear range from LLOQ up to 3521{\~{}}5720 ng/mL (or 1056 {\~{}} 1716 pg on column) and a high throughput with total time of 6 min per sample, as well as easier experimental setup, less maintenance and higher adaptation flexibility. Ammonium formate in the mobile phase, though commonly used in HILIC, was found unnecessary in our experimental setup, and its removal from mobile phase was key for significant improvement in sensitivity (4{\~{}}74 times higher than with 5 mM ammonium formate). Addition of 10 (or up to100 mM) hydrochloric acid (HCl) in the sample diluent was crucial to keep response linearity for basic amino acids of histidine, lysine and arginine. Different HCl concentration (10{\~{}}100 mM) in sample diluent also excreted an effect on detection sensitivity, and it is of importance to keep the final prepared sample and calibrators in the same HCl level. Leucine and isoleucine were distinguished using different transitions. Validated at seven concentration levels, accuracy was bound within 75{\~{}}125{\%}, matrix effect generally within 90{\~{}}110{\%}, and precision error mostly below 2.5{\%}. Using this newly developed method, the free amino acids were then quantified in a total of 544 African indigenous vegetables (AIVs) samples from African nightshades (AN), Ethiopian mustards (EM), amaranths (AM) and spider plants (SP), comprising a total of 8 identified species and 43 accessions, cultivated and harvested in USA, Kenya and Tanzania over several years, 2013{\~{}}2018. The AN, EM, AM and SP were distinguished based on free AAs profile using machine learning methods (ML) including principle component analysis, discriminant analysis, na{\"i}ve Bayes, elastic net-regularized logistic regression, random forest and support vector machine, with prediction accuracy achieved at ca. 83{\~{}}97{\%} on the test set (train/test ratio at 7/3). An interactive ML platform was constructed using R Shiny at https://boyuan.shinyapps.io/AIV{\_}Classifier/ for modeling train-test simulation and category prediction of unknown AIV sample(s). This new method presents a robust and rapid approach to quantifying free amino acids in plants for use in evaluating plants, biofortification, botanical authentication, safety, adulteration and with applications to nutrition, health and food product development. {\copyright} 2020},
 author = {Yuan, B. and Lyu, W. and Dinssa, F. F. and Simon, J. E. and Wu, Q.},
 year = {2021},
 title = {Free amino acids in African indigenous vegetables: Analysis with improved hydrophilic interaction ultra-high performance liquid chromatography tandem mass spectrometry and interactive machine learning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098704554&doi=10.1016%2fj.chroma.2020.461733&partnerID=40&md5=cc05afcf3c63b7f7c6c103a93d2972f6},
 keywords = {ML implementation},
 volume = {1637},
 journal = {Journal of Chromatography A},
 doi = {10.1016/j.chroma.2020.461733}
}


@article{Zafaranieh.2023,
 abstract = {Detecting and quantifying surface densities of placental villi and their vasculature adds important information on the development of the placenta under different exposures and pathological conditions. Today, a larger number of samples and tissue areas can be examined using automated Artificial Intelligence-based approaches. Although each image series calls for a particular approach, sharing the methods will help in facilitating reproducibility and comparability. Here we show the protocol of a software-based quantification of vessels (number and area) in villous tissues of human placentas, based on scanned images of full-size placental sections. {\copyright} 2023},
 author = {Zafaranieh, S. and Kummer, D. and {van Poppel}, M.N.M. and Desoye, G. and Huppertz, B.},
 year = {2023},
 title = {Automated stereological image analysis approach of the human placenta: Surface areas and vascularization},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169916343&doi=10.1016%2fj.placenta.2023.09.002&partnerID=40&md5=b1ff5c2e8c4fcd740e2a1a2443637211},
 pages = {115--118},
 volume = {142},
 journal = {Placenta},
 doi = {10.1016/j.placenta.2023.09.002}
}


@inproceedings{Zaidi.2021,
 abstract = {Ensembles of neural networks achieve superior performance compared to standalone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. Deep ensembles, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a fixed architecture. Instead, we propose two methods for automatically constructing ensembles with varying architectures, which implicitly trade-off individual architectures' strengths against the ensemble's diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: https://github.com/automl/nes {\copyright} 2021 Neural information processing systems foundation. All rights reserved.},
 author = {Zaidi, S. and Zela, A. and Elsken, T. and Holmes, C. and Hutter, F. and Teh, Y. W.},
 title = {Neural Ensemble Search for Uncertainty Estimation and Dataset Shift},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131838712&partnerID=40&md5=34c6f74a7d555d29bcc9e9e062ea19ca},
 keywords = {Artefact Design},
 pages = {7898--7911},
 year = {2021},
 file = {Zaidi, Zela et al 2021 - Neural Ensemble Search for Uncertainty:Attachments/Zaidi, Zela et al 2021 - Neural Ensemble Search for Uncertainty.pdf:application/pdf}
}


@article{ZargariKhuzani.2021,
 abstract = {Chest-X ray (CXR) radiography can be used as a first-line triage process for non-COVID-19 patients with pneumonia. However, the similarity between features of CXR images of COVID-19 and pneumonia caused by other infections makes the differential diagnosis by radiologists challenging. We hypothesized that machine learning-based classifiers can reliably distinguish the CXR images of COVID-19 patients from other forms of pneumonia. We used a dimensionality reduction method to generate a set of optimal features of CXR images to build an efficient machine learning classifier that can distinguish COVID-19 cases from non-COVID-19 cases with high accuracy and sensitivity. By using global features of the whole CXR images, we successfully implemented our classifier using a relatively small dataset of CXR images. We propose that our COVID-Classifier can be used in conjunction with other tests for optimal allocation of hospital resources by rapid triage of non-COVID-19 cases. {\copyright} 2021, The Author(s).},
 author = {{Zargari Khuzani}, A. and Heidari, M. and Shariati, S. A.},
 year = {2021},
 title = {COVID-Classifier: an automated machine learning model to assist in the diagnosis of COVID-19 infection in chest X-ray images},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105577777&doi=10.1038%2fs41598-021-88807-2&partnerID=40&md5=15086e25f58cfe2326c8b0825d391bbe},
 keywords = {ML implementation},
 volume = {11},
 number = {1},
 journal = {Scientific Reports},
 doi = {10.1038/s41598-021-88807-2},
 file = {Zargari Khuzani, Heidari et al. 2021 - COVID-Classifier:Attachments/Zargari Khuzani, Heidari et al. 2021 - COVID-Classifier.pdf:application/pdf}
}


@article{Zeng.2020,
 abstract = {Objectives: This study is aimed to assess the feasibility of AutoML technology for the identification of invasive ductal carcinoma (IDC) in whole slide images (WSI).

Methods: The study presents an experimental machine learning (ML) model based on Google Cloud AutoML Vision instead of a handcrafted neural network. A public dataset of 278,124 labeled histopathology images is used as the original dataset for the model creation. In order to balance the number of positive and negative IDC samples, this study also augments the original public dataset by rotating a large portion of positive image samples. As a result, a total number of 378,215 labeled images are applied.

Results: A score of 91.6{\%} average accuracy is achieved during the model evaluation as measured by the area under precision-recall curve (AuPRC). A subsequent test on a held-out test dataset (unseen by the model) yields a balanced accuracy of 84.6{\%}. These results outperform the ones reported in the earlier studies. Similar performance is observed from a generalization test with new breast tissue samples we collected from the hospital.

Conclusions: The results obtained from this study demonstrate the maturity and feasibility of an AutoML approach for IDC identification. The study also shows the advantage of AutoML approach when combined at scale with cloud computing.},
 author = {Zeng, Y. and Zhang, J. M.},
 year = {2020},
 title = {A machine learning model for detecting invasive ductal carcinoma with Google Cloud AutoML Vision},
 keywords = {ML implementation},
 volume = {122},
 journal = {Computers in Biology and Medicine},
 doi = {10.1016/j.compbiomed.2020.103861}
}


@article{ZhangZhou.2024,
 abstract = {Although machine learning (ML) has brought new insights into geochemistry research, its implementation is laborious and time-consuming. Here, we announce Geochemistry pi, an open-source automated ML Python framework. Geochemists only need to provide tabulated data and select the desired options to clean data and run ML algorithms. The process operates in a question-and-answer format, and thus does not require that users have coding experience. After either automatic or manual parameter tuning, the automated Python framework provides users with performance and prediction results for the trained ML model. Based on the scikit-learn library, Geochemistry pi has established a customized automated process for implementing classification, regression, dimensionality reduction, and clustering algorithms. The Python framework enables extensibility and portability by constructing a hierarchical pipeline architecture that separates data transmission from the algorithm application. The AutoML module is constructed using the Cost-Frugal Optimization and Blended Search Strategy hyperparameter search methods from the A Fast and Lightweight AutoML Library, and the model parameter optimization process is accelerated by the Ray distributed computing framework. The MLflow library is integrated into ML lifecycle management, which allows users to compare multiple trained models at different scales and manage the data and diagrams generated. In addition, the front-end and back-end frameworks are separated to build the web portal, which demonstrates the ML model and data science workflow through a user-friendly web interface. In summary, Geochemistry pi provides a Python framework for users and developers to accelerate their data mining efficiency with both online and offline operation options.

Geochemistry pi is a helpful tool for scientists who work with geochemical data. One of its standout features is its simplicity. Scientists can use the tool to perform machine learning (ML) on the tabular data by answering a series of questions about what they want to discover. The tool does the rest by using advanced ML techniques to uncover insights from the data. Even scientists without coding skills can use Geochemistry pi effectively. This tool is built on a reliable library called scikit-learn, ensuring that it works well with different ML methods. It is also flexible, allowing researchers to customize it to fit their specific needs. Geochemistry pi separates data processing from ML tasks, making it adaptable and expandable. It includes features for continuous training and managing the entire ML process. To prove its effectiveness, Geochemistry pi was tested against previous geochemical studies in areas such as regression, classification, clustering, and dimensional reduction. The results showed that it could replicate the findings of these studies accurately. Accessible through a web portal or command line, Geochemistry pi is a valuable asset for geochemists and researchers looking to analyze large geochemical data sets.

Open-source Python framework for machine learning applications in geochemistryAutomated pipeline for tabular dataQuestion-and-answer format obviates the need for coding experience},
 author = {ZhangZhou, J. and He, C. and Sun, J. H. and Zhao, J. M. and Lyu, Y. and Wang, S. X. and Zhao, W. Y. and Li, A. Z. and Ji, X. H. and Agarwal, A.},
 year = {2024},
 title = {Geochemistry \textgreek{p}: Automated Machine Learning Python Framework for Tabular Data},
 keywords = {Artefact Design},
 volume = {25},
 number = {1},
 issn = {1525-2027},
 journal = {GEOCHEMISTRY GEOPHYSICS GEOSYSTEMS},
 doi = {10.1029/2023GC011324}
}


@article{Zoller.2023,
 abstract = {In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML, an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab, experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML. We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML, leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself. {\copyright} 2023 Copyright held by the owner/author(s)},
 author = {Z{\"o}ller, M.-A. and Titov, W. and Schlegel, T. and Huber, M. F.},
 year = {2023},
 title = {XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning},
 keywords = {Artefact Design},
 volume = {13},
 number = {4},
 journal = {ACM Transactions on Interactive Intelligent Systems},
 doi = {10.1145/3625240},
 file = {Z{\"o}ller, Titov et al 2023 - XAutoML A Visual Analytics Tool:Attachments/Z{\"o}ller, Titov et al 2023 - XAutoML A Visual Analytics Tool.pdf:application/pdf}
}


